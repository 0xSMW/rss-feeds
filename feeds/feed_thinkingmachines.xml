<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Thinking Machines Blog</title>
    <link>https://thinkingmachines.ai/blog</link>
    <description>Shared science and news from the Thinking Machines team</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>en</language>
    <lastBuildDate>Wed, 11 Feb 2026 00:20:42 +0000</lastBuildDate>
    <item>
      <title>Tinker: General Availability and Vision Input</title>
      <link>https://thinkingmachines.ai/blog/tinker-general-availability/</link>
      <description>Today we are announcing four updates to Tinker:</description>
      <content:encoded>&lt;article class="content"&gt;

&lt;p&gt;Today we are announcing four updates to Tinker:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No more waitlist&lt;/li&gt;
&lt;li&gt;New reasoning model: Kimi K2 Thinking&lt;/li&gt;
&lt;li&gt;New inference interface that is compatible with the OpenAI API&lt;/li&gt;
&lt;li&gt;Vision input support with Qwen3-VL&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="general-availability"&gt;General availability&lt;/h2&gt;
&lt;p&gt;The waitlist is over! Everybody can use Tinker now; &lt;a href="https://auth.thinkingmachines.ai/sign-up"&gt;sign up here&lt;/a&gt; to get started. See the &lt;a href="https://thinkingmachines.ai/tinker/"&gt;Tinker homepage&lt;/a&gt; for available models and pricing, and check out the &lt;a href="https://github.com/thinking-machines-lab/tinker-cookbook"&gt;Tinker cookbook&lt;/a&gt; for code examples.&lt;/p&gt;
&lt;h2 id="more-reasoning-with-kimi-k2-thinking"&gt;More reasoning with Kimi K2 Thinking&lt;/h2&gt;
&lt;p&gt;Users can now fine-tune Kimi K2 Thinking on Tinker. With a trillion parameters, Kimi K2 is the largest model in our lineup so far. It is built for long chains of reasoning and tool use.&lt;/p&gt;
&lt;h2 id="openai-api-compatible-sampling"&gt;OpenAI API-compatible sampling&lt;/h2&gt;
&lt;p&gt;Tinker has a standard function for inference:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="chroma" tabindex="0"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;prompt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;types&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModelInput&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_ints&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"The capital of France is"&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;types&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SamplingParams&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_tokens&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;temperature&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stop&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;future&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sampling_client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prompt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;prompt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sampling_params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With this release, we have added OpenAI API-compatible scaffolding for quickly sampling from a model by specifying a path, even while it’s still training. This also means Tinker can now plug-and-play with any OpenAI API-compatible platform. See more information in our &lt;a href="https://tinker-docs.thinkingmachines.ai/compatible-apis/openai"&gt;Tinker documentation&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="chroma" tabindex="0"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;openai_client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;completions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"tinker://0034d8c9-0a88-52a9-b2b7-bce7cb1e6fef:train:0/sampler_weights/000080"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;    &lt;span class="n"&gt;prompt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"The capital of France is"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;    &lt;span class="n"&gt;max_tokens&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;    &lt;span class="n"&gt;temperature&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;    &lt;span class="n"&gt;stop&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="vision-input-with-qwen3-vl"&gt;Vision input with Qwen3-VL&lt;/h2&gt;
&lt;p&gt;We’ve added two vision models to Tinker: Qwen3-VL-30B-A3B-Instruct and Qwen3-VL-235B-A22B-Instruct. With these, users can process pictures, screenshots, and diagrams for a variety of applications.&lt;/p&gt;
&lt;p&gt;To input images, just interleave together an &lt;a href="https://tinker-docs.thinkingmachines.ai/api-reference/types#imagechunk-objects"&gt;ImageChunk&lt;/a&gt; – consisting of your image, saved as bytes – with text chunks. For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="chroma" tabindex="0"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;model_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tinker&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModelInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chunks&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;  &lt;span class="n"&gt;tinker&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;types&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ImageChunk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;image_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"png"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;  &lt;span class="n"&gt;tinker&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;types&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;EncodedTextChunk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"What is this?"&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;These vision inputs can be used in a variety of applications out-of-the-box, including SFT and RL finetuning.&lt;/p&gt;
&lt;p&gt;To demonstrate vision understanding in action, we are sharing &lt;a href="https://github.com/thinking-machines-lab/tinker-cookbook/tree/main/tinker_cookbook/recipes/vlm_classifier"&gt;a new cookbook recipe for fine-tuning VLMs as image classifiers&lt;/a&gt;. Qwen3-VL-235B-A22B-Instruct obtains reasonable accuracy even given just one example per class; performance improves with more labeled data.&lt;/p&gt;
&lt;h2 id="training-image-classifiers-with-tinker"&gt;Training image classifiers with Tinker&lt;/h2&gt;
&lt;p&gt;To showcase Tinker’s new vision capabilities, we finetuned &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct"&gt;Qwen3-VL-235B-A22B-Instruct&lt;/a&gt; to classify images on four classic datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://data.caltech.edu/records/mzrjq-6wc02"&gt;Caltech 101&lt;/a&gt;, a dataset of 101 general object categories.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/datasets/eduardo4jesus/stanford-cars-dataset"&gt;Stanford Cars&lt;/a&gt;, a dataset of car makes, models, and years.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.robots.ox.ac.uk/~vgg/data/flowers/102/"&gt;Oxford Flowers&lt;/a&gt;, a dataset of flower species.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.robots.ox.ac.uk/~vgg/data/pets/"&gt;Oxford Pets&lt;/a&gt;, a dataset of pet breeds.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since Qwen3-VL is a language model, we frame classification as text generation: given an image, the model outputs the class name. We compare this approach against a traditional vision baseline of finetuning a vision-only model — DINOv2-base. &lt;a href="https://arxiv.org/pdf/2304.07193"&gt;DINOv2&lt;/a&gt; is a self-supervised vision transformer that was trained to encode images, and is commonly used as a backbone for pure computer vision tasks. For DINOv2, we add a classification head that predicts a distribution over all N classes. Both models are fine-tuned with LoRA.&lt;/p&gt;
&lt;p&gt;Labeled image data is scarce for many real-world use cases, so data efficiency is the primary measure we look at. We show the classification accuracy when sweeping across the number of labeled examples per class, starting with just a single one.&lt;/p&gt;
&lt;figure id="fig:qwen-v-dino"&gt;
&lt;img data-zoomable="" src="https://thinkingmachines.ai/blog/tinker-general-availability/images/vlm-graphs.png"/&gt;
&lt;figcaption&gt;Comparison of fine-tuned Qwen3-VL-235-A22B and DINOv2 performance on simple image classification tasks.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In the limited-data regime, Qwen3-VL-235-A22B outperforms DINOv2. Not only is it a bigger model, but as a VLM, it also comes with language knowledge out-of-the-box (i.e. what a “golden retriever” or “sunflower” is). This general language-and-vision capability of Qwen3-VL makes it readily available for vision tasks beyond classification.&lt;/p&gt;
&lt;h2 id="happy-holidays"&gt;Happy Holidays&lt;/h2&gt;
&lt;p&gt;Tinker exists to enable builders and researchers to train and customize state-of-the-art models. As always, we look forward to seeing what you build with Tinker. Happy holidays!&lt;/p&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://thinkingmachines.ai/blog/tinker-general-availability/</guid>
      <category>Research</category>
      <pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Tinker: Call for Community Projects</title>
      <link>https://thinkingmachines.ai/blog/call-for-community-projects/</link>
      <description>We launched Tinker to enable builders and researchers to train models their own way, whether they’re conducting studies or customizing models for new applications. We plan to publish regular roundups of the coolest projects from the Tinker community, and we invite you to submit what you’ve been Tinkering on to be featured on our blog .</description>
      <content:encoded>&lt;article class="content"&gt;

&lt;p&gt;We launched &lt;a href="https://thinkingmachines.ai/tinker/"&gt;Tinker&lt;/a&gt; to enable builders and researchers to train models their own way, whether they’re conducting studies or customizing models for new applications. We plan to publish regular roundups of the coolest projects from the Tinker community, and &lt;strong&gt;we invite you to submit what you’ve been Tinkering on to be featured on our blog&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Below are some broad suggestions for what we hope to see from the Tinker featured projects, and some specific research directions we would particularly love to see pursued.&lt;/p&gt;
&lt;h2 id="guidelines-for-tinker-featured-projects"&gt;Guidelines for Tinker Featured Projects&lt;/h2&gt;
&lt;p&gt;We’re interested in featuring ML research projects, AI-enabled research in other domains, custom models, and other contributions. Some examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A reimplementation of a research project or tech report using Tinker, such as papers that compare algorithmic recipes or datasets.&lt;/li&gt;
&lt;li&gt;Original research in machine learning, such as exploring new approaches to training or optimization or applying novel benchmarks and evaluations.&lt;/li&gt;
&lt;li&gt;Research in a non-AI field that uses fine-tuned models, such as the work on mathematical theorem provers and chemistry models we &lt;a href="https://thinkingmachines.ai/blog/announcing-tinker/#:~:text=Groups%20at%20Princeton%2C%20Stanford"&gt;highlighted previously&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Product prototypes built with Tinker, demoing a model that does something fresh or delightful.&lt;/li&gt;
&lt;li&gt;Novel datasets and task environments for training models.&lt;/li&gt;
&lt;li&gt;High-level libraries built on top of Tinker that enable less experienced practitioners to perform fine-tuning effectively.&lt;/li&gt;
&lt;li&gt;Infrastructure contributions, such as a clean self-hosted implementation of the Tinker training API.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Your submission should include a write-up and, preferably, an open-source release of your code. We encourage you to focus on rigor and clear evaluation in your write-ups: crisp charts, raw output examples, clear comparisons to alternative approaches or models on relevant benchmarks and metrics. Tinkering is experimenting — we want to feature diligent work and transparent results over novelty or hype.&lt;/p&gt;
&lt;p&gt;Please send your projects and any related questions to &lt;a href="https://thinkingmachines.ai/cdn-cgi/l/email-protection#72061b1c19170032061a1b1c191b1c151f13111a1b1c17015c131b4d010710181711064f341713060700171657404222001d18171106574042"&gt;[email protected]&lt;/a&gt; with “Featured Project” in the subject line.&lt;/p&gt;
&lt;h2 id="suggested-research-projects"&gt;Suggested research projects&lt;/h2&gt;
&lt;p&gt;Here are some research directions that we would personally love to see explored and that Tinker can enable real progress on. We have &lt;a href="https://github.com/thinking-machines-lab/tinker-project-ideas"&gt;created a repo&lt;/a&gt; with detailed motivation and guidelines for each; we’ll be adding more resources to it over time to help researchers get started. We expect most project ideas to surprise us, but this short list could serve as inspiration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/thinking-machines-lab/tinker-project-ideas/blob/main/replicate-cai-with-base-models.md"&gt;Replicating Constitutional AI, starting from the base model.&lt;/a&gt;&lt;/strong&gt; Though RLAIF is widely used, it’s most often bootstrapped from existing instruction-tuned models. This makes it difficult to separate the impact of the constitution from the impact of the data-generating model that interprets it. A study of Constitutional AI with and without instruction-tuned models in the pipeline would shed light on the use of constitutions and RLAIF.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/thinking-machines-lab/tinker-project-ideas/blob/main/noisy-student.md"&gt;RLVR with Noisy student.&lt;/a&gt;&lt;/strong&gt; Noisy student self-distillation was a popular technique in an earlier era of machine learning for making use of large unlabeled datasets, but it hasn’t been adapted widely to LLMs. One possible adaptation is to start RLVR with a small labeled training set and a large unlabeled one, then have the student apply labels to the latter set after each RL run and iterate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/thinking-machines-lab/tinker-project-ideas/blob/main/on-policy-context-distillation.md"&gt;On-Policy Context Distillation.&lt;/a&gt;&lt;/strong&gt; Context distillation trains a student model with empty context on a teacher model with long and detailed context. Prior work used off-policy distillation — fine-tuning on teacher samples. We have found that &lt;a href="https://thinkingmachines.ai/blog/on-policy-distillation/"&gt;on-policy distillation&lt;/a&gt; is often much more effective; it would be useful to compare the two approaches for context distillation in particular.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/thinking-machines-lab/tinker-project-ideas/blob/main/memorization-empirical-study.md"&gt;RL memory test.&lt;/a&gt;&lt;/strong&gt; Our &lt;a href="https://thinkingmachines.ai/blog/lora/"&gt;post on LoRA&lt;/a&gt; presented theoretical arguments on the rate of information acquisition by both SFT and RL. You can set up a toy environment where RL must learn a completely random number sequence, to compare the empirical learning rate under various reward functions to the theoretical estimate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/thinking-machines-lab/tinker-project-ideas/blob/main/direct-rl-on-pairwise-judge.md"&gt;Direct RL on pairwise judge.&lt;/a&gt;&lt;/strong&gt; RLHF and RLAIF use datasets of pairwise preferences, which are used to train a reward model, which is then used in RL. As an alternative “direct” approach, we can do RL using a prompted model that does pairwise comparisons, without training the reward model. It would be interesting to do experiments comparing the direct and indirect approaches.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/thinking-machines-lab/tinker-project-ideas/blob/main/replicate-open-character-training.md"&gt;Replicate Open Character Training.&lt;/a&gt;&lt;/strong&gt; Replicate the recent paper on &lt;a href="https://arxiv.org/abs/2511.01689"&gt;Open Character Training&lt;/a&gt; using Tinker.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/thinking-machines-lab/tinker-project-ideas/blob/main/gan-joke-generation.md"&gt;GAN for jokes.&lt;/a&gt;&lt;/strong&gt; In domains such as humor, it is easier to curate a human-vetted set of demonstrations than to train a reliable judge or reward model. Try implementing GAN-style training for a joke evaluator and joke generator that can craft a joke with a requested subject and keywords.&lt;/p&gt;
&lt;h2 id="tips-for-high-quality-ml-experiments"&gt;Tips for high-quality ML experiments&lt;/h2&gt;
&lt;p&gt;In closing, we want to offer a few guidelines for running quality ML studies, the same guidelines we strive to adhere to internally when running experiments and documenting the results.&lt;/p&gt;
&lt;p&gt;We encourage researchers to apply multiple analyses for examining each result. When creating datasets or environments, we recommend training a range of models and applying different evals. When developing novel methods, we suggest comparing to simpler baseline methods and sweeping hyperparameters that performance is sensitive to, particularly learning rate.&lt;/p&gt;
&lt;p&gt;We’d love to see your reasoning in the write-up: assumptions you made, how your approach diverges from previous reports, and what motivated each change. We hope to see examples of the raw data and model rollouts, along with the summarized results. Finally, we appreciate crisp and detailed write-ups with clean and well-labeled charts and illustrations of the inner workings of the methods used.&lt;/p&gt;
&lt;p&gt;We are excited to see what our community creates with Tinker, and hope that our featured projects will inspire your own work.&lt;/p&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://thinkingmachines.ai/blog/call-for-community-projects/</guid>
      <category>Research</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Announcing Tinker Research and Teaching Grants</title>
      <link>https://thinkingmachines.ai/blog/tinker-research-and-teaching-grants/</link>
      <description>We launched Tinker nearly one month ago. Since then, researchers across academia and non-profits have been using Tinker to train custom models and advance their research.</description>
      <content:encoded>&lt;article class="content"&gt;

&lt;p&gt;We launched &lt;a href="https://thinkingmachines.ai/blog/announcing-tinker/"&gt;Tinker&lt;/a&gt; nearly one month ago. Since then, researchers across academia and non-profits have been using Tinker to train custom models and advance their research.&lt;/p&gt;
&lt;p&gt;Today, we’re launching research and teaching grants for Tinker access. As part of our commitment to open and collaborative science, we want to make it as easy as possible for students and scholars to use Tinker. If your research or teaching involves training open-weight LLMs, we encourage you to apply.&lt;/p&gt;
&lt;p&gt;We’re offering two types of grants to support your work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Teaching Grants:&lt;/strong&gt; We provide $250 in free credits per student for academic classes using Tinker, whether you’re integrating it into an assignment or enabling students to use Tinker for self-directed projects, this is sized to support your entire class for the duration of the course.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Research Grants:&lt;/strong&gt; We provide grants starting at $5,000 to support research projects and open-source software that uses Tinker.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A selection of early grants we have awarded:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Diyi Yang’s &lt;a href="https://web.stanford.edu/class/cs329x/"&gt;Stanford class on Human-Centered LLMs&lt;/a&gt; uses Tinker to compare different approaches for training personalized LLMs that capture unique writing styles and align with user habits.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Aviral Kumar and Katerina Fragkiadaki’s &lt;a href="https://cmudeeprl.github.io/703website_f25/"&gt;CMU class on Deep RL&lt;/a&gt; will use Tinker to enable class projects to experiment with state-of-the-art methods for training LLM and VLM based policies via RL.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://chemistry.stanford.edu/people/grant-m-rotskoff"&gt;Grant Rotskoff&lt;/a&gt;’s lab at Stanford is fine-tuning small-molecule chemistry models with Tinker to help solve problems in computational chemistry.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Instructors&lt;/strong&gt;, please apply for teaching grants &lt;a href="https://form.typeform.com/to/JgPkuMvB"&gt;&lt;strong&gt;here.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Researchers&lt;/strong&gt;, please apply for research grants &lt;a href="https://form.typeform.com/to/E9wVFZJJ"&gt;&lt;strong&gt;here.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We’re assessing applications on a rolling basis and will aim to respond within a week of your application.&lt;/p&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://thinkingmachines.ai/blog/tinker-research-and-teaching-grants/</guid>
      <category>Research</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>On-Policy Distillation</title>
      <link>https://thinkingmachines.ai/blog/on-policy-distillation/</link>
      <description>LLMs are capable of expert performance in focused domains, a result of several capabilities stacked together: perception of input, knowledge retrieval, plan selection, and reliable execution. This requires a stack of training approaches, which we can divide into three broad stages:</description>
      <content:encoded>&lt;article class="content"&gt;

&lt;p&gt;LLMs are capable of expert performance in focused domains, a result of several capabilities stacked together: perception of input, knowledge retrieval, plan selection, and reliable execution. This requires a stack of training approaches, which we can divide into three broad stages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pre-training&lt;/strong&gt; teaches general capacities such as language use, broad reasoning, and world knowledge.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mid-training&lt;/strong&gt; imparts domain knowledge, such as code, medical databases, or internal company documents.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Post-training&lt;/strong&gt; elicits targeted behavior, such as instruction following, reasoning through math problems, or chat.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Smaller models with stronger training often outperform larger, generalist models in their trained domains of expertise. There are many benefits to using smaller models: they can be deployed locally for privacy or security considerations, can continuously train and get updated more easily, and save on inference costs. Taking advantage of these requires picking the right approach for the later stages of training.&lt;/p&gt;
&lt;p&gt;Approaches to post-training a “student” model can be divided into two kinds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;On-policy training&lt;/strong&gt; samples rollouts from the student model itself, and assigns them some reward.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Off-policy training&lt;/strong&gt; relies on target outputs from some external source that the student learns to imitate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, we may wish to train a compact model to solve math questions such as:&lt;/p&gt;
&lt;figure id="fig:prompt" style="margin-bottom: 1.2rem;"&gt;
&lt;picture style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"&gt;
&lt;source media="(max-width: 768px)" srcset="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/prompt-mobile.svg"/&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/prompt.svg" style="display: block; width: 100%; height: auto;"/&gt;
&lt;/picture&gt;
&lt;/figure&gt;
&lt;p&gt;We can do on-policy training via reinforcement learning, by grading each student rollout on whether it solves the question. This grading can be done by a human, or by a “teacher” model that reliably gets the correct answer.&lt;/p&gt;
&lt;figure id="fig:reinforcement-learning"&gt;
&lt;picture style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"&gt;
&lt;source media="(max-width: 768px)" srcset="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/reinforcement-learning-mobile.svg"/&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/reinforcement-learning.svg" style="display: block; width: 100%; height: auto;"/&gt;
&lt;/picture&gt;
&lt;/figure&gt;
&lt;br/&gt;
&lt;p&gt;The strength of on-policy training is that by training on samples from itself, the student learns to avoid mistakes in a more direct way. But RL has a major downside: it provides very sparse feedback, teaching a &lt;a href="https://thinkingmachines.ai/blog/lora/#how-much-capacity-is-needed-by-supervised-and-reinforcement-learning"&gt;fixed number of bits per training episode&lt;/a&gt; regardless of the number of tokens used. In our example above, the student learns that “21” is the wrong answer and updates away from producing the rollout it tried. But it doesn’t learn where exactly the mistake was made, whether it got the order of operations wrong or erred in the arithmetic itself. This sparsity of feedback makes RL inefficient for many applications.&lt;/p&gt;
&lt;p&gt;Off-policy training is often done with supervised fine-tuning (SFT): training on a curated set of task-specific labeled examples. The source of these labeled examples can be a teacher model that is proven to perform well on the task at hand.&lt;/p&gt;
&lt;p&gt;We can use a mechanism called &lt;strong&gt;distillation:&lt;/strong&gt; training the student to match the output distribution of a teacher model. We train on teacher &lt;strong&gt;trajectories:&lt;/strong&gt; the complete sequence of generated tokens including intermediate thinking steps. We can use the teacher’s full next-token distribution at each step (often called “logit distillation”) or just sample given sequences. In practice, sampling sequences provides an unbiased estimation of the teacher’s distribution and arrives at the same objective. The student updates towards each token in the sequence in proportion to how unlikely it was to generate that token itself, represented by darker color in the example below:&lt;/p&gt;
&lt;figure id="fig:off-policy-distillation"&gt;
&lt;picture style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"&gt;
&lt;source media="(max-width: 768px)" srcset="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/off-policy-distillation-mobile.svg"/&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/off-policy-distillation.svg" style="display: block; width: 100%; height: auto;"/&gt;
&lt;/picture&gt;
&lt;/figure&gt;
&lt;br/&gt;
&lt;p&gt;Distillation from large model teachers has proven effective in training small models to &lt;a href="https://crfm.stanford.edu/2023/03/13/alpaca.html"&gt;follow instructions&lt;/a&gt;,&lt;label class="sidenote-number" for="snalpaca"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://crfm.stanford.edu/2023/03/13/alpaca.html"&gt;Alpaca: A Strong, Replicable Instruction-Following Model&lt;/a&gt; (Taori et al, 2021)&lt;/span&gt; &lt;a href="https://arxiv.org/abs/2506.04178"&gt;reason on math and science&lt;/a&gt;,&lt;label class="sidenote-number" for="sn20"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2506.04178"&gt;OpenThoughts: Data Recipes for Reasoning Models&lt;/a&gt; (Guha et al, 2025)&lt;/span&gt; &lt;a href="https://arxiv.org/html/2501.00031v1"&gt;extract clinical information&lt;/a&gt;&lt;label class="sidenote-number" for="snmedical"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2501.00031"&gt;Distilling Large Language Models for Efficient Clinical Information Extraction&lt;/a&gt; (Vedula et al, 2025)&lt;/span&gt; from medical notes, and &lt;a href="https://arxiv.org/abs/2305.14233"&gt;engage in multi-turn chat dialogues&lt;/a&gt;.&lt;label class="sidenote-number" for="snmultiturnchat"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2305.14233"&gt;Enhancing Chat Language Models by Scaling High-quality Instructional Conversations&lt;/a&gt; (Ding et al, 2023)&lt;/span&gt; The distillation datasets used for these and other applications are often open-sourced and published.&lt;/p&gt;
&lt;p&gt;The drawback of off-policy training is that the student learns in contexts frequented by the teachers, not ones the student itself will often find itself in. This can cause compounding error: if the student makes an early mistake that the teacher never makes, it finds itself diverging ever farther from the states it observed in training. This problem becomes particularly acute when we care about the student’s performance on long sequences. To avoid this divergence, the student must learn to recover from its own mistakes.&lt;/p&gt;
&lt;p&gt;Another issue observed with off-policy distillation is that the student can learn to imitate the teacher’s style and confidence but &lt;a href="https://arxiv.org/abs/2305.15717"&gt;not necessarily its factual accuracy&lt;/a&gt;.&lt;label class="sidenote-number" for="snimitation"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2305.15717"&gt;The False Promise of Imitating Proprietary LLMs&lt;/a&gt; (Gudibande et al, 2023)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If you’re learning to play chess, on-policy RL is analogous to playing games with no coaching. The feedback of winning or losing a match is tied directly to your own play, but is received only once per match and doesn’t tell you which moves contributed most to the outcome. Off-policy distillation is analogous to watching a grandmaster playing — you observe extremely strong chess moves, but they are played in board states that a novice player will rarely find themselves in.&lt;/p&gt;
&lt;p&gt;We want to combine the on-policy relevance of RL with the dense reward signal of distillation. For learning chess, this would be a teacher that grades each of &lt;em&gt;your own&lt;/em&gt; moves on a scale from “blunder” to “brilliant”. For LLM post-training, it’s on-policy distillation.&lt;/p&gt;
&lt;figure id="fig:chess-screenshot"&gt;
&lt;img data-zoomable="" src="https://thinkingmachines.ai/blog/on-policy-distillation/images/chess.png"/&gt;
&lt;figcaption&gt;Screenshot from &lt;a href="https://www.chess.com/"&gt;chess.com&lt;/a&gt;. Each move is color-graded by an analysis engine, which labels moves as blunders (red), mistakes (orange), inaccuracies (yellow), or brilliant (blue).&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id="on-policy-distillation--best-of-both-worlds"&gt;On-policy distillation — best of both worlds&lt;/h2&gt;
&lt;p&gt;The core idea of on-policy distillation is to sample trajectories from the &lt;em&gt;student&lt;/em&gt; model and use a high-performing teacher to grade &lt;em&gt;each token&lt;/em&gt; of each trajectory. Returning to our math example above, on-policy distillation would score each step of the solution punishing the mistakes that caused the student to arrive at the wrong answer while reinforcing the ones that were executed correctly.&lt;/p&gt;
&lt;figure id="fig:on-policy-distillation"&gt;
&lt;picture style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"&gt;
&lt;source media="(max-width: 768px)" srcset="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/on-policy-distillation-mobile.svg"/&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/on-policy-distillation.svg" style="display: block; width: 100%; height: auto;"/&gt;
&lt;/picture&gt;
&lt;/figure&gt;
&lt;br/&gt;
&lt;p&gt;In this post, we explore the application of on-policy distillation for tasks such as training a model for math reasoning and training an assistant model that combines domain knowledge with instruction following. We apply on-policy distillation on models that have a foundation of capabilities from pre- and mid-training. We find that it is a cheap and powerful approach to post-training, combining the advantages of on-policy training with a dense reward signal.&lt;/p&gt;
&lt;table border="1" style="border-collapse: collapse; text-align: center;"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:center;"&gt;Method&lt;/th&gt;
&lt;th style="text-align:center;"&gt;Sampling&lt;/th&gt;
&lt;th style="text-align:center;"&gt;Reward signal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;span class="distillation-color-square blue"&gt;&lt;/span&gt;Supervised finetuning&lt;/td&gt;
&lt;td style="text-align:center;"&gt;off-policy&lt;/td&gt;
&lt;td style="text-align:center; background-color: #E2FAF3;"&gt;&lt;u&gt;&lt;b&gt;dense&lt;/b&gt;&lt;/u&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;span class="distillation-color-square light-blue"&gt;&lt;/span&gt;Reinforcement learning&lt;/td&gt;
&lt;td style="text-align: center; background-color: #E2FAF3;"&gt;&lt;u&gt;&lt;b&gt;on-policy&lt;/b&gt;&lt;/u&gt;&lt;/td&gt;
&lt;td style="text-align:center;"&gt;sparse&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;span class="distillation-color-square green"&gt;&lt;/span&gt;On-policy distillation&lt;/td&gt;
&lt;td style="text-align:center; background-color: #E2FAF3;"&gt;&lt;u&gt;&lt;b&gt;on-policy&lt;/b&gt;&lt;/u&gt;&lt;/td&gt;
&lt;td style="text-align:center; background-color: #E2FAF3;"&gt;&lt;u&gt;&lt;b&gt;dense&lt;/b&gt;&lt;/u&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Our work with on-policy distillation draws inspiration from &lt;a href="https://arxiv.org/abs/1011.0686"&gt;DAGGER&lt;/a&gt;,&lt;label class="sidenote-number" for="sndagger"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/1011.0686"&gt;A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning&lt;/a&gt; (Ross et al, 2010)&lt;/span&gt; an iterative SFT algorithm that includes teacher evaluations of student-visited states. It is also similar to &lt;a href="https://arxiv.org/abs/2305.20050"&gt;process reward modeling,&lt;/a&gt;&lt;label class="sidenote-number" for="snprocessreward"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2305.20050"&gt;Let’s Verify Step by Step&lt;/a&gt; (Lightman et al, 2023)&lt;/span&gt; an RL approach that scores every step in the student model’s chain-of-thought. We extend prior on-policy distillation work by &lt;a href="https://arxiv.org/abs/2306.13649"&gt;Agarwal et al,&lt;/a&gt;&lt;label class="sidenote-number" for="snagarwal"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2306.13649"&gt;On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes&lt;/a&gt; (Agarwal et al, 2023)&lt;/span&gt; &lt;a href="https://arxiv.org/abs/2306.08543"&gt;Gu et al,&lt;/a&gt;&lt;label class="sidenote-number" for="snminillm"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2306.08543"&gt;MiniLLM: Knowledge Distillation of Large Language Models&lt;/a&gt; (Gu et al, 2023)&lt;/span&gt; and &lt;a href="https://arxiv.org/abs/2505.09388"&gt;the Qwen3 team&lt;/a&gt;&lt;label class="sidenote-number" for="snqwen3tech"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2505.09388"&gt;Qwen3 Technical Report&lt;/a&gt; (Qwen Team, 2025)&lt;/span&gt;. Using the &lt;a href="https://thinkingmachines.ai/tinker/"&gt;Tinker training API&lt;/a&gt;, we replicate Qwen3’s result of achieving equivalent performance on reasoning benchmarks with on-policy distillation for a fraction the cost of RL.&lt;/p&gt;
&lt;h2 id="implementation"&gt;Implementation&lt;/h2&gt;
&lt;p&gt;You can follow along with each step of the implementation in the &lt;a href="https://github.com/thinking-machines-lab/tinker-cookbook/tree/main/tinker_cookbook/recipes/distillation"&gt;Tinker cookbook&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="loss-function-reverse-kl"&gt;Loss function: reverse KL&lt;/h3&gt;
&lt;p&gt;On-policy distillation can use a variety of loss functions for grading the student’s trajectories.&lt;label class="sidenote-number" for="snoptions"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;See Agarwal et al. for an analysis of various loss function choices.&lt;/span&gt; For simplicity, we choose the per-token reverse KL — the divergence between the student’s ($\pi_\theta$) and teacher’s ($\pi_\text{teacher}$) distribution for each token conditioned on the same prior trajectory:&lt;/p&gt;
$$\text{KL}\Bigl(\pi_\theta \lvert\rvert \pi_\text{teacher}\Bigr) = \mathbb{E}_{x \sim {\pi_\theta}} \Bigl[ \log \pi_\theta(x_{t+1} | x_{1..t}) - \log \pi_\text{teacher}(x_{t+1} | x_{1..t}) \Bigr]$$&lt;p&gt;Our reward function minimizes the reverse KL, which pushes the student to approximate the teacher’s behavior in every state the student finds itself in. When the student behaves identically to the teacher, reverse KL is zero. For simplicity, we use a discount factor of zero: at any given timestep,&lt;label class="sidenote-number" for="sndiscount"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;Although more mathematically correct, we do not find discount factors &amp;gt; 0 to improve performance in practice, so we choose zero for simplicity.&lt;/span&gt; the student only optimizes the immediate next token, with no consideration for future tokens.&lt;/p&gt;
&lt;p&gt;Reverse KL has natural synergy with RL, which generally optimizes a form of sequence-level reverse KL induced by the reward model. However, unlike most reward models in practice, the reverse KL is “unhackable” in the sense that low KL always corresponds to a high probability of desirable behavior from the teacher model’s point of view. Two other useful properties of reverse KL are that it is “mode seeking”&lt;label class="sidenote-number" for="snjang"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;See &lt;a href="https://blog.evjang.com/2016/08/variational-bayes.html"&gt;Eric Jang’s post&lt;/a&gt; for more discussion of mode seeking behaviors.&lt;/span&gt; — it learns one specific behavior (the teacher’s) instead of spreading its distribution across several suboptimal options — and it reduces &lt;a href="https://arxiv.org/abs/1506.03099"&gt;exposure bias&lt;/a&gt;.&lt;label class="sidenote-number" for="snexposure"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/1506.03099"&gt;Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks&lt;/a&gt; (Bengio et al, 2015). See Gu et al. for more discussion.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This approach offers significant compute savings. Since it doesn’t require a rollout to finish sampling to calculate the reward, we can use shorter or partial rollouts for training. Querying the teacher’s log probabilities also requires just a single forward pass from the larger model, while the trajectories are generated by the smaller and cheaper student.&lt;/p&gt;
&lt;p&gt;We do not require a separate reward or labeling model either. There could be advantages to combining distillation-based per-token rewards with sequence-level environment rewards; this is an interesting area for potential future research.&lt;/p&gt;
&lt;h3 id="illustration"&gt;Illustration&lt;/h3&gt;
&lt;p&gt;Below we see a real example of an incorrect student trajectory graded by a teacher. The example is from &lt;a href="https://simple-bench.com/"&gt;SimpleBench&lt;/a&gt;, and relies on the model making a key observation that the question’s premise is important: the correct answer is “B. 0” since ice cubes will melt in a frying pan. The student, &lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507"&gt;Qwen3-4B-Instruct-2507&lt;/a&gt;, incorrectly treats this as a pure math problem with no consideration of the physical context.&lt;/p&gt;
&lt;figure id="fig:example-kl-illustration" style="text-align: center;"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/example-kl-illustration.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;Example trajectory graded by a teacher model. Tokens in darker red correspond to a higher reverse KL.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Darker colors represent tokens that receive a higher penalty from the teacher model, &lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;Qwen3-235B-A22B-Instruct-2507&lt;/a&gt;, which solves this problem correctly. We see that it penalizes tokens that start phrases which lead the student astray, intuitively corresponding to important &lt;a href="https://arxiv.org/abs/2506.01939"&gt;“forking tokens”&lt;/a&gt; for guiding reasoning.&lt;label class="sidenote-number" for="snforking"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2506.01939"&gt;Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning&lt;/a&gt; (Wang et al, 2025)&lt;/span&gt; The final answer, though wrong, isn’t penalized — it is entirely predictable conditional on the whole preceding sequence.&lt;/p&gt;
&lt;h3 id="pseudocode"&gt;Pseudocode&lt;/h3&gt;
&lt;p&gt;We implement on-policy distillation on top of the &lt;a href="https://github.com/thinking-machines-lab/tinker-cookbook/blob/main/tinker_cookbook/rl/train.py"&gt;RL script in Tinker&lt;/a&gt;, which already implements sampling, reward computation, and policy gradient-style training.&lt;label class="sidenote-number" for="snonel"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;Our implementation can actually be a one-line change on top of RL implementations which use KL regularization: we merely swap out the regularizer model.&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;i&gt;Initialize teacher client.&lt;/i&gt; The Tinker API enables easily creating different clients for different models, without needing to worry about the utilization of model engines. We use a sampling client, as we do not need to propagate logprobs through the teacher model.&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Sample trajectories.&lt;/i&gt; We sample rollouts from the student exactly as we would in RL. During sampling, RL already computes the student’s logprobs $\log \pi_\theta(x)$ for use as part of the &lt;a href="https://tinker-docs.thinkingmachines.ai/losses#policy-gradient-importance_sampling"&gt;importance-sampling&lt;/a&gt; loss.&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Compute reward.&lt;/i&gt; We query the teacher client with &lt;code&gt;compute_logprobs&lt;/code&gt; on the sampled trajectories, which returns the teacher’s logprobs $\log \pi_\text{teacher}(x)$ on the tokens $x$ sampled by the student.&lt;label class="sidenote-number" for="snlogit"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;In this post, we do not consider logit (top-k) distillation in any of our experiments, which could be used to further improve compute efficiency.&lt;/span&gt; We then use this to calculate the reverse KL.&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Train with RL.&lt;/i&gt; We set the per-token advantage to the negative reverse KL, and call the RL importance-sampling loss function to perform the training update on the student model.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre class="chroma" tabindex="0"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Initialize teacher client (main):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;teacher_client&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;service_client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create_sampling_client&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;    &lt;span class="n"&gt;base_model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;teacher_config&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;base_model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;    &lt;span class="n"&gt;model_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;teacher_config&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_checkpoint_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Sample trajectories (main):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;trajectories&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;do_group_rollout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;student_client&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;env_group_builder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;sampled_logprobs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;trajectories&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss_fn_inputs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"logprobs"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Compute reward (compute_teacher_reverse_kl):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;teacher_logprobs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;teacher_client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compute_logprobs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trajectories&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;reverse_kl&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sampled_logprobs&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;teacher_logprobs&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;trajectories&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"advantages"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;reverse_kl&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Train with RL (train_step):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;training_client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;forward_backward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trajectories&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"importance_sampling"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the experiments below, we generally apply on-policy distillation to models that have already been mid-trained on specific domain knowledge. This training increases the probability that the student will generate tokens within the teacher’s distribution, though it’s usually far from sufficient for replicating the teacher’s performance.&lt;label class="sidenote-number" for="snsupport"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;SFT, using forwards KL, adds &lt;a href="https://en.wikipedia.org/wiki/Support_(mathematics)"&gt;support&lt;/a&gt; for new tokens. Reverse-KL methods can then perform mode seeking within the initialization’s support.&lt;/span&gt; Often, as we’ll see in our personalization example, the probability of generating the relevant tokens starts at zero as the student lacks any relevant domain knowledge.&lt;/p&gt;
&lt;p&gt;We use on-policy distillation for post-training, and compare it to other approaches to this last crucial stage of training expert models.&lt;/p&gt;
&lt;h2 id="distillation-for-reasoning"&gt;Distillation for reasoning&lt;/h2&gt;
&lt;p&gt;We use distillation to train mathematical reasoning in the &lt;a href="https://huggingface.co/Qwen/Qwen3-8B-Base"&gt;Qwen3-8B-Base&lt;/a&gt; model, using &lt;a href="https://huggingface.co/Qwen/Qwen3-32B"&gt;Qwen3-32B&lt;/a&gt; as a teacher model. Both the teacher (Qwen3-32B) and student (Qwen3-8B-Base) are &lt;a href="https://tinker-docs.thinkingmachines.ai/model-lineup"&gt;supported models&lt;/a&gt; on Tinker today, so you can reproduce our experiments with the Tinker cookbook.&lt;/p&gt;
&lt;h3 id="off-policy-distillation"&gt;Off-policy distillation&lt;/h3&gt;
&lt;p&gt;As mentioned above, all our experiments start with mid-training in the form of off-policy distillation — supervised fine-tuning on a dataset of teacher-generated examples. The dataset used for mathematical reasoning is &lt;a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts3-1.2M"&gt;OpenThoughts-3&lt;/a&gt;, a collection of reasoning prompts and responses generated by &lt;a href="https://huggingface.co/Qwen/QwQ-32B"&gt;QwQ-32B&lt;/a&gt; (a reasoning model similar to Qwen3-32B).&lt;/p&gt;
&lt;p&gt;Training the student (Qwen3-8B-Base) on 400k prompts with full fine-tuning achieves a score of 60% on AIME'24, a benchmark of math problems. We can also train with LoRA,&lt;label class="sidenote-number" for="sn10"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2106.09685"&gt;LoRA: Low-Rank Adaptation of Large Language Models&lt;/a&gt; (Hu et al, 2021)&lt;/span&gt; though it lags behind full fine-tuning when training on high-volume datasets. In all cases, we see performance increase log-linearly — the initial performance gains are cheap but the latter are costly.&lt;/p&gt;
&lt;figure id="fig:experiment-off-policy-distillation"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/experiment-off-policy-distillation.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;AIME'24 score over the course of off-policy distillation (SFT). After the initial 50-100K prompts, performance follows a predictable log-linear scaling curve. As predicted in &lt;a href="https://thinkingmachines.ai/blog/lora/"&gt;LoRA Without Regret&lt;/a&gt;, we observe worse LoRA performance when running large-scale SFT with high batch size.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We can treat the model fine-tuned on 400k prompts as a checkpoint before trying various post-training approaches to increase its performance. We can compare the effort it would take to raise the score on the AIME’24 benchmark from 60% to 70%.&lt;/p&gt;
&lt;p&gt;The default approach is to fine-tune on more prompts, continuing the process of off-policy distillation. Extrapolating the log-linear trend, we estimate that the model would achieve 70% on AIME’24 at approximately 2M prompts. This extrapolation requires the scaling law to hold up without stalling, which isn’t trivial. However, there are examples of large-scale off-policy distillation improving an 8B model’s performance past 70%, such as &lt;a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts3-1.2M"&gt;OpenThoughts-3&lt;/a&gt; and &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"&gt;DeepSeek-R1-0528-Qwen3-8B&lt;/a&gt;.&lt;label class="sidenote-number" for="sndeepseekdistill"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;DeepSeek-R1-0528-Qwen3-8B achieves 86% on the benchmark, with the number of training prompts not specified. Older models (Qwen2.5-7B, Qwen2.5-14B) &lt;a href="https://arxiv.org/abs/2501.12948"&gt;achieve performance of 55.5% and 69.7%&lt;/a&gt;, respectively, after training on 800k distillation prompts from DeepSeek-R1.&lt;/span&gt; We can use this extrapolation as an optimistic estimate for the cost-performance ratio of off-policy distillation.&lt;/p&gt;
&lt;h3 id="reinforcement-learning"&gt;Reinforcement learning&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://arxiv.org/pdf/2505.09388"&gt;Qwen3 technical report&lt;/a&gt; reaches performance of 67.6% on the benchmark using 17,920 GPU hours of RL on top of a similar SFT initialization. It is hard to compare this directly to the cost of distillation, but given some reasonable assumptions about the SFT training stack, this is similar to the cost of training on 2M off-policy distillation prompts.&lt;/p&gt;
&lt;table border="1" style="border-collapse: collapse; text-align: center;"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:center;"&gt;Method&lt;/th&gt;
&lt;th style="text-align:center;"&gt;AIME’24&lt;/th&gt;
&lt;th style="text-align:center;"&gt;GPQA-Diamond&lt;/th&gt;
&lt;th style="text-align:center;"&gt;GPU Hours&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;span class="distillation-color-square blue"&gt;&lt;/span&gt;Off-policy distillation&lt;/td&gt;
&lt;td style="text-align:center;"&gt;55.0%&lt;/td&gt;
&lt;td style="text-align:center;"&gt;55.6%&lt;/td&gt;
&lt;td style="text-align:center;"&gt;Unreported&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;span class="distillation-color-square light-blue"&gt;&lt;/span&gt;+ Reinforcement learning&lt;/td&gt;
&lt;td style="text-align:center;"&gt;67.6%&lt;/td&gt;
&lt;td style="text-align:center;"&gt;61.3%&lt;/td&gt;
&lt;td style="text-align:center;"&gt;17,920&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;span class="distillation-color-square green"&gt;&lt;/span&gt;&lt;b&gt;+ On-policy distillation&lt;/b&gt;&lt;/td&gt;
&lt;td style="text-align:center;"&gt;&lt;b&gt;74.4%&lt;/b&gt;&lt;/td&gt;
&lt;td style="text-align:center;"&gt;&lt;b&gt;63.3%&lt;/b&gt;&lt;/td&gt;
&lt;td style="text-align:center;"&gt;&lt;b&gt;1,800&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p style="text-align: center; font-style: italic; margin-top: 0.5em; margin-bottom: 1em;"&gt;From &lt;a href="https://arxiv.org/abs/2505.09388"&gt;Qwen3 Technical Report&lt;/a&gt;, Table 21.&lt;/p&gt;
&lt;p&gt;The Qwen team also reports reaching a higher score of 74.4 on AIME’24 at one-tenth the cost of RL with on-policy distillation, which served as inspiration for our work. We attempt to replicate it below in our basic setup.&lt;/p&gt;
&lt;h3 id="on-policy-distillation"&gt;On-policy distillation&lt;/h3&gt;
&lt;p&gt;As an alternative to off-policy distillation or RL, we run on-policy distillation as described above.&lt;label class="sidenote-number" for="snteacher"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;We actually use Qwen3-8B as a teacher, as it performs slightly better. However, for the purpose of comparing compute, we can still measure the FLOPs of the 32B model.&lt;/span&gt; Starting from the 400k SFT checkpoint, on-policy distillation achieves an AIME’24 of 70% in about 150 steps.&lt;label class="sidenote-number" for="sndprompts"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;150 steps corresponds to about 77K prompts; we train with 4 samples per prompt.&lt;/span&gt;&lt;/p&gt;
&lt;figure id="fig:experiment-on-policy-distillation-loras" style="text-align: center;"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/experiment-on-policy-distillation-loras.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;AIME'24 over the course of on-policy distillation. We measure additonal compute in terms of training FLOPs (see below). On-policy distillation is significantly more compute-efficient than SFT, especially for LoRA models. At rank = 32, LoRA trails full finetuning by 13% after SFT, but only 6% after on-policy distillation.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Comparing compute costs across methods is nontrivial, as the ratio of training vs sampling vs log-prob computation cost varies significantly depending on implementation. Below, we compute the cost in terms of FLOPs which penalizes methods that can be effectively parallelized on GPUs. In particular, it overestimates the practical cost of computing log-probs.&lt;/p&gt;
&lt;table border="1" style="border-collapse: collapse;"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:center;"&gt;Method&lt;/th&gt;
&lt;th style="text-align:center;"&gt;AIME’24&lt;/th&gt;
&lt;th style="text-align:center;"&gt;Teacher FLOPs&lt;/th&gt;
&lt;th style="text-align:center;"&gt;Student FLOPs&lt;/th&gt;
&lt;th style="text-align:center;"&gt;CE vs SFT-2M&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;&lt;i&gt;Initialization: SFT-400K&lt;/i&gt;&lt;/td&gt;
&lt;td style="text-align:center;"&gt;60%&lt;/td&gt;
&lt;td style="text-align:center;"&gt;8.5 × 10&lt;sup&gt;20&lt;/sup&gt;&lt;/td&gt;
&lt;td style="text-align:center;"&gt;3.8 × 10&lt;sup&gt;20&lt;/sup&gt;&lt;/td&gt;
&lt;td style="text-align:center;"&gt;–&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;&lt;span class="distillation-color-square blue"&gt;&lt;/span&gt;SFT-2M (extrapolated)&lt;/td&gt;
&lt;td style="text-align:center;"&gt;~70% (extrapolated)&lt;/td&gt;
&lt;td style="text-align:center;"&gt;3.4 × 10&lt;sup&gt;21&lt;/sup&gt;&lt;/td&gt;
&lt;td style="text-align:center;"&gt;1.5 × 10&lt;sup&gt;21&lt;/sup&gt;&lt;/td&gt;
&lt;td style="text-align:center;"&gt;1×&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;&lt;span class="distillation-color-square light-blue"&gt;&lt;/span&gt;Reinforcement learning&lt;/td&gt;
&lt;td style="text-align:center;"&gt;68%&lt;/td&gt;
&lt;td style="text-align:center;"&gt;-&lt;/td&gt;
&lt;td style="text-align:center;"&gt;-&lt;/td&gt;
&lt;td style="text-align:center;"&gt;≈1×&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;&lt;span class="distillation-color-square green"&gt;&lt;/span&gt;On-policy distillation&lt;/td&gt;
&lt;td style="text-align:center;"&gt;70%&lt;/td&gt;
&lt;td style="text-align:center;"&gt;8.4 × 10&lt;sup&gt;19&lt;/sup&gt;&lt;/td&gt;
&lt;td style="text-align:center;"&gt;8.2 × 10&lt;sup&gt;19&lt;/sup&gt;&lt;/td&gt;
&lt;td style="text-align:center;"&gt;&lt;b&gt;&lt;u&gt;9-30×&lt;/u&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We find a baseline cost reduction of 9x when the SFT dataset is given, as in our example with OpenThoughts-3, or is amortized across many training runs.&lt;label class="sidenote-number" for="snceformula"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;CE = (&lt;span class="distillation-color-square green"&gt;&lt;/span&gt;Student + &lt;span class="distillation-color-square green"&gt;&lt;/span&gt;Teacher) / (&lt;span class="distillation-color-square blue"&gt;&lt;/span&gt;Student)&lt;/span&gt; In this case we do not count the cost of teacher FLOPs for off-policy training but do for on-policy, since we must run the teacher model to compute log-probs for the student’s trajectory. Since this computation can be cheaply parallelized across GPUs, the cost reduction in GPU hours is closer to 18x.&lt;/p&gt;
&lt;p&gt;However, we often want to train a small model on a new task for which no off-policy distillation dataset is available. If we include the full cost of the teacher model in off-policy distillation – ie, including the additional cost of sampling from the teacher model – the total cost reduction is about 30x.&lt;label class="sidenote-number" for="snceformula2"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;CE = (&lt;span class="distillation-color-square green"&gt;&lt;/span&gt;Student + &lt;span class="distillation-color-square green"&gt;&lt;/span&gt;Teacher) / (&lt;span class="distillation-color-square blue"&gt;&lt;/span&gt;Student + &lt;span class="distillation-color-square blue"&gt;&lt;/span&gt;Teacher)&lt;/span&gt;&lt;/p&gt;
&lt;h2 id="distillation-for-personalization"&gt;Distillation for personalization&lt;/h2&gt;
&lt;p&gt;In addition to training small models to high performance on common tasks, another use-case for distillation is personalization. Examples include adhering to a particular tone in conversation and format of output, or capabilities like tool use and cost budgeting. We often want to train this behavior in combination with new domain knowledge.&lt;/p&gt;
&lt;p&gt;Training both at once is generally difficult, and light-weight finetunes are often insufficient for this objective,&lt;label class="sidenote-number" for="snmidtrain"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2403.05612"&gt;Unfamiliar Finetuning Examples Control How Language Models Hallucinate&lt;/a&gt; (Kang et al, 2024)&lt;/span&gt; thereby requiring a larger midtrain. Learning post-training behaviors on top of new knowledge requires a complex post-training stack, often consisting of proprietary data and reward models. While this approach is accessible to frontier labs, it can be difficult or prohibitively expensive for other practitioners to replicate.&lt;/p&gt;
&lt;p&gt;In this section, we show that on-policy distillation can be used effectively to post-train specialized behavior. This approach is also applicable to continual learning or “test-time training”: updating models when they are deployed without regressing the base performance. We use an example application of a model mid-trained on our internal company documents.&lt;/p&gt;
&lt;h3 id="training-an-internal-assistant"&gt;Training an internal assistant&lt;/h3&gt;
&lt;p&gt;A common objective for a custom model is to act as an assistant: to possess expert knowledge in some field and, in addition, reliable assistant-like behavior. We may need separate training for each, especially when the domain of expertise can’t be learned from pre-training data only or when learning it interferes with behavior.&lt;/p&gt;
&lt;p&gt;Our example is an internal company assistant, for which we have two desiderata:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The model is &lt;strong&gt;knowledgeable&lt;/strong&gt; about the domain (company documents). Pretrained models have seen zero internal documents from the company, and therefore can only guess, regardless of model scale. We will measure this using an internal knowledge recall eval (“internal QA”).&lt;/li&gt;
&lt;li&gt;The model exhibits strong &lt;strong&gt;post-training&lt;/strong&gt; behavior, ie. instruction following. We will measure this with the commonly-used &lt;a href="https://arxiv.org/abs/2311.07911"&gt;IF-eval&lt;/a&gt;.&lt;label class="sidenote-number" for="snifeval"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2311.07911"&gt;Instruction-Following Evaluation for Large Language Models&lt;/a&gt; (Zhou et al, 2023)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="training-on-new-knowledge-degrades-learned-behavior"&gt;Training on new knowledge degrades learned behavior&lt;/h3&gt;
&lt;p&gt;We will start with Qwen3-8B, rather than the base model. Qwen3-8B is post-trained on useful skills for an assistant, such as instruction following and reasoning with RL. Prior research has shown that such reinforcement learning only trains small subnetworks of the original model,&lt;label class="sidenote-number" for="snmukherjee2025"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2505.11711"&gt;Reinforcement Learning Finetunes Small Subnetworks in Large Language Models&lt;/a&gt; (Mukherjee et al, 2025)&lt;/span&gt; and thus can be fragile when the network is further trained on a large amount of data. We study the extent to which it happens, and how the desired behavior can be recovered.&lt;/p&gt;
&lt;p&gt;In order to reduce such catastrophic forgetting, a common approach in mid-training is to mix in “background data” from the original model’s pretraining distribution.&lt;label class="sidenote-number" for="snmidtrain"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2510.14865"&gt;Midtraining Bridges Pretraining and Posttraining Distributions&lt;/a&gt; (Liu et al, 2025)&lt;/span&gt; In this case, we don’t have access to Qwen3’s pretraining distribution. Therefore, we consider a stronger and more expensive baseline: we take &lt;a href="https://huggingface.co/datasets/allenai/tulu-3-sft-mixture"&gt;Tulu3&lt;/a&gt;&lt;label class="sidenote-number" for="sntulu"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2411.15124"&gt;Tulu 3: Pushing Frontiers in Open Language Model Post-Training&lt;/a&gt; (Ivison et al, 2024)&lt;/span&gt; prompts – a broad chat and instruction-following dataset – and re-sample them with Qwen3-8B in order to act as chat background data.&lt;/p&gt;
&lt;p&gt;This “on-policy” background data sampled by Qwen3-8B acts as a forwards KL regularizer, reinforcing the model’s original behavior throughout mid-training. We find sampling from Qwen3-8B is better than Qwen3-32B for preserving chat capabilities throughout mid-training, highlighting the sensitivity of the data source; similar on-policy SFT results have been found in Chen et al.&lt;label class="sidenote-number" for="snchenforget"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2510.18874"&gt;Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting&lt;/a&gt; (Chen et al, 2025)&lt;/span&gt; We hypothesize this approach can be even more effective than having access to the original pretraining data distribution, at the cost of having to sample a large-scale dataset.&lt;/p&gt;
&lt;p&gt;We then fine-tune Qwen3-8B on different mixes of our internal documents and chat data. Increasing the proportion of document data straightforwardly improves the model’s knowledge. However, although mixing in at least 30% of chat data helps preserve most instruction-following ability, there is no weighting which maintains the original performance on IF-eval.&lt;label class="sidenote-number" for="snifevalperf"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;This is true even if the SFT dataset contains 100% chat data. We address this further in the discussion on continual learning.&lt;/span&gt;&lt;/p&gt;
&lt;figure id="fig:experiment-personalization-midtrain" style="text-align: center;"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/experiment-personalization-midtrain.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;Sweeping over the ratio of internal documents: background chat data during mid-training. Although mixing in a little chat data helps to prevent a catastrophic regression, no weight maintains the original IF-eval performance.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;For any given mix, we observe IF-eval performance degrades during fine-tuning. This compromises our ability to use longer training to help specialize the model further.&lt;label class="sidenote-number" for="snoverparametrized"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;Directionally, we may expect an overparametrized model trained on some dataset to update its behavior only the context of the dataset without impacting what it does in other contexts. However, we do not observe this in practice, as training on raw document data regresses performance even in the QA context.&lt;/span&gt;&lt;/p&gt;
&lt;figure id="fig:experiment-midtrain-if-eval" style="text-align: center;"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/experiment-midtrain-if-eval.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;IF-eval decreases during midtraining across all mixes of data. When we use a linear learning rate (pictured), the degradation eventually flattens and slowly begins to recover as the learning rate decays. However, performance never fully recovers.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;An alternative commonly-used approach is to use LoRA in order to constrain the parameter update, thereby reducing the possibility of catastrophic forgetting. However, this approach is still insufficient for preserving IF-eval, and the LoRA learns less.&lt;label class="sidenote-number" for="snbiderman"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2405.09673"&gt;LoRA Learns Less and Forgets Less&lt;/a&gt; (Biderman et al, 2024)&lt;/span&gt;&lt;/p&gt;
&lt;figure id="fig:experiment-midtrain-lora" style="text-align: center;"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/experiment-midtrain-lora.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;When used for our personalization midtrain on top of the post-trained Qwen3-8B, LoRA learns less (knowledge) and still forgets its original post-training behaviors.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id="on-policy-distillation-recovers-post-training-behavior"&gt;On-policy distillation recovers post-training behavior&lt;/h3&gt;
&lt;p&gt;Next, we seek to restore instruction-following behavior after fine-tuning on internal documents. This behavior was originally trained with RL which is expensive and, as we have seen, fragile. Instead, we run on-policy distillation with the earlier version of the model, Qwen3-8B, as the teacher on &lt;a href="https://huggingface.co/datasets/allenai/tulu-3-sft-mixture"&gt;Tulu3&lt;/a&gt; prompts. Note that this phase of training has no relation to the internal document data, and is designed solely to recover instruction following.&lt;/p&gt;
&lt;p&gt;The use of an earlier version of the model as a teacher to “re-invoke” capabilities lost during fine-tuning makes on-policy distillation very promising for continuous learning. We could alternate between phases of fine-tuning on new data and distillation to recover behavior to allow our model to learn and stay up-to-date on knowledge over time. This phase-alternating approach has previously been explored by Cobbe et al.&lt;label class="sidenote-number" for="snbiderman"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2009.04416"&gt;Phasic Policy Gradient&lt;/a&gt; (Cobbe et al, 2020)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After fine-tuning on an 70-30 mix of internal document data and chat data, on-policy distillation recovers nearly full performance on IF-eval without losing any knowledge; we also observe some positive transfer between chat capabilities and the model’s “knowledge” performance on the internal QA eval.&lt;/p&gt;
&lt;table border="1" style="border-collapse: collapse;"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:center;"&gt;Model&lt;/th&gt;
&lt;th style="text-align:center;"&gt;Internal QA Eval (Knowledge)&lt;/th&gt;
&lt;th style="text-align:center;"&gt;IF-eval (Chat)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;&lt;i&gt;Qwen3-8B&lt;/i&gt;&lt;/td&gt;
&lt;td style="text-align:center;"&gt;18%&lt;/td&gt;
&lt;td style="text-align:center; background-color: #E2FAF3;"&gt;&lt;b&gt;&lt;u&gt;85%&lt;/u&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;&lt;i&gt;+ midtrain (100%)&lt;/i&gt;&lt;/td&gt;
&lt;td style="text-align:center; background-color: #E2FAF3;"&gt;&lt;u&gt;&lt;b&gt;43%&lt;/b&gt;&lt;/u&gt;&lt;/td&gt;
&lt;td style="text-align:center;"&gt;45%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;&lt;span class="distillation-color-square blue"&gt;&lt;/span&gt;+ midtrain (70%)&lt;/td&gt;
&lt;td style="text-align:center;"&gt;36%&lt;/td&gt;
&lt;td style="text-align:center;"&gt;79%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;&lt;span class="distillation-color-square green"&gt;&lt;/span&gt;+ midtrain (70%) + distill&lt;/td&gt;
&lt;td style="text-align:center; background-color: #E2FAF3;"&gt;&lt;b&gt;&lt;u&gt;41%&lt;/u&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td style="text-align:center; background-color: #E2FAF3;"&gt;&lt;b&gt;&lt;u&gt;83%&lt;/u&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;
&lt;/table&gt;
&lt;p style="text-align: center; font-style: italic; margin-top: 0.5em; margin-bottom: 1em;"&gt;
Domain-specific (Internal QA eval) and chat (IF-eval) performance after mid-training. Although mid-training forgets the post-trained behaviors of Qwen3-8B, they are cheaply restored via on-policy distillation, alongside the additional knowledge learned via the mid-train.
&lt;p&gt;In essence, we have treated the language model itself as a reward model, with high-probability behaviors being rewarded.&lt;label class="sidenote-number" for="sndpo"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2305.18290"&gt;Direct Preference Optimization: Your Language Model is Secretly a Reward Model&lt;/a&gt; (Rafailov et al, 2023)&lt;/span&gt; This has connections to inverse RL: high-probability behaviors correspond to advantageous rewards in an assumed underlying preference model.&lt;label class="sidenote-number" for="snirl"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://ai.stanford.edu/~ang/papers/icml00-irl.pdf"&gt;Algorithms for Inverse Reinforcement Learning&lt;/a&gt; (Ng and Russell, 2000)&lt;/span&gt; Any instruction-tuned open-weight model can be used as a reward model in this sense; we merely need access to the &lt;code&gt;compute_logprobs&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;Distillation as a tool for integrating behaviors and knowledge has also been explored for hybrid reasoning models (&lt;a href="https://arxiv.org/abs/2505.09388"&gt;Qwen3&lt;/a&gt;) and specialist distillation.&lt;label class="sidenote-number" for="snspecialist"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf"&gt;DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention&lt;/a&gt; (DeepSeek-AI Team, 2025)&lt;/span&gt; As our and &lt;a href="https://arxiv.org/abs/2510.18874"&gt;Chen et al.’s&lt;/a&gt; results suggest, on-policy learning can be a crucial tool for augmenting similar distillation-based “model merging” setups.&lt;/p&gt;
&lt;h2 id="discussion"&gt;Discussion&lt;/h2&gt;
&lt;h3 id="dense-supervision-greatly-improves-compute-efficiency"&gt;Dense supervision greatly improves compute efficiency&lt;/h3&gt;
&lt;p&gt;Reinforcement learning and on-policy distillation both learn via reverse KL, pruning the space of actions present in the base policy. The difference is in the density of reward. In &lt;a href="https://thinkingmachines.ai/blog/lora/"&gt;LoRA Without Regret&lt;/a&gt; we presented the information-theoretic perspective that reinforcement learning only teaches $O(1)$ bits per episode. In contrast, distillation teaches $O(N)$ bits per episode, where $N$ is the number of tokens. Can we quantify the training efficiency gained through denser reward?&lt;/p&gt;
&lt;p&gt;We ran an experiment for making a direct comparison between the two:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Start with Qwen3-8B-Base (no additional SFT).&lt;/li&gt;
&lt;li&gt;Run RL on DeepMath, matching our procedure from &lt;a href="https://thinkingmachines.ai/blog/lora/"&gt;LoRA Without Regret&lt;/a&gt;. We use a LoRA rank of 128. The resultant model is the teacher for distillation.&lt;/li&gt;
&lt;li&gt;On-policy distill from the RL-trained model (2) back into the base model (1).&lt;/li&gt;
&lt;/ol&gt;
&lt;figure id="fig:experiment-self-distillation" style="text-align: center;"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/experiment-self-distillation.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;Starting from the same initialization, on-policy distillation can learns the RL-trained policy in approximately 7-10x fewer gradient steps, which corresponds to a compute efficiency of 50-100x.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We see that distillation reaches the teacher’s level of performance approximately 7-10x faster than RL with matched model architecture (LoRA rank 128). The reverse KL decreases to near-zero and the AIME score is recovered in under 10 gradient steps, while RL took 70 steps to reach that level.&lt;/p&gt;
&lt;p&gt;Cumulatively, the reduction in compute required is on the order of 50-100x:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;While RL requires training at approximately the evaluation context (in order for the policy to learn the context limit and not incur format penalty), distillation learns reasonably at shorter context lengths, as there is no sharp cutoff in reward between a trajectory which has finished sampling and a trajectory that continues.&lt;/li&gt;
&lt;li&gt;When the SFT initialization is strong,&lt;label class="sidenote-number" for="snbs"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;ie: when the teacher policy is within the support of the student policy. When this is not true, as in “Distillation for reasoning”, we require a significantly larger batch size.&lt;/span&gt; on-policy distillation works effectively with much smaller batch sizes since it provides significantly more bits per episode, thereby reducing gradient noise.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although it is generally difficult to train reinforcement learning models with process supervision, these results indicate that as a broad direction, process supervision and dense rewards have the potential to improve learning efficiency by an order of magnitude. This matches earlier results in RL research from Lightman et al.&lt;/p&gt;
&lt;h3 id="distillation-can-effectively-reuse-training-data-for-data-efficiency"&gt;Distillation can effectively reuse training data for data efficiency&lt;/h3&gt;
&lt;p&gt;For practitioners, collecting large datasets of training prompts can be difficult and time-consuming. Therefore, we want to be able to reuse prompts several times in training. With RL, training multiple epochs on the same prompt often leads to simple memorization of the final answer, especially with large models.&lt;label class="sidenote-number" for="snleviathan2023"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2504.20571"&gt;“Reinforcement Learning for Reasoning in Large Language Models with One Training Example”&lt;/a&gt; (Wang et al, 2025) presents a positive result, however, in some settings.&lt;/span&gt; In contrast, on-policy distillation learns to approximate the teacher’s complete distribution by minimizing reverse KL, rather than memorizing a single answer. This allows us to train many samples from the same prompt.&lt;/p&gt;
&lt;p&gt;We repeat the above experiment of training Qwen3-8B-Base on math, but now with only a single randomly chosen prompt from the dataset.&lt;label class="sidenote-number" for="snsingle"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;Prompt: “Evaluate the limit: $\lim_{x \to \infty} \sqrt{x} \left( \sqrt[3]{x+1} - \sqrt[3]{x-1} \right)$"&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We train on this prompt for 20 consecutive steps, each with a batch of 256 rollouts, for 5120 graded sequences in total. We train on the same prompt for multiple steps in sequential fashion, which normally leads to overfitting. Though this is naturally less compute-efficient, we do approximately match the performance of the teacher model despite only training on a single prompt.&lt;/p&gt;
&lt;figure id="fig:experiment-self-distillation-multiepoch" style="text-align: center;"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/experiment-self-distillation-multiepoch.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;In this example, multi-epoch training on top of one training example is sufficient to distill the teacher's AIME'24 performance. Our default configuration (also used in the personalization experiments) runs on-policy distillation with 64 prompts / batch, and 4 samples / prompt. All methods shown are trained with 256 samples / batch. Note that the right chart is showing training KL, hence it is natural for 1 prompt total to outperform 1 prompt / batch.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id="rl-searches-in-the-space-of-semantic-strategies"&gt;RL searches in the space of semantic strategies&lt;/h3&gt;
&lt;p&gt;We have seen that on-policy distillation can replicate the learning provided by RL with much fewer steps of training. One interpretation of this result is that, unlike pre-training, RL doesn’t spend a lot of compute on the gradient steps themselves. We should think of RL as spending most of its compute on &lt;em&gt;search&lt;/em&gt; — rolling out a policy and assigning credit — rather than on making updates.&lt;label class="sidenote-number" for="snbitterlessonmentioned"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;From &lt;a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"&gt;The Bitter Lesson&lt;/a&gt; (Rich Sutton): “breakthrough progress eventually arrives by an opposing approach based on scaling computation by &lt;b&gt;search&lt;/b&gt; and &lt;b&gt;learning&lt;/b&gt;"&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Pre-training via stochastic gradient descent is exploring the high-dimensional parameter space. Pre-training requires a vast amount of information and is very difficult to distill, in part because the parameter space is somewhat unique to each network.&lt;label class="sidenote-number" for="sngudibande"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/1803.03635"&gt;“The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks”&lt;/a&gt; (Frankle and Carbin, 2018)&lt;/span&gt; The gradient steps required for pretraining are extremely computationally expensive and time-consuming.&lt;/p&gt;
&lt;p&gt;In contrast, we should think of RL as exploring the space of semantic strategies.&lt;label class="sidenote-number" for="snexploration"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;Note that exploration over strategies is subtlely different from exploration over outcomes; RL requires the base model to have nonzero success to start with, and therefore has already “found the outcome”, but the strategy can be refined over the course of RL to make successful outcomes more likely.&lt;/span&gt; At every step, RL tries a small modification of some strategy it has found in the past. Rather than exploring in the parameter space, it “stumbles” onto new strategies by luck — it is randomly sampling from the set of weights it already has.&lt;/p&gt;
&lt;p&gt;Once a good strategy is found, distillation serves as a shortcut for learning it: on-policy distillation does not need to model the intermediate strategies during the curriculum of RL, but rather only the final strategy learned. If we are only interested in the final strategy (common in production use-cases), we need not spend the compute to model all the intermediate ones.&lt;/p&gt;
&lt;p&gt;Consider an analogy: in science research, we spend a lot of time and resources looking for answers and exploring new ideas. Once a result is discovered, it is much simpler to teach it to others by expressing it in natural language. We can contrast this to intuitive physical skills such as playing a sport. They are much harder to teach to others, since the knowledge exists in an innate language (e.g., muscle memory) that is only readily understood by ourselves. Sports are only learned with repeated practice.&lt;/p&gt;
&lt;h3 id="on-policy-learning-as-a-tool-for-continual-learning"&gt;On-policy learning as a tool for continual learning&lt;/h3&gt;
&lt;p&gt;In the section on distillation for personalization, we explored the ability of on-policy distillation to re-introduce specialized trained behaviors into the model. This generalizes to a broader set of continual learning tasks, which require acquiring new knowledge without degrading prior capabilities.&lt;/p&gt;
&lt;p&gt;Prior work has found on-policy learning (RL) forgets less than off-policy learning.&lt;label class="sidenote-number" for="snrazor"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2509.04259"&gt;“RL’s Razor: Why Online Reinforcement Learning Forgets Less”&lt;/a&gt; (Shenfeld et al, 2025)&lt;/span&gt; However, RL only shapes behavior — it cannot teach new knowledge well, and thus can’t be sufficient for continual learning.&lt;/p&gt;
&lt;p&gt;In the above section, we saw that SFT (including off-policy distillation) fails at scaffolding continual learning because it degrades behavior. We investigate this more closely and demonstrate this with a direct example. Similarly to above, we build a dataset by taking Tulu3 prompts and sampling from Qwen3-32B at &lt;code&gt;temperature = 1.0&lt;/code&gt; and no further modifications. This dataset therefore has a KL of zero against Qwen3-32B.&lt;label class="sidenote-number" for="kl0"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;The importance of “truly on-policy” KL=0 data was also explored in our previous post, &lt;a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/"&gt;Defeating Nondeterminism in LLM Inference&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What happens when we run SFT on this dataset of a model’s own samples? We see that any practical learning rate that’s greater than zero leads to a degradation of performance on the instruction-following evaluation!&lt;/p&gt;
&lt;figure id="fig:experiment-sft-on-policy" style="text-align: center;"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/on-policy-distillation/svgs/experiment-sft-on-policy.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;Running SFT on top of Qwen3-32B's own samples degrades performance. We use the same learning rate as from the personalization section, which was swept for practical performance considerations. A linear learning rate can prevent forwards KL / IF-eval from regressing indefinitely, but does not recover performance before the LR decays to zero.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;A possible explanation for this is that while KL divergence is 0 in expectation, every finite batch will exhibit a slightly different distribution in practice. Training on these finite batches causes a non-zero gradient update, which then diverges the updated model’s policy from that of its original state. This process turns training on one’s own samples into off-policy training over time, which causes the same error accumulation and divergence over long sequences that happens with off-policy training.&lt;/p&gt;
&lt;p&gt;On-policy distillation always stays on-policy, and since the teacher stays fixed, the student converges on the teacher’s desirable behavior, without regressing in the self-distillation setting as SFT does. This makes on-policy distillation a very promising tool for continual learning.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We have explored the application of on-policy distillation for applications such as training a small model for math reasoning or a continuously-learning assistant. We compared on-policy distillation to two other approaches to post-training: off-policy distillation, and on-policy RL. We find that on-policy distillation combines the best of both worlds: the reliable performance of on-policy training, with the cost-efficiency of a dense reward signal.&lt;/p&gt;
&lt;p&gt;Post-training is a crucial part of reaching frontier model capabilities. By leveraging on-policy sampling from the student with dense supervision from a teacher, the on-policy distillation recipe reaches those capabilities at a fraction of the cost of frontier high-compute RL runs.&lt;/p&gt;
&lt;p&gt;Our implementation can be found in the &lt;a href="https://github.com/thinking-machines-lab/tinker-cookbook/tree/main/tinker_cookbook/recipes/distillation"&gt;Tinker cookbook&lt;/a&gt;. Our work explored simple and straightforward instantiations of on-policy distillation to clearly showcase its advantages. We hope to continue research on new applications of distillation, new methods for improving teacher supervision, and ways to improve data efficiency and continual learning.&lt;/p&gt;
&lt;p&gt;At Thinking Machines, our mission is to empower people with AI models that combine frontier performance with adaptability and personalization. On-policy distillation is a potent tool for achieving that.&lt;/p&gt;
&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;
&lt;p&gt;Please cite this work as:&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;Lu, Kevin and Thinking Machines Lab, "On-Policy Distillation",
Thinking Machines Lab: Connectionism, Oct 2025.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Or use the BibTeX citation:&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;@article{lu2025onpolicydistillation,
  author = {Kevin Lu and Thinking Machines Lab},
  title = {On-Policy Distillation},
  journal = {Thinking Machines Lab: Connectionism},
  year = {2025},
  note = {https://thinkingmachines.ai/blog/on-policy-distillation},
  doi = {10.64434/tml.20251026},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://thinkingmachines.ai/blog/on-policy-distillation/</guid>
      <category>Research</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Announcing Tinker</title>
      <link>https://thinkingmachines.ai/blog/announcing-tinker/</link>
      <description>TinkerToy Computer invented by Daniel Hillis and Brian Silverman</description>
      <content:encoded>&lt;article class="content"&gt;

&lt;p class="image-caption" style="text-align: center; margin-top: -1rem; margin-bottom: 2rem; font-size: 0.9rem; color: var(--fg-muted, #666);"&gt;
&lt;a href="https://www.computerhistory.org/collections/catalog/X39.81/" rel="noopener" target="_blank"&gt;TinkerToy Computer&lt;/a&gt; invented by &lt;a href="https://en.wikipedia.org/wiki/Danny_Hillis" rel="noopener" target="_blank"&gt;Daniel Hillis&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Brian_Silverman" rel="noopener" target="_blank"&gt;Brian Silverman&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Today, we are launching &lt;a href="https://thinkingmachines.ai/tinker"&gt;Tinker&lt;/a&gt;, a flexible API for fine-tuning language models. It empowers researchers and hackers to experiment with models by giving them control over the algorithms and data while we handle the complexity of distributed training. Tinker advances our mission of enabling more people to do research on cutting-edge models and customize them to their needs.&lt;/p&gt;
&lt;p&gt;Tinker lets you fine-tune a range of large and small open-weight models, including large mixture-of-experts models such as Qwen-235B-A22B. Switching from a small model to a large one is as simple as changing a single string in your Python code.&lt;/p&gt;
&lt;p&gt;Tinker is a managed service that runs on our internal clusters and training infrastructure. We handle scheduling, resource allocation, and failure recovery. This allows you to get small or large runs started immediately, without worrying about managing infrastructure. We use LoRA so that we can share the same pool of compute between multiple training runs, lowering costs.&lt;/p&gt;
&lt;p&gt;Tinker’s API gives you low-level primitives like &lt;code&gt;forward_backward&lt;/code&gt; and &lt;code&gt;sample&lt;/code&gt;, which can be used to express most common post-training methods. Even so, achieving good results requires getting many details right. That’s why we’re releasing an open-source library, the &lt;a href="http://github.com/thinking-machines-lab/tinker-cookbook"&gt;Tinker Cookbook&lt;/a&gt;, with modern implementations of post-training methods that run on top of the Tinker API.&lt;/p&gt;
&lt;p&gt;Groups at Princeton, Stanford, Berkeley, and Redwood Research have already been using Tinker:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href="https://blog.goedel-prover.com/"&gt;Princeton Goedel Team&lt;/a&gt; trained mathematical theorem provers&lt;/li&gt;
&lt;li&gt;The &lt;a href="https://statmech.stanford.edu/"&gt;Rotskoff Chemistry group&lt;/a&gt; at Stanford fine-tuned a model to complete chemistry reasoning tasks&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sky.cs.berkeley.edu/project/skyrl/"&gt;Berkeley’s SkyRL group&lt;/a&gt; ran experiments on a custom async off-policy RL training loop with multi-agents and multi-turn tool-use.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.redwoodresearch.org/"&gt;Redwood Research&lt;/a&gt; used Tinker to RL Qwen3-32B on difficult AI control tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tinker is now in private beta for researchers and developers. You can sign up for the Tinker waitlist &lt;a href="https://thinkingmachines.ai/tinker"&gt;here&lt;/a&gt;. We will be onboarding users to the platform starting today.&lt;/p&gt;
&lt;p&gt;If you’re an organization interested in using Tinker, please contact us &lt;a href="https://thinkingmachines.ai/cdn-cgi/l/email-protection#2e5a4740454b5c6e5a46474045474049434f4d4647404b5d004f47"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Tinker will be free to start. We will introduce usage-based pricing in the coming weeks.&lt;/p&gt;
&lt;p&gt;We’re excited to see what you discover and make with Tinker!&lt;/p&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://thinkingmachines.ai/blog/announcing-tinker/</guid>
      <category>Research</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>LoRA Without Regret</title>
      <link>https://thinkingmachines.ai/blog/lora/</link>
      <description>Today’s leading language models contain upwards of a trillion parameters, pretrained on tens of trillions of tokens. Base model performance keeps improving with scale, as these trillions are necessary for learning and representing all the patterns in written-down human knowledge.</description>
      <content:encoded>&lt;article class="content"&gt;


&lt;p&gt;Today’s leading language models contain upwards of a trillion parameters, pretrained on tens of trillions of tokens. Base model performance keeps improving with scale, as these trillions are necessary for learning and representing all the patterns in written-down human knowledge.&lt;/p&gt;
&lt;p&gt;In contrast, post-training involves smaller datasets and generally focuses on narrower domains of knowledge and ranges of behavior. It seems wasteful to use a terabit of weights to represent updates from a gigabit or megabit of training data. This intuition has motivated parameter efficient fine-tuning (PEFT), which adjusts a large network by updating a much smaller set of parameters.&lt;/p&gt;
&lt;p&gt;The leading PEFT method is low-rank adaptation, or LoRA. LoRA replaces each weight matrix W from the original model with a modified version $W' = W + \gamma BA$, where B and A are matrices that together have far fewer parameters than W, and $\gamma$ is a constant scaling factor. In effect, LoRA creates a low-dimensional representation of the updates imparted by fine-tuning.&lt;/p&gt;
&lt;p&gt;LoRA may offer advantages in the cost and speed of post-training, and there are also a few operational reasons to prefer it to full fine-tuning (henceforth, FullFT):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-tenant serving.&lt;/strong&gt; Since LoRA trains an adapter (i.e., the A and B matrices) while keeping the original weights unchanged, a single inference server can keep many adapters (different model versions) in memory and sample from them simultaneously in a batched way.&lt;label class="sidenote-number" for="sn12"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2310.18547"&gt;Punica: Multi-Tenant LoRA Serving&lt;/a&gt; (Chen, Ye, et al, 2023)&lt;/span&gt; Modern inference engines such as vLLM and SGLang implement this feature.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Layout size for training.&lt;/strong&gt; When fine-tuning the whole model, the optimizer state needs to be stored along with the original weights, often at higher precision. As a result, FullFT usually requires an order of magnitude more accelerators than sampling from the same model does, and thus a different layout.&lt;label class="sidenote-number" for="sn13"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;For training, besides storing the weights, we typically need to store gradients and optimizer moments for all of the weights; moreover, these variables are often stored in higher precision (float32) than what’s used to store the weights for inference (bfloat16 or lower).&lt;/span&gt; Since LoRA trains far fewer weights and uses far less memory, it can be trained on a layout only slightly larger than what is used for sampling. This makes training more accessible, and often more efficient.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ease of loading and transfer.&lt;/strong&gt; With fewer weights to store, LoRA adapters are fast and easy to set up or transfer between machines.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These reasons are sufficient to explain the growing popularity of LoRA since the publication of the original LoRA paper in 2021.&lt;label class="sidenote-number" for="sn10"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2106.09685"&gt;LoRA: Low-Rank Adaptation of Large Language Models&lt;/a&gt; (Hu et al, 2021)&lt;/span&gt; However, the literature is unclear on how well LoRA performs relative to FullFT.&lt;/p&gt;
&lt;p&gt;There is agreement that LoRA underperforms in settings that resemble pre-training,&lt;label class="sidenote-number" for="sn11"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2405.09673"&gt;LoRA Learns Less and Forgets Less&lt;/a&gt; (Biderman et al, 2024)&lt;/span&gt; namely those with very large datasets that exceed the storage limits of LoRA parameters. But for dataset sizes that are typical in post-training, LoRA has sufficient capacity to store the essential information. However, this fact makes no guarantees regarding sample efficiency and compute efficiency. The question is: &lt;em&gt;can LoRA match the performance of full fine-tuning, and if so, under which conditions?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In our experiments, we find that indeed, when we get a few key details right, LoRA learns with the same sample efficiency as FullFT and achieves the same ultimate performance.&lt;/p&gt;
&lt;h2 id="what-matters-for-lora"&gt;What matters for LoRA&lt;/h2&gt;
&lt;p&gt;This article covers a series of supervised fine-tuning and reinforcement learning experiments we conducted to determine the conditions under which LoRA matches FullFT efficiency. To this end, we did a few things differently from previous experiments on LoRA:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We investigated the general relationship between training set size and number of LoRA parameters, rather than focusing on specific datasets and tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In supervised learning, we measured &lt;em&gt;log loss&lt;/em&gt; rather than employing sampling-based evals, with the same goal of generality in mind. Log loss measurement gives clean results and scaling laws over ranges of training steps and training parameters.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We find that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For supervised fine-tuning on small-to-medium-sized instruction-tuning and reasoning datasets, LoRA performs the same as full fine-tuning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For datasets that exceed LoRA capacity, LoRA underperforms FullFT. Rather than the loss reaching a distinct floor that it can’t go below, LoRA results in worse training efficiency that depends on the relationship between model capacity to dataset size.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In some scenarios, LoRA is less tolerant of large batch sizes than full fine-tuning — it pays a larger penalty in loss as batch size increases beyond some point. This penalty is not mitigated by increasing the LoRA rank; it is a property of the product-of-matrices parametrization, which has different training dynamics than optimizing the original weight matrix.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Even in small data settings, LoRA performs better when applied to all weight matrices, especially MLP and MoE layers. Attention-only LoRA underperforms even when we match the number of trainable parameters by using higher rank for attention-only LoRA.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LoRA performs equivalently to FullFT for reinforcement learning even with small ranks. We find that RL requires very low capacity, a result we anticipated based on information-theoretical arguments.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also studied the impact of hyperparameters used for LoRA on its learning rate relative to full fine-tuning. We examine some invariances in hyperparameters like init scales and multipliers, and explain why the 1/r prefactor makes the optimal learning rate (LR) approximately independent of rank. We also show experimentally how the optimal LR for LoRA relates to the optimal LR for FullFT.&lt;/p&gt;
&lt;p&gt;The outcome of our experiments is the characterization of a “low-regret regime” where LoRA performs similarly to FullFT in terms of dataset size and LoRA parameters. We found this regime covers most post-training scenarios, opening the door to the use of efficient fine-tuning in many applications.&lt;/p&gt;
&lt;h2 id="methods-and-results"&gt;Methods and results&lt;/h2&gt;
&lt;p&gt;We designed our experiments to measure in detail the relative performance of LoRA compared to FullFT across a range of conditions. Here are some details of our experimental setup:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We varied the LoRA rank over three orders of magnitude, with rank between 1 and 512, and compared these to full fine-tuning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To eliminate potential confounds from using a suboptimal learning rate, we swept the LR for each experimental condition. We used constant learning rate schedule (no warmup or cooldown).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Our experiments used Llama 3 series models&lt;label class="sidenote-number" for="sn24"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2407.21783"&gt;The Llama 3 Herd of Models&lt;/a&gt; (Dubey et al, 2024)&lt;/span&gt; and Qwen3 models&lt;label class="sidenote-number" for="sn25"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2505.09388"&gt;Qwen3 Technical Report&lt;/a&gt; (Qwen Team, 2025)&lt;/span&gt;, including a mixture of experts (MoE) model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The main supervised learning experiments used the Tulu3&lt;label class="sidenote-number" for="sn19"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2411.15124"&gt;Tulu 3: Pushing Frontiers in Open Language Model Post-Training&lt;/a&gt; (Ivison et al, 2024)&lt;/span&gt; and OpenThoughts3&lt;label class="sidenote-number" for="sn20"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2506.04178"&gt;OpenThoughts: Data Recipes for Reasoning Models&lt;/a&gt; (Guha et al, 2025)&lt;/span&gt; datasets, focused on instruction following and reasoning, respectively. The two sets differ significantly in scope, structure, and application, supporting the generality of our results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Our RL experiments used mathematical reasoning tasks with answer correctness as the reward.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="lora-rank"&gt;LoRA rank&lt;/h3&gt;
&lt;p&gt;We trained for a single epoch on the Tulu3 dataset and a subset of the OpenThoughts3 datasets. For each dataset and model size, we swept over LoRA rank and learning rate. In the plots below, we draw one colored line for each rank, where the line is obtained by taking the pointwise minimum over all learning rates at each training step:&lt;/p&gt;
&lt;figure id="fig:rank-sweep"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/lora/svg/fig1.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;LoRA training curves for various ranks on Tulu3 and OpenThoughts3 datasets. FullFT and high-rank LoRAs have similar learning curves with loss decreasing linearly with the logarithm of steps. Lower-rank LoRAs fall off the minimum-loss curve when the adapter runs out of capacity. In the bottom plots (1B model) high-rank LoRA performs better than FullFT on one dataset and worse on the other. There might be some random variation in how LoRA performs on different datasets, due to differences in training dynamics or generalization behavior.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We see that FullFT and high-rank LoRAs have similar learning curves with loss decreasing linearly with the logarithm of the number of steps. Medium and low-rank LoRAs fall off the minimum-loss learning curves at some threshold of steps that correlates with rank. Intuitively, learning slows down when the adapter runs out of capacity, which in turn is determined by rank.&lt;/p&gt;
&lt;p&gt;Next, we plot how loss changes with LR to check that our sweep covers the best learning rate for each rank.&lt;/p&gt;
&lt;figure id="fig:lr-sweep"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/lora/svg/fig2.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;Learning rate versus final loss for various LoRA ranks on Tulu3. Minimum loss is approximately the same for high rank LoRA and FullFT. Optimal LR is 10 times higher for LoRA.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We find that the optimal learning rate for FullFT is lower by a factor of 10 than for high-rank LoRAs.&lt;label class="sidenote-number" for="sn14"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;See Biderman et al. (2024), Figure S1, for an experiment with sampling evals, which finds a similar 10x ratio.&lt;/span&gt; We’ll return to this in our discussion of LoRA hyperparameters later on.&lt;/p&gt;
&lt;p&gt;The optimal LR seems to be similar for all the LoRA runs across different ranks; we give a theoretical explanation for this finding below. However, there does seem to be some rank dependence, with lower optimal LR for rank=1 than for higher-rank LoRAs. The optimal LR changes by a factor of less than 2 between rank=4 and rank=512.&lt;/p&gt;
&lt;h3 id="batch-size-effects"&gt;Batch size effects&lt;/h3&gt;
&lt;p&gt;We found that in some settings, LoRA is less tolerant of large batch sizes than FullFT. The performance gap grows with larger batch sizes, independent of rank. For this next experiment, we used a small 10,000-example subset of OpenThoughts3.&lt;/p&gt;
&lt;figure id="fig:bs-sweep"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/lora/svg/fig3.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;Batch size effects on LoRA vs FullFT performance. Left: Learning curves for different batch sizes show a persistent gap between LoRA (dashed) and FullFT (solid) at large batch sizes. Right: Final loss as a function of batch size shows LoRA pays a larger penalty for increased batch size.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The left-hand plot in Figure 3 shows a persistent gap between the LoRA (dashed lines) and FullFT (solid line) learning curves at large batch sizes. The gap is smaller and shrinks over time for the smaller batch size of 32.&lt;/p&gt;
&lt;p&gt;The right-hand chart plots final loss as a function of batch size. We see the gap in loss for LoRA increasingly diverging from FullFT for larger batch sizes.&lt;/p&gt;
&lt;p&gt;The learning gap at large batches doesn’t seem to depend on rank, but rather seems to be a property of LoRA. The likely reason is that the product-of-matrices parametrization (BA) has less favorable optimization dynamics on this dataset than the full matrix (W). However, both LoRA and FullFT achieve their best loss at smaller batch sizes, so this gap may not matter as much in practice.&lt;/p&gt;
&lt;h2 id="layers-where-lora-is-applied"&gt;Layers Where LoRA Is Applied&lt;/h2&gt;
&lt;p&gt;We investigated the effects of applying LoRA to different layers in the network. The original paper by Hu et al. recommended applying LoRA only to the attention matrices, and many subsequent papers followed suit, though a recent trend has been to apply it to all layers.&lt;label class="sidenote-number" for="sn15"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;Similar to our results, the QLoRA paper also found that LoRA performed worse than MLP or MLP+attention, though they found that MLP+attention &amp;gt; MLP &amp;gt; attention, whereas we found the first two to be roughly equal.&lt;/span&gt; Indeed, we achieved far better results when applying LoRA to all layers, in particular, the MLP (including MoE) layers. In fact, applying LoRA to the attention matrices shows no additional benefits beyond applying it to the MLPs only.&lt;label class="sidenote-number" for="sn16"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;Biderman et al. (2024) obtained a similar result, with attention-only LoRA providing no additional benefit on top of MLP-only.&lt;/span&gt;&lt;/p&gt;
&lt;figure id="fig:fig4"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/lora/svg/fig4.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;Attention-only LoRA significantly underperforms MLP-only LoRA, and does not further improve performance on top of LoRA-on-MLP. This effect holds for a dense model (Llama-3.1-8B) and a sparse MoE (Qwen3-30B-A3B-Base).&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The underperformance of attention-only LoRA is not explained by having fewer parameters. In this particular case, attention-only with rank 256 underperforms MLP-only with rank 128, despite them having approximately the same number of parameters. (Compare the bold numbers in the table below.)&lt;/p&gt;
&lt;table border="1" style="border-collapse: collapse;"&gt;
&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;LoRA configuration&lt;/th&gt;&lt;th&gt;Params&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;span class="lora-color-square green"&gt;&lt;/span&gt;mlp, rank=256&lt;/td&gt;&lt;td&gt;0.49B&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;span class="lora-color-square red"&gt;&lt;/span&gt;attn, rank=256&lt;/td&gt;&lt;td&gt;&lt;b&gt;0.25B&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;span class="lora-color-square blue"&gt;&lt;/span&gt;all, rank=256&lt;/td&gt;&lt;td&gt;0.70B&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;span class="lora-color-square gold"&gt;&lt;/span&gt;mlp, rank=128&lt;/td&gt;&lt;td&gt;&lt;b&gt;0.24B&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p style="text-align: center; font-style: italic; margin-top: 0.5em; margin-bottom: 1em;"&gt;Parameter counts for LoRA on Llama-3.1-8B&lt;/p&gt;
&lt;p&gt;For the MoE experiment, we trained a separate LoRA on each expert, with the rank of each equal to the total rank divided by the number of active experts (equal to 8 for Qwen3 MoE). This scaling keeps the ratio of LoRA parameters to FullFT parameters the same for MoE layers as for other layers.&lt;/p&gt;
&lt;p&gt;We did similar experiments comparing different LoRA layers in two additional settings: (1) supervised learning on a small subset of the OpenThoughts3 dataset with rank=256, and (2) reinforcement learning on the MATH dataset. We describe our experimental setup in the following section. Attention-only LoRA underperforms MLP-only LoRA (which performs similarly to MLP+attention) in these settings as well.&lt;/p&gt;
&lt;figure id="fig:fig5"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/lora/svg/fig5.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;Learning rate vs final loss or reward, when varying which layers we apply LoRA to.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id="reinforcement-learning"&gt;Reinforcement learning&lt;/h3&gt;
&lt;p&gt;A key finding from our experiments is that LoRA fully matches the learning performance of FullFT when running policy gradient algorithms for reinforcement learning, even with ranks as low as 1.&lt;/p&gt;
&lt;p&gt;For these experiments, we used a basic policy gradient algorithm with an importance sampling correction; $\text{objective} =\sum_t \frac{p_{\text{learner}}}{p_{\text{sampler}}} Adv_t$.&lt;label class="sidenote-number" for="sn28"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;See &lt;a href="https://fengyao.notion.site/off-policy-rl"&gt;Your Efficient RL Framework Secretly Brings You Off-Policy RL Training&lt;/a&gt;&lt;/span&gt;
We used a GRPO-like centering scheme&lt;label class="sidenote-number" for="sn26"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2402.03300"&gt;DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models&lt;/a&gt; (Shao et al, 2024)&lt;/span&gt; where we sample multiple completions per problem and subtract the mean reward per group.&lt;/p&gt;
&lt;p&gt;Figure 6 (below) shows LR sweeps on the MATH&lt;label class="sidenote-number" for="sn21"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2103.03874"&gt;Measuring Mathematical Problem Solving With the MATH Dataset&lt;/a&gt; (Hendrycks et al, 2021)&lt;/span&gt; and GSM&lt;label class="sidenote-number" for="sn22"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2110.14168"&gt;GSM8K: Training Verifiers to Solve Math Word Problems&lt;/a&gt; (Cobbe et al, 2021)&lt;/span&gt; datasets, using typical hyperparameters for each. We used the Llama-3.1-8B base model as Qwen2.5 and Qwen3 are known to have been pretrained on data that improves their math performance, as described by the Qwen tech reports&lt;label class="sidenote-number" for="sn1"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2412.15115"&gt;Qwen2.5 Technical Report&lt;/a&gt; (Qwen Team, 2024)&lt;/span&gt;, which makes it harder to measure what is being learned only during RL.&lt;/p&gt;
&lt;p&gt;LoRA shows a wider range of performant learning rates and arrives at the same peak performance as FullFT (black line), at least within the precision limits afforded by the noisiness of RL.&lt;/p&gt;
&lt;figure id="fig:fig5-"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/lora/svg/fig5-.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;Learning rate vs final reward (accuracy) when doing RL on grade school math (GSM, left) or MATH (right) dataset.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This result is anticipated by an information-theoretic argument. Supervised learning arguably provides O(number of tokens) bits per episode. In contrast, in policy gradient methods, learning is driven by the advantage function which provides only O(1) bits per episode. When each episode contains thousands of tokens, RL absorbs ~1000 times less information per token in training than supervised learning does.&lt;/p&gt;
&lt;p&gt;We can use more precise numbers based on our experiments. In the MATH example, we trained on ~10,000 problems with 32 samples per problem. Assuming each completion yields a single bit of information, the whole training process only needs to absorb 320,000 bits. Rank-1 LoRA for Llama-3.1-8B already has 3M parameters&lt;label class="sidenote-number" for="sn17"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;We calculated this by adding up rank·$d_{in}$ (for matrix A) and rank·$d_{out}$ (for B) over all weight matrices in the model.&lt;/span&gt;, almost 10 times that number. Even at rank-1, LoRA has more than enough capacity to absorb all the information provided during training.&lt;/p&gt;
&lt;p&gt;As another point of comparison, &lt;a href="https://www.nature.com/articles/s41586-025-09422-z"&gt;DeepSeek-R1-Zero&lt;/a&gt; was trained on 5.3M episodes&lt;label class="sidenote-number" for="sn18"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;Training took place for 10,400 steps, each step consisting of 32 unique questions, each question sampled 16 times.&lt;/span&gt;, corresponding to 5.3M bits of information. This is less than the number of parameters in a low-rank LoRA, and we predict that the results can be replicated with LoRA.&lt;/p&gt;
&lt;p&gt;For additional validation of our findings of LoRA’s effectiveness in reasoning RL, we carried out larger-scale experiments with Qwen3-8b-base on the DeepMath dataset&lt;label class="sidenote-number" for="sn23"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2504.11456"&gt;DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning&lt;/a&gt; (He et al, 2025)&lt;/span&gt; as it is much larger than the MATH dataset and in general contains harder problems. To speed up experiments, we restricted the samples to a length of 8192 tokens for training and evaluation. This sample length allows for backtracking and reasoning but limits the performance, relative to longer chain-of-thought.&lt;/p&gt;
&lt;figure id="fig:fig6"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/lora/svg/fig6.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;Experiments on the DeepMath dataset with Qwen3-8b-base. In the left plot, we show the learning curve for different ranks and full fine-tuning. For each of these settings, we show the best learning rate, which results in the highest final performance. On the right, we plot learning rate vs final performance. As in our previous math experiments, LoRA seems to have a wider peak of near-optimal learning rates.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure id="fig:fig7"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/lora/svg/fig7.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;Additional plots from experiments on the DeepMath dataset with Qwen3-8b-Base. The left plot shows the benchmark scores on the AIME test set, which is more challenging than the training set. The right plot shows the chain-of-thought (CoT) length over training steps, which can be seen as a sign of learning to reason.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We observe that when picking the optimal learning rates for each setting, training progresses in an almost identical way for LoRAs with different sizes and full fine-tuning. Moreover, we see similar findings when we evaluate the models on the held-out problems of AIME 2024 and AIME 2025. Furthermore, we observe similar qualitative behavior from the LoRA and full-finetuning runs: both develop advanced reasoning behaviors such as backtracking, self-verification and in-context exploration, which is visible in the lengthening of the model CoTs.&lt;/p&gt;
&lt;h2 id="setting-lora-hyperparameters"&gt;Setting LoRA hyperparameters&lt;/h2&gt;
&lt;p&gt;One barrier to LoRA adoption is the necessity to choose optimal hyperparameters, which are different from ones optimized for FullFT. In this section, we show that this problem isn’t as daunting as it appears at first glance and discuss our findings related to hyperparameter choice.&lt;/p&gt;
&lt;h3 id="optimal-learning-rate-and-rank"&gt;Optimal learning rate and rank&lt;/h3&gt;
&lt;p&gt;Following Hu et al., we consider the following parametrization for LoRA:&lt;/p&gt;
$$W' = W + \frac{\alpha}{r}BA$$&lt;p&gt;Where $r$ is the LoRA rank, $\alpha$ is the LoRA scaling factor, and $A$, $B$ are the LoRA weight matrices (of rank $r$). We use $\alpha = 32$ for the experiments in this article, following standard practice from other implementations.&lt;/p&gt;
&lt;p&gt;The $1/r$ scaling factor makes the optimal learning rate approximately independent of rank. In fact, a stronger condition holds – the learning curve is exactly the same at the beginning of training, regardless of rank. This effect is striking, and in our experiments the closeness of the learning curves for different ranks had us worried that a bug caused the rank parameter to be ignored. It follows that in a short training regime, the optimal LR is also independent of rank. However, as we showed above in our plots of learning rate vs loss (Figure 2), optimal LR has some rank-dependence in the longer-training regime.&lt;/p&gt;
&lt;figure id="fig:fig8"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/lora/svg/fig8.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;These plots look at the differences in the learning curves, early in training, for different ranks with the same learning rate. On the left, we show the learning curves. The right shows the difference between rank 16 and 256, which grows over time. Strangely, it is negative (though tiny) for the first few steps, so that part of the curve is missing from the plot.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We can partly explain this result by looking at the expected update to the LoRA matrix after the very first training update. We can think of the LoRA product $BA$ as the sum of $r$ rank-1 outer products: $BA = \sum_{i=1}^r b_i a_i^T = \sum_{i=1}^r \Delta_i$, where we define $\Delta_i = b_i a_i^T$. Here, $\partial \text{Loss}/\partial \Delta_i$ is the same for all $i$; however the gradients $\partial \text{Loss}/\partial b_i$ and $\partial \text{Loss}/\partial a_i$ will depend on the initialization ($\partial \text{Loss}/\partial b_i$ depends on $a_i$, for example). Since the initialization of $a_i$ and $b_i$ do not depend on rank, it follows that $\mathbb{E}[\Delta_i]$ is the same for all $i$ and does not depend on rank. At the first step of training, the expected update from each of these terms is equal and independent of the rank. It follows that $(1/r)\sum_{i=1}^r \Delta_i$ is just a sample average of $r$ terms with the same expectation, so the expectation of the average, i.e., the change to the adapter $(1/r)BA$, doesn’t depend on the rank.&lt;/p&gt;
&lt;h3 id="parametrization-invariances"&gt;Parametrization invariances&lt;/h3&gt;
&lt;p&gt;There are four hyperparameters potentially applicable to LoRA:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The scale factor $α$ which appears in $α / r$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The learning rate for the down-projection matrix $A$, $LR_A$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The learning rate for the up-projection matrix $B$, $LR_B$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The initialization scale of matrix $A$, $\text{init}_A$. For a random initialization, this is the standard deviation of $A$’s initial elements. Matrix $B$ is initialized to zero, so there is no need to define $\text{init}_B$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Having to tune four different parameters may seem overwhelming. However, invariances in the training dynamics mean that two of these are redundant, and learning behavior is determined by two. We show this invariance by noting that when training with Adam and $ε = 0$, the&lt;label class="sidenote-number" for="sn2"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;We can extend this result to $ε &amp;gt; 0$; we need to scale it by $1/q$ because the gradients are scaled by that factor.&lt;/span&gt; optimization process is invariant to the following two-parameter transformation.&lt;/p&gt;
&lt;p&gt;For $p, q &amp;gt; 0$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$α → \frac{1}{pq} \cdot α$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\text{init}_A → p \cdot \text{init}_A$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$LR_A → p \cdot LR_A$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$LR_B → q \cdot LR_B$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since two degrees of freedom out of the four don’t affect the learning process, we are left with a 2D parameter space. We can choose different bases for this 2D space, such as the following one which lends itself to a straightforward interpretation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$α \cdot \text{init}_A \cdot LR_B$. This determines the scale of initial updates, or, equivalently, the initial slope of the learning curve. Since $B$ is initialized to zero, $LR_A$ and the initial updates to $A$ are irrelevant.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\text{init}_A / LR_A$. Since Adam updates the elements of $A$ by approximately $LR_A$ at each step, this timescale parameter determines the number of steps it takes to significantly transform $A$ away from its initial state.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can reinterpret some proposals from previous work on LoRA in terms of this basis.&lt;/p&gt;
&lt;p&gt;LoRA+&lt;label class="sidenote-number" for="sn3"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2402.12354"&gt;LoRA+: Efficient Low Rank Adaptation of Large Models&lt;/a&gt; (Hayou et al, 2024)&lt;/span&gt; proposes to use different LRs on $A$ and $B$, with a higher rate for $B$. Expressed in terms of our basis above, increasing $LR_B$ is equivalent to increasing $\text{init}_A/LR_A$ so that $A$ changes on a longer timescale.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide"&gt;Unsloth’s LoRA Hyperparameter Guide&lt;/a&gt; recommends using higher values of $α$ for high-rank LoRA, e.g. by avoiding the $1/r$ scaling. This is also equivalent to increasing $\text{init}_A/LR_A$. When we increase $α$, $LR_A$ and $LR_B$ need to be lowered in compensation to get the same update size. This in turn simply makes $LR_A$ smaller relative to $\text{init}_A$.&lt;/p&gt;
&lt;p&gt;In our experiments, we used the standard parametrization used in the Huggingface &lt;code&gt;peft&lt;/code&gt; library&lt;label class="sidenote-number" for="sn4"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://github.com/huggingface/peft"&gt;PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods&lt;/a&gt; (Mangrulkar et al, 2022)&lt;/span&gt; proposed by Hu et al: a uniform distribution for $A$ with scale $1/\sqrt{d_{in}}$, zero initialization for $B$, the same LR for both, and $α = 32$. We were unable to improve on these hyperparameters in our experimentation.&lt;/p&gt;
&lt;h3 id="optimal-learning-rates-for-lora-vs-fullft"&gt;Optimal learning rates for LoRA vs. FullFT&lt;/h3&gt;
&lt;p&gt;Our experiments showed that the optimal LR for LoRA is consistently 10x the one used for FullFT in the same application, for both supervised learning and reinforcement learning. This shows up in every U-shaped plot of performance (loss or reward) charted against learning rate. This observation should make it more straightforward to transfer learning hyperparameters from FullFT to LoRA.&lt;/p&gt;
&lt;p&gt;We don’t yet have an adequate theoretical explanation for this observation. We can attempt to derive this result from the facts that optimal LoRA LR is invariant to rank and that full-rank LoRA is directly comparable to FullFT. This analysis suggests a LR ratio of the model’s hidden size divided by $2 \cdot \alpha$, which doesn’t match the empirical result of the optimal ratio being fixed at 10 independent of the base model.&lt;/p&gt;
&lt;p&gt;For our empirical analysis, we conducted an LR sweep of 14 different Llama and Qwen models for both LoRA and FullFT on the Tulu3 dataset. From those sweeps, we fit a function that predicts the optimal learning rate based on the model’s hidden size and an indicator of whether it’s Llama or Qwen. The functional form used was:&lt;/p&gt;
$$\text{LR} = M_{\text{LoRA}} \cdot \left(\frac{2000}{\text{hidden size}}\right)^{\text{model pow} + \text{LoRA pow}}$$&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$M_{\text{LoRA}}$ is a multiplier applied when LoRA is used (1 if FullFT)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\text{model pow}$ is an exponent adjustment, calculated separately for each model source (Llama and Qwen)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\text{LoRA pow}$ is an additional exponent adjustment for LoRA&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\text{hidden size}$ is the dimension of the residual stream of the model.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We scored a predicted learning rate by using linear interpolation to predict the loss, based on the data from our sweep, and rated the parameters by summing the predicted loss over the 14 problems. Our optimization found a multiplier of 9.8 for LoRA over FullFT, and different dependence on hidden_size for Qwen3 and Llama models, but LoRA LRs had the same dependence on hidden_size as FullFT LRs, i.e., the optimization found $\text{LoRA pow} = 0$.&lt;/p&gt;
&lt;h3 id="learning-rates-in-short-and-long-runs"&gt;Learning rates in short and long runs&lt;/h3&gt;
&lt;p&gt;The typical initialization of LoRA creates an implicit schedule of change in the effective learning rate. This leads to differences between short and long training runs, and some differences in the shape of learning curves compared to FullFT.&lt;/p&gt;
&lt;p&gt;At the start of training, $B$ is initialized to zero. While $B$ is very small, changes in $A$ have negligible effects on the adapter $BA$ which is added to the original network weights. As $B$ grows larger, updates to $A$ start to have a bigger impact on the network outputs, with the effective learning rate increasing over the course of training as $B$ approaches $A$ in scale. We found that by the end of the full training runs on the Tulu3 and OpenThoughts datasets, the $B$ matrices ended up with larger spectral norms than the $A$ matrices.&lt;/p&gt;
&lt;p&gt;This implies that the optimal LR should be set higher for shorter training runs. Preliminary evidence suggests an optimal multiplier around 15x over the FullFT for short runs&lt;label class="sidenote-number" for="sn5"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;Based on anecdotal evidence, the higher multiplier is effective under ~100 steps or so.&lt;/span&gt;, converging to the aforementioned 10x multiplier for longer runs.&lt;/p&gt;
&lt;h2 id="discussion"&gt;Discussion&lt;/h2&gt;
&lt;p&gt;We want to move beyond our empirical results to discuss some broader considerations related to LoRA performance and applicability that would be of interest to both researchers and builders.&lt;/p&gt;
&lt;p&gt;First, let us examine in more depth our main result, namely the two conditions under which LoRA performs similarly to full fine-tuning:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;LoRA is applied to all layers of the network, especially the MLP/MoE layers which house most of the parameters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LoRA works well when not capacity constrained, i.e., the number of trainable parameters exceeds the amount of information to be learned, which can be estimated in terms of dataset size.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When (1) is satisfied, we get similar learning dynamics to FullFT at the very start of training. Then, as per (2), LoRA continues to look like FullFT until we start reaching capacity limits.&lt;/p&gt;
&lt;h3 id="why-lora-might-be-needed-on-all-layers"&gt;Why LoRA might be needed on all layers&lt;/h3&gt;
&lt;p&gt;As we showed earlier, if we put LoRA on only the attention layers, we get slower learning even in the tiny-data regime.&lt;/p&gt;
&lt;p&gt;One possible explanation could come from thinking about the empirical neural tangent kernel (eNTK) as an approximation of what happens when we do a small amount of fine-tuning, following Malladi et al.&lt;label class="sidenote-number" for="sn6"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2210.05643"&gt;A Kernel-Based View of Language Model Fine-Tuning&lt;/a&gt; (Malladi et al, 2022)&lt;/span&gt; eNTK is based on the dot products of gradients, specifically gradients $g_i = \partial/\partial\theta \log p(\text{token}_i | \text{prefix}_i)$, and $K(i, j) = g_i \cdot g_j$. As a consequence, the layers with the most parameters will typically have the most influence on the kernel. The paper also points out the eNTK for LoRA is approximately the same as that for full fine-tuning, when you train all the layers. So LoRA training $\approx$ eNTK(LoRA) $\approx$ eNTK(FullFT) $\approx$ FullFT. The approximation eNTK(LoRA) $\approx$ eNTK(FullFT) only holds when we apply LoRA to the layers that contain most of the parameters which make up the dot products.&lt;/p&gt;
&lt;h3 id="how-much-capacity-is-needed-by-supervised-and-reinforcement-learning"&gt;How much capacity is needed by supervised and reinforcement learning?&lt;/h3&gt;
&lt;p&gt;Past work&lt;label class="sidenote-number" for="sn7"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2404.05405"&gt;Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws&lt;/a&gt; (Allen-Zhu and Li, 2024)&lt;/span&gt; has shown that neural networks can store 2 bits per parameter. These results pertain to the maximum amount of information absorbed in the long-training limit, not to the compute efficiency or rate of learning.&lt;/p&gt;
&lt;p&gt;The 2-bits-per-parameter result relied on synthetic datasets cleverly constructed to contain a precise amount of information. It’s not as straightforward to estimate the information content required for a given realistic learning problem. One classic observation is that when minimizing log-loss, the total log-loss measured during the first epoch of training provides a measurement of the dataset’s description length. That is, an upper bound for the number of bits required to memorize the dataset. LLM datasets usually have a loss of around 1 bit (0.69 nats) per token, depending on dataset and model size.&lt;/p&gt;
&lt;p&gt;This estimate measures the capacity required to perfectly memorize the dataset, which overestimates the actual capacity needed for “generalizable” learning that reduces log-loss on test data. Measuring the capacity requirements of supervised learning and how these interact with the number of trainable parameters is an open question for future work.&lt;/p&gt;
&lt;p&gt;For RL, we claimed that policy gradient algorithms learn roughly 1 bit of information per episode, given that there’s a single reward value at the end of the episode. This isn’t a fundamental property of RL, as other algorithms could conceivably learn a lot more from each episode. For example, model-based RL algorithms train the learning agent to predict the observations and build a world model, potentially extractingmore information per episode. The claim of 1-bit-per-episode may only apply narrowly to policy gradient algorithms.&lt;/p&gt;
&lt;p&gt;We can sharpen the bits-counting argument in information-theoretic terms. Consider an episode, consisting of a trajectory $\tau$ and final reward, as a message (i.e., a noisy channel) that provides some information about the unknown reward function $R$. We’ll condition on the current policy and training history and look at the mutual information between the policy gradient estimator and $R$. The REINFORCE update is $G = S \cdot \text{Adv}$ with $S = \nabla \log p_\theta(\tau)$. $S$ is independent of $R$ given the history, so the only $R$-dependent component is the scalar advantage.&lt;/p&gt;
&lt;p&gt;By the data processing inequality:&lt;/p&gt;
$$I(G ; R | \text{history}) \leq I((S, \text{Adv}) ; R | \text{history}) = I(\text{Adv} ; R | S, \text{history}) \leq H(\text{Adv}).$$&lt;p&gt;If we quantize the advantage into $B$ bins, then $H(\text{Adv}) \lesssim \log(B)$. That is, the number of bits of useful information gleaned per episode is $O(1)$, independent of model size. These bits tell us which member of a discrete set of reward functions (or, equivalently, optimal-policy classes) we’re in. This analysis of mutual information mirrors what’s used in some theoretical analysis of optimization algorithms.&lt;label class="sidenote-number" for="sn8"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://www.mit.edu/~rakhlin/papers/ibc_optimization.pdf"&gt;Information Complexity of Black-Box Convex Optimization: A New Look via Feedback Information Theory&lt;/a&gt; (Raginsky and Rakhlin, 2009)&lt;/span&gt; Note that this estimate is an &lt;em&gt;upper&lt;/em&gt; bound on the information absorbed by training; the actual amount learned will depend on the policy initialization and other details. For example, if we initialize with a policy that doesn’t get any reward, then the entropy of the advantage is zero (not log(B)), and it won’t learn anything.&lt;/p&gt;
&lt;h3 id="compute-efficiency-advantage-of-lora"&gt;Compute efficiency advantage of LoRA&lt;/h3&gt;
&lt;p&gt;Our experiments above measured learning progress against the number of training steps, but we may also be interested in the &lt;em&gt;compute efficiency&lt;/em&gt; of different methods. We calculate that LoRA takes slightly more than ⅔ of the FLOPs that full fine-tuning does per pass. As a result, it will often outperform FullFT on compute efficiency overall.&lt;/p&gt;
&lt;p&gt;We derive this ⅔ ratio by analyzing the FLOPs used in the forward–backward pass on a given weight matrix. These operations account for the vast majority of FLOPs in neural network models. We use the following notation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$W \in \mathbb{R}^{N \times N}$ is a weight matrix&lt;/li&gt;
&lt;li&gt;$x \in \mathbb{R}^N$ is an input vector&lt;/li&gt;
&lt;li&gt;$y = Wx \in \mathbb{R}^N$ is an output vector&lt;/li&gt;
&lt;li&gt;$\bar{x}, \bar{y} \in \mathbb{R}^N$ are the gradients of the loss with respect to $x$ and $y$, computed in the backward pass&lt;/li&gt;
&lt;li&gt;$\bar{W} \in \mathbb{R}^{N \times N}$ is the gradient of the loss with respect to $W$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Full fine-tuning performs the following operations:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Forward&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$y = Wx$ ($N^2$ multiply–adds)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Backward&lt;/strong&gt;&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;$\bar{x} = W^T \bar{y}$ ($N^2$ multiply–adds)&lt;/li&gt;
&lt;li&gt;$\bar{W} \mathrel{+}= x \bar{y}^T$ ($N^2$ multiply–adds)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The forward pass requires $N^2$ multiply-adds, and the backward pass requires another $2 \cdot N^2$ for $3N^2$ total. Training, which requires both, thus uses 3 times the FLOPs of forward-only inference.&lt;/p&gt;
&lt;p&gt;With LoRA, we replace $W$ by $W + BA$, where $B \in \mathbb{R}^{N \times R}$ and $A \in \mathbb{R}^{R \times N}$, with $R \ll N$. Since we only update $\bar{A}$ and $\bar{B}$, we replace the third step of updating $\bar{W}$ with a much cheaper operation. $A$ and $B$ are $N \cdot R$ matrices, so the full forward-backward computation on each requires $3NR$ multiply-adds instead of $3N^2$ for $W$. The total for both is $6NR$. We also perform the forward-backward pass on $Wx$ and $\bar{x}$, equivalent to the first two steps of FullFT. The total number of multiply-adds is $2N^2 + 6NR$. With $R \ll N$, this is slightly more than $\frac{2}{3}$ of $3N^2$.&lt;/p&gt;
&lt;p&gt;If we plotted LoRA performance over FLOPs&lt;label class="sidenote-number" for="sn9"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;This analysis omits FLOPs used for attention, which could be significant in long-context settings.&lt;/span&gt; instead of training steps, it would show a clear advantage over FullFT.&lt;/p&gt;
&lt;h3 id="open-questions"&gt;Open questions&lt;/h3&gt;
&lt;p&gt;There are several questions related to our results that we would love to see investigated in the future:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sharpening our predictions of LoRA performance and the precise conditions under which it matches full fine-tuning. We have roughly characterized the regime of equal performance and can estimate the required capacity in terms of tokens or episodes, but we can’t yet make accurate forecasts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Our theoretical understanding of LoRA learning rates and training dynamics is limited. A fuller theory that explains the ratio between LoRA and FullFT learning rates would be valuable.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How do LoRA variants such as PiSSA&lt;label class="sidenote-number" for="sn27"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://arxiv.org/abs/2404.02948"&gt;PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models&lt;/a&gt; (Meng, Wang &amp;amp; Zhang, 2024)&lt;/span&gt; perform when measured according to the methodology in this article?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are various options for applying LoRA to MoE layers. LoRA users would benefit from an investigation into how well they perform, and how compatible each approach is with methods like tensor parallelism and expert parallelism that are important for large MoE models.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="closing-thoughts"&gt;Closing thoughts&lt;/h2&gt;
&lt;p&gt;At Thinking Machines, we believe in the power of fine-tuning to advance AI usefulness in many domains of expertise. Our interest in LoRA is driven by a goal of making this power widely accessible and easily customizable to specific needs.&lt;/p&gt;
&lt;p&gt;Aside from its practical uses, research on LoRA has also led us to deeper investigations of model capacity, dataset complexity, and sample efficiency. Looking at how learning speed and performance depend on capacity provides a lens for studying fundamental questions in machine learning. We look forward to advancing this study in the future.&lt;/p&gt;
&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;We thank Dan Alexander Biderman, Weizhu Chen, Daniel Han, and Sadhika Malladi for their insightful feedback on an earlier draft of this post.&lt;/p&gt;
&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;
&lt;p&gt;Please cite this work as:&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;Schulman, John and Thinking Machines Lab, "LoRA Without Regret",
Thinking Machines Lab: Connectionism, Sep 2025.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Or use the BibTeX citation:&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;@article{schulman2025lora,
  author = {John Schulman and Thinking Machines Lab},
  title = {LoRA Without Regret},
  journal = {Thinking Machines Lab: Connectionism},
  year = {2025},
  note = {https://thinkingmachines.ai/blog/lora/},
  doi = {10.64434/tml.20250929},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://thinkingmachines.ai/blog/lora/</guid>
      <category>Research</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Modular Manifolds</title>
      <link>https://thinkingmachines.ai/blog/modular-manifolds/</link>
      <description>When we train large neural networks, we need to keep them healthy. We do not want the tensors in the network—either the weights, activations or gradients—to grow too large or too small. Very small and very large tensors cause a variety of problems not just limited to numerical underflow and overflow. For example, weight matrices changing size during training makes it harder to design training algorithms—since the relative size of updates to weights has a significant impact on the speed of learning.</description>
      <content:encoded>&lt;article class="content"&gt;


&lt;p&gt;When we train large neural networks, we need to keep them healthy. We do not want the tensors in the network—either the weights, activations or gradients—to grow too large or too small. Very small and very large tensors cause a variety of problems not just limited to numerical underflow and overflow. For example, weight matrices changing size during training makes it harder to design training algorithms—since the relative size of updates to weights has a significant impact on the speed of learning.&lt;/p&gt;
&lt;p&gt;The gold standard for keeping tensors healthy is to normalize them. Normalization is commonplace for activation vectors, where we use techniques like &lt;a href="https://arxiv.org/abs/1607.06450"&gt;layer norm&lt;/a&gt; to put the activations on a good scale before passing them to the next layer. It is also commonplace to normalize gradient updates, where we can interpret fast training algorithms like the &lt;a href="https://kellerjordan.github.io/posts/muon/"&gt;Muon optimizer&lt;/a&gt; as spectrally normalizing the updates. Normalization provides us with certainty about the sizes of tensors—without needing to check Wandb!—and when training large neural networks with many interacting components, having certainty about the network internals is valuable.&lt;/p&gt;
&lt;p&gt;Normalization is less commonly applied to weight matrices, although it is not unheard of. For example, the &lt;a href="https://github.com/NVlabs/edm2"&gt;EDM2&lt;/a&gt; diffusion model codebase uses weight constraints and the authors report benefits in &lt;a href="https://arxiv.org/abs/2312.02696"&gt;their paper&lt;/a&gt;. And &lt;a href="https://arxiv.org/abs/1912.11370"&gt;BiT&lt;/a&gt; uses &lt;a href="https://arxiv.org/abs/1903.10520"&gt;weight standardization&lt;/a&gt;. Various other techniques have been proposed but are not common practice in modern large-scale training.&lt;label class="sidenote-number" for="sn1"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;For some more examples, see &lt;a href="https://arxiv.org/abs/1602.07868"&gt;Salimans et al, 2016&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1802.05957"&gt;Miyato et al, 2018&lt;/a&gt; and our paper &lt;a href="https://arxiv.org/abs/2102.07227"&gt;Liu et al, 2021&lt;/a&gt;.&lt;/span&gt; Normalizing the weight matrices might be a good idea for a few reasons. Weight constraints make understanding the relative size of optimization updates easier. They remove the problem of weight norms exploding. They allow us to focus hyperparameter tuning effort on tensors whose size matters most. They can force matrices to have a small &lt;a href="https://en.wikipedia.org/wiki/Condition_number"&gt;condition number&lt;/a&gt;, making their behaviour more predictable. And relatedly, weight constraints facilitate &lt;a href="https://theses.hal.science/tel-04674274v1"&gt;Lipschitz guarantees&lt;/a&gt; for robustness to perturbations.&lt;/p&gt;
&lt;p&gt;This post covers one appealing way to constrain the weight matrices of a neural network—by keeping the tensors constrained to submanifolds at each layer. This opens the door to re-thinking optimization, as we can co-design optimization algorithms with these manifold constraints. As an example, we propose&lt;label class="sidenote-number" for="sna"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;This algorithm builds on work from Jianlin Su and Franz Louis Cesista, as discussed further below.&lt;/span&gt; a manifold version of the Muon optimizer whose weights are constrained to the Stiefel manifold: the manifold of matrices with unit &lt;a href="https://en.wikipedia.org/wiki/Condition_number"&gt;condition number&lt;/a&gt;. We conclude the post by defining the idea of a &lt;em&gt;modular manifold&lt;/em&gt;, which is a composable manifold that attempts to make it easier to scale up and train large networks.&lt;/p&gt;
&lt;p&gt;Our goal in writing this post is to provide an introduction to a research area that we are excited about, and highlight many directions for future work. We would love to see more work from the community on the topics mentioned at the end of the post!&lt;/p&gt;
&lt;h2 id="the-shape-of-a-manifold-optimizer"&gt;The shape of a manifold optimizer&lt;/h2&gt;
&lt;p&gt;This section works through the simplest example of learning on a manifold: a vector parameter constrained to a hypersphere in $\mathbb{R}^d$. The vector parameter is trained to minimize a loss function defined over the full space $\mathbb{R}^d$. This setup might be useful for, say, individual embedding vectors in a transformer model. This section will be a good warmup for the following section on &lt;a href="#muon-on-the-stiefel-manifold"&gt;manifold Muon&lt;/a&gt; that considers matrix parameters.&lt;/p&gt;
&lt;p&gt;We will not be too formal about the definition of a manifold here: it is enough to understand that a manifold is a curved surface that looks flat when you zoom in close enough. The locally flat approximation at a point on the manifold is called the &lt;em&gt;tangent space&lt;/em&gt; to the manifold, as visualized in &lt;a href="#fig:tangent-space"&gt;Figure &lt;span data-ref="tangent-space"&gt;&lt;/span&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;figure id="fig:tangent-space"&gt;


&lt;figcaption&gt;The sphere in three dimensions—or the hypersphere in higher dimensions—is a manifold. The locally flat approximation at a point on the manifold is called the tangent space to the manifold and is visualized as the red plane in the figure.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We can characterize the hypersphere in $d$ dimensions as the set of points $w \in \mathbb{R}^d$ of unit Euclidean norm. And the tangent space at a point $w$ on the hypersphere is the set of all vectors $a \in \mathbb{R}^d$ that are orthogonal to $w$.&lt;/p&gt;
&lt;p&gt;To keep the weights constrained to the manifold, we could use a non-manifold optimizer and just project the weights back to the manifold after each step. Instead, we are interested in designing methods that take steps in the tangent space. The reason is that we would like to be able to equate the learning rate of our optimizer with the actual length of the optimization step. But if the optimization steps are pointing significantly off manifold and then being projected back, this nice property does not hold. Similar motivation is given in Section 2.3 of the &lt;a href="https://arxiv.org/abs/2312.02696"&gt;EDM2 paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Before we can design a training algorithm for this manifold, something important we need to decide on is how to measure distance&lt;label class="sidenote-number" for="sn3"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;For a manifold to be “Riemannian”, the distance measure must be induced by an inner product. The &lt;a href="https://en.wikipedia.org/wiki/Euclidean_distance"&gt;Euclidean ($\ell_2$) norm&lt;/a&gt; is induced by an inner product, but the &lt;a href="https://en.wikipedia.org/wiki/Taxicab_geometry"&gt;Manhattan ($\ell_1$) distance&lt;/a&gt; is not.&lt;/span&gt; in the tangent space. A common choice is the Euclidean distance, but we could also choose to measure distance in other ways, as visualized in &lt;a href="#fig:inscribed-norm-balls"&gt;Figure &lt;span data-ref="inscribed-norm-balls"&gt;&lt;/span&gt;&lt;/a&gt;. In the next section, we will talk about choosing a distance measure based on the functionality of the module.&lt;/p&gt;
&lt;figure id="fig:inscribed-norm-balls"&gt;


&lt;figcaption&gt;Inscribing unit balls in the tangent space for different distance measures. The $\ell_2$ (Euclidean) unit ball is a circle while the $\ell_1$ (Manhattan) unit ball is a diamond.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Crucially, the choice of distance measure changes the direction of the best optimization step. If the distance measure is non-Euclidean, then for a fixed length step, we may be able to move further in the direction of the gradient&lt;label class="sidenote-number" for="sn4"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;By gradient, we mean the partial derivative of the loss with respect to the weights. Mathematicians reserve the term gradient for &lt;a href="https://en.wikipedia.org/wiki/Gradient#Riemannian_manifolds"&gt;something else&lt;/a&gt; in Riemannian geometry.&lt;/span&gt; by not following the gradient direction exactly! This concept is visualized in &lt;a href="#fig:tangent-space-with-gradient"&gt;Figure &lt;span data-ref="tangent-space-with-gradient"&gt;&lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;figure id="fig:tangent-space-with-gradient"&gt;


&lt;figcaption&gt;How geometry influences the direction of the best optimization step. The pink arrow represents the raw gradient—meaning the partial derivative of the loss with respect to the weights. The yellow diamond denotes the $\ell_1$ unit ball. The green arrow is the unit vector pointing furthest in the direction of the gradient. Notice how the green arrow is not parallel to the pink arrow. (In practice, the pink arrow need not lie in the tangent space, although the green arrow will do by construction.) Try dragging the pink arrow to see how the best update direction changes.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To see how this works out in math, we can formulate the optimal update direction given a manifold constraint and a distance measure as itself solving a constrained optimization problem. We will demonstrate this for the case of the hypersphere equipped with the Euclidean norm. Letting $g$ denote the gradient, $w$ the current point on the hypersphere, $a$ the update direction and $\eta$ the learning rate, we need to solve:&lt;/p&gt;
$$\min_{a\in\mathbb{R}^d} \quad \underbrace{a^\top g\vphantom{\|a\|_2 = 1}}_{\mathclap{\text{linear change in loss}}} \quad \text{such that} \quad  \underbrace{\|a\|_2 = \eta}_{\mathclap{\text{size constraint}}} \quad \text{and} \quad \underbrace{a^\top w = 0\vphantom{\|a\|_2 = 1}}_{\mathclap{\text{tangent constraint}}}.\tag{$\star$}$$&lt;p&gt;Mapping back to the visual language of Figures &lt;a href="#fig:tangent-space"&gt;&lt;span data-ref="tangent-space"&gt;&lt;/span&gt;&lt;/a&gt;, &lt;a href="#fig:inscribed-norm-balls"&gt;&lt;span data-ref="inscribed-norm-balls"&gt;&lt;/span&gt;&lt;/a&gt; and &lt;a href="#fig:tangent-space-with-gradient"&gt;&lt;span data-ref="tangent-space-with-gradient"&gt;&lt;/span&gt;&lt;/a&gt;, this formula says that the green arrow (optimal value of $a$) must belong to the red tangent hyperplane ($a^\top w = 0$) and must also lie on a yellow circle of radius $\eta$ ($\|a\|_2 = \eta$). To solve $(\star)$, we can apply the &lt;a href="https://en.wikipedia.org/wiki/Lagrange_multiplier"&gt;method of Lagrange multipliers&lt;/a&gt;. The relevant Lagrangian function is given by:&lt;/p&gt;
$$\mathcal{L}(a, \lambda, \mu) = a^\top g + \frac{\lambda}{2} \cdot (a^\top a - \eta) + \mu \cdot (a^\top w),$$&lt;p&gt;
where $\lambda$ and $\mu$ are Lagrange multipliers. Setting the derivative of the Lagrangian with respect to $a$ to zero and applying the constraints to solve for $\lambda$ and $\mu$, the optimal update $a_\mathrm{opt}$ ends up being given by the following formula:
&lt;/p&gt;
$$a_\mathrm{opt} = - \eta \times \frac{g - ww^\top g}{\|g-ww^\top g\|_2}.$$&lt;p&gt;In words, the optimal update is given by subtracting out the radial component from the gradient, normalizing and multiplying by the learning rate. Since this update lies in the tangent space, actually a very small&lt;label class="sidenote-number" for="sn5"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;For a learning rate $\eta$, the effect of the retraction map is $\mathcal{O}(\eta^2)$ small, so the learning rate &lt;em&gt;almost&lt;/em&gt; equals the length of the step.&lt;/span&gt; correction is needed to stay on the manifold. The correction is known as a “retraction map” and is visualized in &lt;a href="#fig:retraction-map"&gt;Figure &lt;span data-ref="retraction-map"&gt;&lt;/span&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;figure id="fig:retraction-map"&gt;


&lt;figcaption&gt;Visualizing the retraction map. The green arrow is the update taken in the tangent space. Since for large step sizes the tangent space starts to diverge from the manifold, we need to project the updated weights back to the manifold using the retraction map—illustrated by the purple arrow.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We can solve for the retraction map by applying Pythagoras’ theorem to &lt;a href="#fig:retraction-map"&gt;Figure &lt;span data-ref="retraction-map"&gt;&lt;/span&gt;&lt;/a&gt;. For a unit hypersphere and a step of length $\eta$, the hypotenuse has length $\sqrt{1+\eta^2}$ and therefore the retraction map for the hypersphere equipped with the Euclidean norm is simply given by dividing the updated weights through by $\sqrt{1+\eta^2}$. Putting everything together, the full manifold optimization algorithm is then given by:&lt;/p&gt;
$$w \gets \frac{1}{\sqrt{1+\eta^2}} \left[w - \eta \times \frac{g - ww^\top g}{\|g-ww^\top g\|_2}\right].$$&lt;p&gt;As an exercise for the reader: try calculating the Euclidean norm of the updated weight vector and check that the updated weight vector indeed lies on the hypersphere.&lt;/p&gt;
&lt;p&gt;To summarize this section, a first-order manifold optimizer has three steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find the tangent vector of unit length that goes furthest in the gradient direction.&lt;/li&gt;
&lt;li&gt;Multiply this direction by the learning rate and subtract from the weights;&lt;/li&gt;
&lt;li&gt;Retract the updated weights back to the manifold.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are two decisions to make in applying this procedure: what manifold constraint we should use and how we should measure length. By making different decisions, we can generate different optimization algorithms as shown in the following table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Manifold&lt;/th&gt;
&lt;th&gt;Norm&lt;/th&gt;
&lt;th&gt;Optimizer&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Euclidean $\mathbb{R}^n$&lt;/td&gt;
&lt;td&gt;Euclidean norm&lt;/td&gt;
&lt;td&gt;vanilla gradient descent&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Euclidean $\mathbb{R}^n$&lt;/td&gt;
&lt;td&gt;infinity norm&lt;/td&gt;
&lt;td&gt;sign gradient descent&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hypersphere $S^n$&lt;/td&gt;
&lt;td&gt;Euclidean norm&lt;/td&gt;
&lt;td&gt;hyperspherical descent&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;matrix space $\mathbb{R}^{m\times n}$&lt;/td&gt;
&lt;td&gt;spectral norm&lt;/td&gt;
&lt;td&gt;Muon&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stiefel manifold $\subset\mathbb{R}^{m\times n}$&lt;/td&gt;
&lt;td&gt;spectral norm&lt;/td&gt;
&lt;td&gt;manifold Muon&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We will derive the final algorithm in the table, manifold Muon, in the next section. To design a manifold constraint and a distance function for a matrix parameter, we shall think carefully about the role that a weight matrix plays inside a neural network.&lt;/p&gt;
&lt;h2 id="manifold-muon"&gt;Manifold Muon&lt;/h2&gt;
&lt;p&gt;A typical weight matrix $W$ in a transformer is a “vector-multiplier”, meaning that it transforms an input vector $x$ into an output vector $y = Wx$. We will design a manifold constraint and a distance function so that the matrix acts in a good way on input vectors: the matrix should not produce excessively small or large outputs, and updates to the matrix should not cause the output vector to change too much or too little.&lt;/p&gt;
&lt;p&gt;A good way to think about how a matrix acts on vectors is through the &lt;a href="https://en.wikipedia.org/wiki/Singular_value_decomposition"&gt;singular value decomposition&lt;/a&gt;, illustrated in &lt;a href="#fig:svd"&gt;Figure &lt;span data-ref="svd"&gt;&lt;/span&gt;&lt;/a&gt;. The SVD decomposes a matrix in a way that tells us how the matrix stretches input vectors along different axes.&lt;/p&gt;
&lt;figure id="fig:svd"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/modular-manifolds/svgs/svd.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;The singular value decomposition. A matrix $M\in\mathbb{R}^{m\times n}$ of rank $k$ can always be decomposed as $M = U \Sigma V^\top$, where $U\in\mathbb{R}^{m\times k}$ and $V\in\mathbb{R}^{n\times k}$ have orthonormal columns and $\Sigma\in\mathbb{R}^{k\times k}$ is a diagonal matrix with only positive entries. The entries of $\Sigma$ are called the singular values of $M$. The singular values measure the stretching effect that the matrix has on vectors that align with the corresponding columns of $U$ and $V$.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We would like the matrix to have a stretching effect close to one, so we will choose a matrix manifold where all the singular values are exactly one. This matrix manifold is known formally as the &lt;a href="https://en.wikipedia.org/wiki/Stiefel_manifold"&gt;&lt;em&gt;Stiefel manifold&lt;/em&gt;&lt;/a&gt;. We can assume without loss of generality that we are dealing with a tall matrix ($m \geq n$), and then the Stiefel manifold can be equivalently defined as the following set:&lt;/p&gt;
$$\mathsf{Stiefel}(m,n) := \left\{ W \in \mathbb{R}^{m \times n} \mid W^T W = I_n \right\}.$$&lt;p&gt;Furthermore, &lt;a href="https://cseweb.ucsd.edu/classes/sp24/cse291-e/papers/StiefelManifold/StiefelNotes.pdf"&gt;one may show&lt;/a&gt; that a matrix $A \in \mathbb{R}^{m \times n}$ lies tangent&lt;label class="sidenote-number" for="sn-stiefel-tangent"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;Notice that the Stiefel constraint $W^T W = I_n$ directly generalizes the hyperspherical constraint $w^\top w = 1$ from the previous section. Similarly, the tangent space condition generalizes the hyperspherical one that $a^\top w = 0$.&lt;/span&gt; to the Stiefel manifold at matrix $W$ if and only if:&lt;/p&gt;
$$A^\top W + W^\top A = 0.$$&lt;p&gt;To design a manifold optimizer for the Stiefel manifold, all that remains is to choose a distance function. To limit the maximum stretching effect the weight update can have on an input vector, we will choose the &lt;em&gt;spectral norm&lt;/em&gt;, which measures the largest singular value of a matrix. Although this only limits the maximum effect the update can have, since the optimizer we derive will saturate this bound, it will turn out to prevent the minimum effect of the update from being too small.&lt;label class="sidenote-number" for="sn-stiefel-spectral-norm"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;There are some exceptions to this statement, such as when a weight matrix has a fan-out less than its fan-in, in which case we cannot escape from the matrix and its updates having a null space and mapping some inputs to zero.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The idea of doing gradient descent under a spectral norm constraint is what &lt;a href="https://jeremybernste.in/writing/deriving-muon"&gt;led to&lt;/a&gt; the Muon optimizer and, when combined with the Stiefel manifold constraint, we obtain a problem that we shall call &lt;em&gt;manifold Muon&lt;/em&gt;:&lt;/p&gt;
$$\min_{A\in\mathbb{R}^{m\times n}} \quad \underbrace{\operatorname{trace}(G^T A)}_{\mathclap{\text{linear change in loss}}} \quad \text{such that} \quad \underbrace{\|A\|_{\text{spectral}} \leq \eta}_{\mathclap{\text{size constraint}}} \quad \text{and} \quad \underbrace{A^T W + W^T A = 0\vphantom{\|A\|_{\text{spectral}} = \eta}}_{\mathclap{\text{tangent constraint}}} \tag{$\dagger$}.$$&lt;p&gt;The manifold Muon problem $(\dagger)$ directly generalizes problem $(\star)$ from the previous section. Solving $(\dagger)$ is harder than solving $(\star)$, and here we will present a numerical solution inspired&lt;label class="sidenote-number" for="sn6"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;I &lt;a href="https://docs.modula.systems/algorithms/manifold/orthogonal/"&gt;figured out&lt;/a&gt; how to solve manifold Muon in the square case late last year, but I was unable to solve the full rectangular case and thus &lt;a href="https://docs.modula.systems/algorithms/manifold/orthogonal/#open-problem-extending-to-the-stiefel-manifold"&gt;posed the problem&lt;/a&gt; as an open problem on the Modula docs. Jianlin Su &lt;a href="https://kexue.fm/archives/11221"&gt;solved the problem&lt;/a&gt; this summer by taking a Lagrangian approach and working out a fixed point iteration on the optimality condition. I saw an early version of Jianlin’s work (which did not quite work yet) and also &lt;a href="https://leloykun.github.io/ponder/steepest-descent-stiefel/"&gt;related work&lt;/a&gt; by Franz Louis Cesista, and I was able to work out the dual ascent algorithm presented here.&lt;/span&gt; by work done by Jianlin Su and Franz Louis Cesista.&lt;/p&gt;
&lt;p&gt;Our key insight is that $(\dagger)$ is a convex optimization problem that may be solved via a standard method known as &lt;a href="https://www.cs.cmu.edu/~pradeepr/convexopt/Lecture_Slides/dual-ascent.pdf"&gt;&lt;em&gt;dual ascent&lt;/em&gt;&lt;/a&gt;. Here we will just sketch the main idea, but you can find a more detailed derivation &lt;a href="https://docs.modula.systems/algorithms/manifold/stiefel/"&gt;on this page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Similar to Jianlin’s approach, we introduce a matrix of Lagrange multipliers $\Lambda\in\mathbb{R}^{n\times n}$. We then apply a series of transformations to convert the problem $(\dagger)$ from a &lt;em&gt;constrained minimization problem&lt;/em&gt; to an &lt;em&gt;unconstrained maximization problem&lt;/em&gt;:&lt;/p&gt;
$$
\begin{align}
(\dagger) &amp;amp;= \min_{\|A\|_\mathrm{spectral} \leq \eta} \max_{\Lambda} \;\operatorname{trace} G^\top A + \operatorname{trace}\Lambda^\top (A^\top W + W^\top A) \\
&amp;amp;= \min_{\|A\|_\mathrm{spectral} \leq \eta} \max_{\Lambda}\; \operatorname{trace}A^\top (G + 2W(\Lambda+\Lambda^\top))\\
&amp;amp;= \max_{\Lambda} \min_{\|A\|_\mathrm{spectral} \leq \eta} \; \operatorname{trace}A^\top (G + 2W(\Lambda+\Lambda^\top))\\
&amp;amp;= \max_{\Lambda} \; - \eta \times \|G + 2W(\Lambda+\Lambda^\top)\|_\mathrm{nuclear}.
\end{align}
$$&lt;p&gt;Equation (1) reformulates the problem as a &lt;a href="https://www-cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf"&gt;saddle point problem&lt;/a&gt;: the maximization over $\Lambda$ will send the objective to infinity whenever the tangent space condition is violated. Equation (2) follows by applying properties of the trace and equation (3) follows from Sion’s minimax theorem. The inner minimization in equation (3) is solved by setting $A_\mathrm{opt}(\Lambda) = - \eta \times \operatorname{msign}(G + 2W(\Lambda+\Lambda^\top))$ where $\operatorname{msign}$ is the &lt;a href="https://docs.modula.systems/algorithms/newton-schulz/"&gt;matrix sign function&lt;/a&gt;.&lt;label class="sidenote-number" for="sn7"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;The matrix sign function snaps the singular values of a matrix to one. It may be computed efficiently on GPUs via Newton-Schulz iteration or the recent &lt;a href="https://arxiv.org/abs/2505.16932"&gt;Polar Express&lt;/a&gt; algorithm.&lt;/span&gt; And we obtain equation (4) by substituting this expression for $A_\mathrm{opt}(\Lambda)$ into equation (3). Equation (4) is known as the “dual problem” to $(\dagger)$ and we can solve it by gradient ascent. After some work, the gradient of the dual function is given by:&lt;/p&gt;
$$
\begin{align}
H(\Lambda) &amp;amp;:= - \eta \times \nabla_\Lambda \|G + W (\Lambda+\Lambda^\top)\|_\mathrm{nuclear} \\
&amp;amp;= - \eta \times [W^\top\mathrm{msign}(G + 2W (\Lambda+\Lambda^\top)) + \operatorname{msign}(G + 2W (\Lambda+\Lambda^\top))^\top W],
\end{align}
$$&lt;p&gt;where the nuclear norm $\|\cdot\|_\mathrm{nuclear}$ measures the sum of the singular values of a matrix.&lt;/p&gt;
&lt;p&gt;Finally, we can write down the manifold Muon algorithm:&lt;label class="sidenote-number" for="sn8"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;Note that this algorithm is closely related to &lt;a href="https://kexue.fm/archives/11221"&gt;Jianlin Su’s solution&lt;/a&gt;. Where we run dual ascent, Jianlin’s solution amounts to solving for the maximum of the dual function $H(\Lambda)=0$ via a fixed point iteration.&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run gradient ascent on the dual variable $\Lambda \gets \Lambda + \alpha \times H(\Lambda)$ to solve for $\Lambda_\mathrm{opt}$.&lt;/li&gt;
&lt;li&gt;Compute the update $A_\mathrm{opt} = - \eta \times \operatorname{msign}(G + 2W(\Lambda_{\mathrm{opt}}+\Lambda_\mathrm{opt}^\top))$.&lt;/li&gt;
&lt;li&gt;Apply the update to the weights $W \gets W + A_\mathrm{opt}$.&lt;/li&gt;
&lt;li&gt;Retract the weights back to the manifold $W \gets \operatorname{msign}(W)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We ran a very small experiment to sanity check the algorithm and provide a minimal implementation for students or researchers to play with. Each training run finishes in less than a minute. The code is &lt;a href="https://github.com/thinking-machines-lab/manifolds/"&gt;here&lt;/a&gt; and see &lt;a href="#fig:cifar10"&gt;Figure &lt;span data-ref="cifar10"&gt;&lt;/span&gt;&lt;/a&gt; for the setup and results.&lt;/p&gt;
&lt;figure id="fig:cifar10"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/modular-manifolds/svgs/cifar10.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;Training a small MLP for 3 epochs on the CIFAR-10 dataset. The different lightly shaded blue curves show different weight decay settings for AdamW. Results were averaged over 3 random seeds. The manifold Muon optimizer attained higher train and test accuracy than AdamW. The third plot shows the final singular value distribution of the first weight matrix for the best performing learning rate: the singular values after training with manifold Muon are all close to 1. Manifold Muon increased the wall clock time per step compared to AdamW, although this could be improved by running fewer steps of dual ascent or adding momentum to the algorithm and running dual ascent online. Depending on other systems bottlenecks, the overhead may not be an issue.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id="modular-manifolds"&gt;Modular manifolds&lt;/h2&gt;
&lt;p&gt;So far in this post, we have discussed manifold constraints for individual parameter tensors and co-designed optimization logic for these constraints. A question we have not answered is: what happens when we combine layers to build networks? Can we think about individual layers in isolation—or do we need to be careful about interactions between layers and modify the optimization logic in response? The goal of this section is to point out that there is a way to extend the reasoning we introduced in the previous two sections to the case of whole networks, and we call this &lt;em&gt;the theory of modular manifolds&lt;/em&gt;.&lt;label class="sidenote-number" for="sn-modular-manifolds-1"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;The theory of modular manifolds builds on research I did with my friend Tim Large, my postdoc advisor Phillip Isola, my PhD advisor Yisong Yue and many other amazing collaborators.&lt;/span&gt; At the end of the section, we provide some links to learn more.&lt;/p&gt;
&lt;p&gt;The idea of modular manifolds is to build an abstraction that tells us how to budget learning rates across layers. The actual optimization logic in each layer ends up being the same as what we already worked out, except that the learning rate for a layer is modified depending on where the layer appears in the network. The abstraction rests upon a key observation made in our paper on the &lt;a href="https://arxiv.org/abs/2405.14813"&gt;modular norm&lt;/a&gt;, that budgeting learning rates—both across layers and when scaling up individual layers—is intimately tied to understanding the Lipschitz sensitivity of the network output with respect to the weights. The abstraction tracks this sensitivity as we build the network, and manifold constraints help us get a much tighter understanding of this sensitivity.&lt;/p&gt;
&lt;p&gt;The starting point for the abstraction is to think of any neural network module—from a layer to a whole transformer—as a mathematical object with three attributes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A forward function $f:\mathcal{W} \times \mathcal{X} \to \mathcal{Y}$ that maps from a parameter space $\mathcal{W} = \mathbb{R}^d$ and an input space $\mathcal{X}$ to an output space $\mathcal{Y}$.&lt;/li&gt;
&lt;li&gt;A submanifold of the weight space $\mathcal{M}\subset\mathcal{W}$ that the weights are constrained to.&lt;/li&gt;
&lt;li&gt;A norm $\|\cdot\| : \mathcal{W} \to \mathbb{R}$ that acts as a measuring stick on weight space.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, a linear module equipped with the spectral norm and constrained to the Stiefel manifold, for which we have already worked out an optimizer, would be written:&lt;/p&gt;
$$ \mathsf{StiefelLinear} = \begin{cases}(W, x) \mapsto Wx, &amp;amp; \text{(forward function)}\\ \mathsf{Stiefel}(m,n), &amp;amp; \text{(manifold)}\\ \|\cdot\|_\mathrm{spectral}. &amp;amp; \text{(norm)}\end{cases}$$&lt;p&gt;Provided that an input $x$ to the $\mathsf{StiefelLinear}$ module has unit $\ell_2$ norm, then $\mathsf{StiefelLinear}$ is Lipschitz with respect to its weights in the module’s assigned norm with Lipschitz constant one:&lt;label class="sidenote-number" for="sn-stiefel-lipschitz"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;This argument can be extended to the RMS norm on the input and the RMS–RMS operator norm on the weights.&lt;/span&gt;&lt;/p&gt;
$$\|(W + \Delta W) x - Wx\|_2 \leq \|\Delta W\|_\mathrm{spectral} \times \|x\|_2 = \|\Delta W\|_\mathrm{spectral}.$$&lt;p&gt;This type of Lipschitz statement helps us understand how to scale weight updates to this module since it gives us a bound on how much the output can change when we perturb the weights. But when we compose two modules, can we automatically compile a Lipschitz statement on the joint weight space of the new module? The answer turns out to be yes, if we follow special rules for building the new module:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The new forward function $f_3$ is given by composing the two existing forward functions $f_1$ and $f_2$:
$$f_3((w_1, w_2), x) := f_2(w_2, f_1(w_1, x)). \qquad$$&lt;/li&gt;
&lt;li&gt;The new manifold constraint $\mathcal{M}_3$ is just the Cartesian product (see &lt;a href="#fig:product-manifold"&gt;Figure &lt;span data-ref="product-manifold"&gt;&lt;/span&gt;&lt;/a&gt; for a fun example) of the two existing manifolds $\mathcal{M}_1$ and $\mathcal{M}_2$:
$$\mathcal{M}_3 = \mathcal{M}_1 \times \mathcal{M}_2. \qquad$$&lt;/li&gt;
&lt;li&gt;The new norm function is the max of the two existing norm functions weighted by special scalar coefficients $s_1$ and $s_2$. Letting $\|\cdot\|_1$ denote the first module’s norm and $\|\cdot\|_2$ denote the second module’s norm, the new norm $\|\cdot\|_3$ is given by:
$$\|(w_1, w_2)\|_3 := \max(s_1\cdot \|w_1\|_1, s_2\cdot \|w_2\|_2). \qquad$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When we use this composite norm to derive optimizers—following the same recipe we used in the first two sections of this post—we end up deriving separate optimizers for each layer, but the scalar coefficients $s_i$ budget the learning rates across layers.&lt;/p&gt;
&lt;p&gt;We give much more detail on this construction, including extending it to other ways of combining modules, in our paper on the &lt;a href="https://arxiv.org/abs/2405.14813"&gt;modular norm&lt;/a&gt;—although the paper does not cover manifold optimization. You can also check out our paper on &lt;a href="https://arxiv.org/abs/2410.21265"&gt;modular duality&lt;/a&gt; for more on building optimizers in the modular norm. The &lt;a href="https://modula.systems"&gt;Modula project&lt;/a&gt; builds toward a programmatic implementation of this construction.&lt;/p&gt;
&lt;figure id="fig:product-manifold"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/modular-manifolds/svgs/product-manifold.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;The Cartesian product is a simple way to glue together two manifolds. For example, the product of a line and a disk is a cylinder. We get one copy of the disk at every point on the line.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id="directions-for-future-work"&gt;Directions for future work&lt;/h2&gt;
&lt;p&gt;We are excited about any research that tries to make neural network training as principled and automatic as the forward pass. The ideas in this post benefitted strongly from interactions with external researchers like Jianlin Su and Franz Louis Cesista. We would love to see more work on these topics from the community.&lt;/p&gt;
&lt;p&gt;Some possible directions for future work are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Modularity.&lt;/strong&gt; What manifolds should attention heads live on? Should embeddings be constrained differently than unembeddings? We can mix-and-match constraints in different parts of the network, or leave some tensors unconstrained.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Numerics.&lt;/strong&gt; Manifold constraints also place constraints on the range of values that individual weight entries can take. Does this impact numerics, or make low-precision training easier?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Convex optimization.&lt;/strong&gt; The manifold Muon algorithm involves running dual ascent. Can we apply more sophisticated convex optimization techniques to solve the dual problem faster or more reliably?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Convergence analysis.&lt;/strong&gt; How fast do these algorithms converge? Does good conditioning of the weight matrices benefit convergence? Is there more that we can say theoretically?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Regularization.&lt;/strong&gt; Manifold constraints implicitly regularize the model. Could we design constraints or tune their radii to improve generalization?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Architecture-optimizer co-design.&lt;/strong&gt; While hard manifold constraints may not ultimately be the right way to constrain weight matrices, they exemplify the idea of tightly co-designing optimization algorithms with architecural components. Are there more opportunities here?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Non-Riemannian geometry.&lt;/strong&gt; Most work on manifold optimization works in a Riemannian world where distances are induced by inner products and norm balls are ellipsoids. But neural networks are different: matrices act as operators, and operator norms like the spectral norm do not emerge from inner products. This implies, for example, that norm balls can have sharp corners and there is no unique gradient flow. Is there more to be discovered in this non-Riemannian world?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Practical implementation.&lt;/strong&gt; Applying these techniques at scale requires efficient manifold operations on GPUs. The recent &lt;a href="https://arxiv.org/abs/2505.16932"&gt;Polar Express&lt;/a&gt; paper shows promise for fast matrix sign computation. What other algorithmic innovations do we need?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="further-reading"&gt;Further reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Manifold optimization.&lt;/strong&gt; &lt;a href="https://press.princeton.edu/absil"&gt;Absil, Mahony &amp;amp; Sepulchre&lt;/a&gt;’s textbook is a standard reference. For the Stiefel manifold specifically, see &lt;a href="https://math.mit.edu/~edelman/publications/geometry_of_algorithms.pdf"&gt;Edelman et al, 1998&lt;/a&gt;. These works live in a Riemannian world. Similarly most machine learning papers that consider optimization on the Stiefel manifold take a Riemannian point of view: see &lt;a href="https://arxiv.org/abs/2002.01113"&gt;Li et al, 2020&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2205.14173"&gt;Kong et al, 2022&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2508.17901"&gt;Park et al, 2025&lt;/a&gt; for some examples.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non-Riemannian geometry in machine learning.&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/1708.00523"&gt;Thomas Flynn’s paper&lt;/a&gt; from 2017 on duality structure gradient descent characterizes the neural network weight space as a &lt;em&gt;Finsler manifold&lt;/em&gt;, meaning a manifold equipped with a norm. It is well worth a read. Also see Jianlin Su’s &lt;a href="https://kexue.fm/archives/11221"&gt;recent blog post&lt;/a&gt; on Stiefel Muon as well as Franz Louis Cesista’s &lt;a href="https://leloykun.github.io/ponder/steepest-descent-stiefel/"&gt;blog post&lt;/a&gt; on a heuristic solution to Muon on the Stiefel manifold. Franz also wrote a &lt;a href="https://leloykun.github.io/ponder/steepest-descent-finsler/"&gt;followup blog post&lt;/a&gt; generalizing the solution presented here. The &lt;a href="https://arxiv.org/abs/2502.07529"&gt;Scion paper&lt;/a&gt; imposes weight constraints a different way via convex combinations and &lt;a href="https://papers.nips.cc/paper_files/paper/2015/hash/f50a6c02a3fc5a3a5d4d9391f05f3efc-Abstract.html"&gt;Carlson et al, 2015&lt;/a&gt; wrote an early paper on (unconstrained) spectral descent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The Modula project.&lt;/strong&gt; The goal of the Modula project is to build a library that automatically compiles steepest descent optimizers along with Lipschitz statements for general architectures. Check out the project page at &lt;a href="https://modula.systems"&gt;https://modula.systems&lt;/a&gt; as well as our paper on the &lt;a href="https://arxiv.org/abs/2405.14813"&gt;modular norm&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2410.21265"&gt;modular duality&lt;/a&gt;. Our &lt;a href="https://arxiv.org/abs/2409.20325"&gt;optimization anthology&lt;/a&gt; also provides an accessible route into this space of ideas.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Lipschitz-constrained deep learning.&lt;/strong&gt; There has been a lot of work on this topic. For example, check out &lt;a href="https://theses.hal.science/tel-04674274v1"&gt;Louis Béthune&lt;/a&gt; and &lt;a href="https://dspace.mit.edu/bitstream/handle/1721.1/129313/1227782217-MIT.pdf?sequence=1&amp;amp;isAllowed=y"&gt;Tsui-Wei Weng’s&lt;/a&gt; PhD theses. Usually work on this topic does not connect weight-Lipschitzness to optimizer design. See also &lt;a href="https://arxiv.org/abs/1811.05381"&gt;Anil et al, 2018&lt;/a&gt; and our paper &lt;a href="https://arxiv.org/abs/2507.13338"&gt;Newhouse et al, 2025&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;
&lt;p&gt;Please cite this work as:&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;Jeremy Bernstein, "Modular Manifolds",
Thinking Machines Lab: Connectionism, Sep 2025.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Or use the BibTeX citation:&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;@article{bernstein2025manifolds,
  author = {Jeremy Bernstein},
  title = {Modular Manifolds},
  journal = {Thinking Machines Lab: Connectionism},
  year = {2025},
  note = {https://thinkingmachines.ai/blog/modular-manifolds/},
  doi = {10.64434/tml.20250926}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://thinkingmachines.ai/blog/modular-manifolds/</guid>
      <category>Research</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Defeating Nondeterminism in LLM Inference</title>
      <link>https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/</link>
      <description>Reproducibility is a bedrock of scientific progress. However, it’s remarkably difficult to get reproducible results out of large language models.</description>
      <content:encoded>&lt;article class="content"&gt;

&lt;p&gt;Reproducibility is a bedrock of scientific progress. However, it’s remarkably difficult to get reproducible results out of large language models.&lt;/p&gt;
&lt;p&gt;For example, you might observe that asking ChatGPT the same question multiple times provides different results. This by itself is not surprising, since getting a result from a language model involves “sampling”, a process that converts the language model’s output into a probability distribution and probabilistically selects a token.&lt;/p&gt;
&lt;p&gt;What might be more surprising is that even when we adjust the temperature down to 0&lt;label class="sidenote-number" for="sn1"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;This means that the LLM always chooses the highest probability token, which is called greedy sampling.&lt;/span&gt; (thus making the sampling theoretically deterministic), LLM APIs are still &lt;strong&gt;not&lt;/strong&gt; deterministic in practice (see past discussions &lt;a href="https://152334h.github.io/blog/non-determinism-in-gpt-4/"&gt;here&lt;/a&gt;, &lt;a href="https://community.openai.com/t/a-question-on-determinism/8185/2"&gt;here&lt;/a&gt;, or &lt;a href="https://cookbook.openai.com/examples/reproducible_outputs_with_the_seed_parameter"&gt;here&lt;/a&gt;). Even when running inference on your own hardware with an OSS inference library like vLLM or SGLang, sampling still isn’t deterministic (see &lt;a href="https://docs.vllm.ai/en/v0.7.0/getting_started/faq.html"&gt;here&lt;/a&gt; or &lt;a href="https://docs.sglang.ai/references/faq.html"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;But why &lt;em&gt;aren’t&lt;/em&gt; LLM inference engines deterministic? One common hypothesis is that some combination of floating-point non-associativity and concurrent execution leads to nondeterminism based on which concurrent core finishes first. We will call this the “concurrency + floating point” hypothesis for LLM inference nondeterminism. For example, &lt;a href="https://arxiv.org/abs/2506.09501"&gt;a recent arXiv preprint&lt;/a&gt; writes:&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;p&gt;Floating-point arithmetic in GPUs exhibits non-associativity, meaning $(a + b) + c \neq a + (b + c)$ due to finite precision and rounding errors. This property directly impacts the computation of attention scores and logits in the transformer architecture, where parallel operations across multiple threads can yield different results based on execution order.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;label class="margin-toggle" for="mn1"&gt;&lt;/label&gt;
&lt;span class="marginnote"&gt;You can also find the “concurrency + floating point” hypothesis repeated by others, like &lt;a href="https://community.openai.com/t/a-question-on-determinism/8185"&gt;here&lt;/a&gt; (&lt;i&gt;“There are speed tradeoffs, and in order to make the endpoints fast GPUs are used, which do parallel [nondeterministic] calculations. Any modern GPU neural net calculations will be subject to these."&lt;/i&gt;), or &lt;a href="https://x.com/hosseeb/status/1773146428594090473"&gt;here&lt;/a&gt; (&lt;i&gt;“Because GPUs are highly parallelized, the ordering of additions or multiplications might be different on each execution, which can cascade into small differences in output."&lt;/i&gt;).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;While this hypothesis is not entirely wrong, it doesn’t reveal the full picture. For example, even on a GPU, running the same matrix multiplication on the same data repeatedly will always provide bitwise equal results. We’re definitely using floating-point numbers. And our GPU definitely has a lot of concurrency. Why don’t we see nondeterminism in this test?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="chroma" tabindex="0"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2048&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2048&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'cuda'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bfloat16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2048&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2048&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'cuda'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bfloat16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;ref&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;ref&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To understand the true cause of LLM inference nondeterminism, we must look deeper.&lt;/p&gt;
&lt;p&gt;Unfortunately, even &lt;em&gt;defining&lt;/em&gt; what it means for LLM inference to be deterministic is difficult. Perhaps confusingly, the following statements are all simultaneously true:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Some kernels on GPUs are &lt;b&gt;nondeterministic&lt;/b&gt;.&lt;/li&gt;
&lt;li&gt;However, all the kernels used in a language model’s forward pass are &lt;b&gt;deterministic&lt;/b&gt;.&lt;/li&gt;
&lt;li&gt;Moreover, the forward pass of an LLM inference server (like vLLM) can also be claimed to be &lt;b&gt;deterministic&lt;/b&gt;.&lt;/li&gt;
&lt;li&gt;Nevertheless, from the perspective of anybody using the inference server, the results are &lt;b&gt;nondeterministic&lt;/b&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this post, we will explain why the “concurrency + floating point” hypothesis misses the mark, unmask the true culprit behind LLM inference nondeterminism, and explain how to defeat nondeterminism and obtain truly reproducible results in LLM inference.&lt;/p&gt;
&lt;h2 id="the-original-sin-floating-point-non-associativity"&gt;The original sin: floating-point non-associativity&lt;/h2&gt;
&lt;p&gt;Before talking about nondeterminism, it’s useful to explain why there are numerical differences at all. After all, we typically think of machine learning models as mathematical functions following structural rules such as commutativity or associativity. Shouldn’t there be a “mathematically correct” result that our machine learning libraries should provide us?&lt;/p&gt;
&lt;p&gt;The culprit is &lt;strong&gt;floating-point non-associativity.&lt;/strong&gt; That is, with floating-point numbers:&lt;/p&gt;
$$ (a + b) + c \neq a + (b + c) $$&lt;div class="highlight"&gt;&lt;pre class="chroma" tabindex="0"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;1e20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;1e20&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1e20&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;1e20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Ironically, breaking associativity is what makes floating-point numbers useful.&lt;/p&gt;
&lt;p&gt;Floating-point numbers are useful because they allow for a “dynamic” level of precision. For the purposes of explanation, we will use base 10 (instead of binary), where floating-point numbers are in the format $\text{mantissa} * 10^\text{exponent}$. We will also use 3 digits for the mantissa and 1 digit for the exponent.&lt;/p&gt;
&lt;p&gt;For example, for the value 3450, we can represent it exactly as $3.45 * 10^3$. We can also represent much smaller values like 0.486 as $4.86 * 10^{-1}$. In this way, floating point allows us to represent both very small as well as very large values. In the sciences, we might say that floating point allows us to maintain a constant number of “significant figures”.&lt;/p&gt;
&lt;p&gt;If you add together two floating-point numbers with the same exponent, it looks similar to integer addition. For example, 123 ($1.23 * 10^2$) + 456 ($4.56 * 10^2$) results in 579 ($5.79 * 10^2$).&lt;/p&gt;
&lt;p&gt;But what happens when we add two floating-point numbers with different exponents, such as 1230 and 23.4?  In this case, the exact result is 1253.4. However, we can only maintain 3 digits of precision at a time. Floating-point addition will thus &lt;em&gt;drop&lt;/em&gt; the last 2 digits and obtain the value $1.25 * 10^3$ (or 1250).&lt;/p&gt;
&lt;figure&gt;
&lt;div class="figure-box"&gt;
&lt;div class="floating-point-visualization"&gt;
&lt;div class="operation-line"&gt;
&lt;div class="number-group"&gt;

&lt;div class="representation" id="num1-rep"&gt;1.23 × 10²&lt;/div&gt;
&lt;/div&gt;
&lt;div class="operator"&gt;+&lt;/div&gt;
&lt;div class="number-group"&gt;

&lt;div class="representation" id="num2-rep"&gt;3.45 × 10¹&lt;/div&gt;
&lt;/div&gt;
&lt;div class="operator"&gt;=&lt;/div&gt;
&lt;div class="result-representation"&gt;
&lt;div id="result-rep"&gt;1.57&lt;span class="lost-digits"&gt;5&lt;/span&gt; × 10²&lt;/div&gt;
&lt;div class="exact-value" id="exact-value"&gt;Exact: 1575&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!-- javascript must go below figure html --&gt;




&lt;figcaption&gt;We require 3 digits of precision to represent 1230 and 3 digits of precision to represent 23.4. However, adding these 2 numbers together results in a number that requires 5 digits of precision to represent (1253.4). Our floating-point format must then drop the 34 off the end. In some sense, we have effectively rounded our original 23.4 to 20.0 before adding it. &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;At this point, however, we’ve destroyed information. Note that this can happen every time we add two floating-point numbers with different “scales” (i.e. different exponents). And adding together floating-point numbers with different exponents happens all of the time. In fact, if we could guarantee that we never needed different exponents, we could just use integers!&lt;/p&gt;
&lt;p&gt;In other words, every time we add together floating-point numbers in a different order, we can get a completely different result. To take an extreme example, there are 102 possible different results for summing this array depending on the order.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="chroma" tabindex="0"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;vals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1e-10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1e-2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;vals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vals&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;vals&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;    &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;    &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vals&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"There are &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; unique results: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Output:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# There are 102 unique results: [-8.326672684688674e-17, -7.45931094670027e-17, ..., 8.326672684688674e-17]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Although this is the underlying cause for non-identical outputs, it does not directly answer where the nondeterminism comes from. It doesn’t help us understand why floating-point values get added in different orders, when that happens, nor how it can be avoided.&lt;/p&gt;
&lt;p&gt;The answers lie in how kernels are implemented.&lt;/p&gt;
&lt;h2 id="why-dont-kernels-always-add-numbers-in-the-same-order"&gt;Why don’t kernels always add numbers in the same order?&lt;/h2&gt;
&lt;p&gt;As mentioned above, one common explanation for why kernels add numbers in different orders is the “concurrency + floating point” hypothesis. The hypothesis states that if the order in which concurrent threads finish is nondeterministic and the accumulation order depends on the order in which concurrent threads finish (such as with an atomic add), our accumulation order will be nondeterministic as well.&lt;/p&gt;
&lt;p&gt;Confusingly, although this can lead to nondeterministic kernels, concurrency (and atomic adds) end up being completely uninvolved in LLM inference nondeterminism! To explain what the real culprit is, let’s first understand why modern GPU kernels rarely need atomic adds.&lt;/p&gt;
&lt;h2 id="when-are-atomic-adds-needed"&gt;When are atomic adds needed?&lt;/h2&gt;
&lt;p&gt;Typically a GPU launches a program concurrently across many “cores” (i.e. SMs). As the cores have no inherent synchronization among them, this poses a challenge if the cores need to communicate among each other. For example, if all cores must accumulate to the same element, you can use an “atomic add” (sometimes known as a “&lt;a href="https://en.wikipedia.org/wiki/Fetch-and-add"&gt;fetch-and-add&lt;/a&gt;”). The atomic add is “nondeterministic” — the order in which the results accumulate is purely dependent on which core finishes first.&lt;/p&gt;
&lt;p&gt;Concretely, imagine that you are reducing a 100-element vector with 100 cores (e.g. &lt;code&gt;torch.sum()&lt;/code&gt;). Although you can load all 100 elements in parallel, we must eventually reduce down to a single element. One way to accomplish this is with some kind of “atomic add” primitive, where the hardware guarantees that all additions will be processed but does not guarantee the order.&lt;/p&gt;
&lt;figure&gt;
&lt;div class="atomic-add-viz"&gt;

&lt;/div&gt;




&lt;figcaption&gt; The atomic add ensures that every core's contributions will be reflected in the final sum. However, it makes no guarantee about what &lt;i&gt;order&lt;/i&gt; the contributions will be added. The order depends entirely on which core finishes first, a nondeterministic property. Thus, executing the same parallel program multiple times can result in nondeterministic outputs. &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This is usually what folks mean by “nondeterminism” — you execute the same kernel twice with exactly the same inputs and you get a different result out. This is known as &lt;i&gt;run-to-run nondeterminism&lt;/i&gt;, where you run the same python script twice with the exact same dependencies but get a different result.&lt;/p&gt;
&lt;p&gt;Although concurrent atomic adds &lt;strong&gt;do&lt;/strong&gt; make a kernel nondeterministic, &lt;i&gt;atomic adds are not necessary for the vast majority of kernels.&lt;/i&gt; In fact, in the typical forward pass of an LLM, there is usually &lt;i&gt;not a single atomic add present.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;This may be surprising, given that parallelizing a reduction can benefit from atomic adds. There are two main reasons why atomic adds do not end up being needed.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;There is often sufficient parallelism along the “batch” dimension that we don’t need to parallelize along the reduction dimension. For example, let’s say that instead of reducing a single 100-dim vector we were reducing 500 vectors in parallel. In this case, we can reduce an entire vector in each core and allow every core to operate on a different vector.&lt;/li&gt;
&lt;li&gt;Over time, most neural network libraries have adopted a variety of strategies for achieving determinism without sacrificing performance. For example, we can perform a “split” (or tree) reduction, where we split the 100-element reduction into five 20-element reductions (thus achieving five-way parallelism). Then, to combine the remaining five elements, we can either perform a separate “clean-up” reduction (which isn’t parallelized, but operates over few enough elements to be cheap) or utilize a semaphore (which ensures that each concurrent thread-block will accumulate in a deterministic order).&lt;label class="sidenote-number" for="sn2"&gt;&lt;/label&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Due to these two factors, avoiding atomics adds is a negligible performance penalty for the vast majority of neural network operations.&lt;/p&gt;
&lt;p&gt;There are still a couple of common operations that have significant performance penalties for avoiding atomics. For example, &lt;code&gt;scatter_add&lt;/code&gt; in PyTorch (&lt;code&gt;a[b] += c&lt;/code&gt;). The only one commonly used in LLMs, however, is FlashAttention backward.&lt;label class="sidenote-number" for="sn3"&gt;&lt;/label&gt;&lt;/p&gt;
&lt;p&gt;However, the forward pass of an LLM involves &lt;i&gt;no operations that require atomic adds.&lt;/i&gt; Thus, the forward pass in an LLM is in fact “run-to-run deterministic.”&lt;/p&gt;
&lt;figure style="margin-top: 2rem; margin-bottom: 2.3rem;"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/svgs/deterministic.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;From the perspective of the inference server, it &lt;i&gt;is&lt;/i&gt; deterministic. Given the exact same user requests, it will always provide the same deterministic output. &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Wikipedia writes that “a deterministic algorithm is an algorithm that, given a particular input, will always produce the same output.” And in this case, given the exact same inputs (i.e. the exact requests the inference server is processing), the forward pass always produces the exact same outputs.&lt;/p&gt;
&lt;p&gt;However, the forward pass itself being “deterministic” is not sufficient to ensure that a system that includes it is deterministic. For example, what if our request’s output depended on the parallel user requests (e.g. batch-norm)? Since each individual request has no way of knowing what the parallel requests will be, from their perspective our overall LLM inference is also nondeterministic!&lt;/p&gt;
&lt;p&gt;As it turns out, our request’s output &lt;em&gt;does&lt;/em&gt; depend on the parallel user requests. Not because we’re somehow leaking information across batches — instead, it’s because our forward pass lacks “batch invariance”, causing our request’s output to depend on the &lt;strong&gt;batch size&lt;/strong&gt; of our forward pass.&lt;/p&gt;
&lt;h3 id="batch-invariance-and-determinism"&gt;Batch invariance and “determinism”&lt;/h3&gt;
&lt;p&gt;To explain batch invariance, let’s simplify the system and look solely at matmuls. You can assume that all matmul implementations are “run-to-run deterministic."&lt;label class="sidenote-number" for="sn4"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;This is not totally true, but most common matmul implementations do have this property.&lt;/span&gt;  However, they are not “batch-invariant.” In other words, when the batch size changes, each element in the batch can get different results.&lt;/p&gt;
&lt;p&gt;This is a fairly unusual property from a mathematical perspective. Matrix multiplication should be “independent” along every element in the batch — neither the other elements in the batch nor how large the batch is should affect the computation results of a specific element in the batch.&lt;/p&gt;
&lt;p&gt;However, as we can observe empirically, this isn’t true.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="chroma" tabindex="0"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_default_device&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'cuda'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2048&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4096&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Doing a matrix vector multiplication by taking&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# the first element of the batch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;out1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Doing a matrix matrix multiplication and then taking&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# the first element of the batch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;out2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)[:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;out1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;out2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="c1"&gt;# tensor(1669.2500, device='cuda:0')&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that this &lt;i&gt;is&lt;/i&gt; “run-to-run deterministic.” If you run the script multiple times, it will deterministically return the same result.&lt;label class="sidenote-number" for="sn5"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;It is not “hardware/software version invariant” — your GPU/PyTorch version may return a different value, but it should deterministically return the same value.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, when a non-batch-invariant kernel is used as part of a larger inference system, the system can become nondeterministic. When you make a query to an inference endpoint, the amount of load the server is under is effectively “nondeterministic” from the user’s perspective. The load determines the batch size that the kernels are run under, and thus changes the eventual result of each individual request!&lt;/p&gt;
&lt;figure style="margin-top: 2.3rem;"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/svgs/nondeterministic.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption style="margin-top: 3rem;"&gt;Although the inference server itself can be claimed to be "deterministic", the story is different for an individual user. From the perspective of an individual user, the other concurrent users are not an "input" to the system but rather a nondeterministic property of the system. This makes LLM inference "nondeterministic" from the perspective of each user.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;If you compose some property under which the kernel is not invariant (i.e. batch-size) with nondeterminism of that property (i.e. the load the server is under), you get a nondeterministic system.&lt;/p&gt;
&lt;p&gt;In other words, &lt;b&gt;the primary reason nearly all LLM inference endpoints are nondeterministic is that the load (and thus batch-size) nondeterministically varies!&lt;/b&gt; This nondeterminism is not unique to GPUs — LLM inference endpoints served from CPUs or TPUs will also have this source of nondeterminism.&lt;/p&gt;
&lt;p&gt;So, if we’d like to avoid nondeterminism in our inference servers, we must achieve batch invariance in our kernels. In order to understand how that can be achieved, let’s first take a look at why kernels don’t have batch invariance in the first place.&lt;/p&gt;
&lt;h2 id="how-do-we-make-kernels-batch-invariant"&gt;How do we make kernels batch-invariant?&lt;/h2&gt;
&lt;p&gt;In order to make a transformer implementation batch-invariant, we must make every kernel batch-invariant. Luckily, we can assume that every pointwise operation is batch-invariant.&lt;label class="sidenote-number" for="sn6"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;Although this is true for all kernels in say, PyTorch, it’s not inherently true. For example, there are some kernel implementations on CPU that will use vectorized intrinsics on some parts of the array and non-vectorized intrinsics on other parts, and these intrinsics don’t necessarily always have bitwise identical numerics.&lt;/span&gt; Thus, we only need to worry about the 3 operations that involve reductions — RMSNorm, matrix multiplication, and attention.&lt;label class="sidenote-number" for="sn7"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;Reductions related to parallelism are out of the scope of this discussion, but the same principles apply. One factoid that may be useful is that NVLink-Sharp in-switch reductions are deterministic on Blackwell as well as Hopper with CUDA 12.8+. As is the case with many things, this information can be found on NCCL’s &lt;a href="https://github.com/NVIDIA/nccl/issues/1497#issuecomment-3210819243"&gt;github issues&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Conveniently, these are also ordered in ascending levels of difficulty. Each one requires some additional considerations to achieve batch invariance with reasonable performance. Let’s talk about RMSNorm first.&lt;/p&gt;
&lt;h3 id="batch-invariant-rmsnorm"&gt;Batch-invariant RMSNorm&lt;/h3&gt;
&lt;figure class="half-width"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/svgs/rmsnorm-01.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;&lt;b&gt;Data Parallel RMSNorm&lt;/b&gt; Ideally, we'd like to avoid communication between cores in our parallelization strategy. One way to achieve that is by assigning one batch-element to each core, thus guaranteeing that each reduction is done entirely within a single core. This is what's known as a "data-parallel" strategy, since we're simply parallelizing along a dimension that doesn't require communication. In this example, we have four rows and four cores, saturating our cores. &lt;/figcaption&gt;
&lt;/figure&gt;
RMSNorm can be implemented as:
&lt;div class="highlight"&gt;&lt;pre class="chroma" tabindex="0"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# x: [batch_size, hidden_dim]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# weight: [hidden_dim]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rms_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rsqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keepdim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The requirement for batch invariance is that the &lt;b&gt;reduction order for each element must be fixed regardless of the batch-size of the kernel.&lt;/b&gt; Note that this doesn’t mean we must always use the same reduction strategy. For example, if we change the number of elements we’re reducing over, we can still be batch-invariant even if our reduction strategy changes.&lt;label class="sidenote-number" for="sn8"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;&lt;a href="https://github.com/Dao-AILab/quack/blob/main/media/2025-07-10-membound-sol.md"&gt;The Quack&lt;/a&gt; blog post has some nice examples showing the hierarchy of various reduction strategies you can do (e.g. thread reduction, warp reduction, block reduction, cluster reduction).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus, we only break batch invariance when our batch-size affects the reduction strategy.&lt;/p&gt;
&lt;p&gt;Let’s look at the standard parallelism strategy for RMSNorm. Generally, parallel algorithms benefit from minimizing communication across cores. &lt;label class="sidenote-number" for="sn9"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;For the purpose of this discussion you can assume that when we refer to “cores” we mean SMs. More specifically, the property here that’s important is that the # of threadblocks our kernel launches is greater than the # of SMs.&lt;/span&gt; So, one strategy we can start with is to assign each batch element to one core, as seen in the above figure.&lt;/p&gt;
&lt;p&gt;Increasing our batch size doesn’t affect our reduction strategy; if a batch size of 200 provides sufficient parallelism to our kernel then a batch size of 2000 will &lt;em&gt;definitely&lt;/em&gt; provide sufficient parallelism.&lt;/p&gt;
&lt;figure class="half-width"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/svgs/rmsnorm-02.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;&lt;b&gt;Data Parallel RMSNorm for larger batches&lt;/b&gt; Extending the data-parallel strategy to larger batches is fairly straightforward --- instead of having each core handle one row you allow each core to handle different rows sequentially. This &lt;i&gt;preserves batch invariance&lt;/i&gt; as the reduction strategy for each batch element remains identical. &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;On the other hand, decreasing the batch size can pose challenges. Because we assign each batch element to one core, decreasing our batch size will eventually lead to having more cores than batch elements, leaving some cores idle.&lt;/p&gt;
&lt;p&gt;Upon encountering this situation, a good kernel engineer would reach for one of the solutions mentioned in the prior section (atomic adds or split reductions), maintaining good parallelism and thus, good performance. Unfortunately, this changes the reduction strategy, preventing this kernel from being batch-invariant.&lt;/p&gt;
&lt;figure class="half-width"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/svgs/rmsnorm-03.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;&lt;b&gt;Split-Reduction RMSNorm&lt;/b&gt; If we have a small batch size, our data-parallel strategy may no longer have sufficient parallelism to saturate our cores. In this case, it may be more efficient to "split" a reduction among multiple cores, allowing us to fully utilize our GPU. However, this &lt;i&gt;loses&lt;/i&gt; batch invariance, as we are no longer reducing each element in the same order.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The easiest solution is to simply ignore these cases altogether. This is not completely &lt;em&gt;unreasonable&lt;/em&gt; — a small batch size means that the kernel is likely to execute quickly anyways, and so a slowdown may not be catastrophic.&lt;/p&gt;
&lt;p&gt;If we &lt;em&gt;were&lt;/em&gt; compelled to optimize this use case, one approach would be to consistently use a reduction strategy that has enough parallelism even for very small batch sizes. Such a reduction strategy would lead to an excess amount of parallelism for larger batch sizes but would allow us to achieve decent (but not peak) performance across the entire range of sizes.&lt;/p&gt;
&lt;h3 id="batch-invariant-matrix-multiplication"&gt;Batch-invariant matrix multiplication&lt;/h3&gt;
&lt;figure class="half-width matmul"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/svgs/matmul-01.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;&lt;b&gt;Data Parallel Matmul&lt;/b&gt; Similar to RMSNorm, the standard parallelism strategy for matmuls is a "data-parallel" strategy, keeping the entire reduction in one core. It is most straightforward to think about splitting the output tensor into 2D tiles and assigning each tile to a different core. Each core then computes the dot products that belong to that tile, once again performing the entire reduction within one core.
&lt;p&gt;Unlike for RMSNorm, additional constraints around arithmetic intensity and utilizing tensorcores force us to split 2D tiles instead of individual output elements for efficient matmul kernels.&lt;/p&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;At its core, you can view matrix multiplication as simply a pointwise operation followed by a reduction. Then, if we parallelize our matrix multiplication by chunking the &lt;strong&gt;output&lt;/strong&gt; into tiles, we have an analogous “data-parallel” kernel strategy that keeps each reduction within one core.&lt;/p&gt;
&lt;p&gt;Also similar to RMSNorm, it is possible for our “batch” dimensions (M and N) to become too small, forcing us to split along the reduction dimension (K). Despite having two “batch” dimensions, matmuls also require us to have much more “work” per core in order to leverage tensorcores effectively. For example, if you have a [1024, K] x [K, 1024] matmul and a standard 2D tile size of [128, 128], a data-parallel strategy would only be able to split this matmul into 64 cores, insufficient to saturate the GPU.&lt;/p&gt;
&lt;p&gt;Splitting along the reduction dimension in a matmul is known as a &lt;a href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/efficient_gemm.md#parallelized-reductions"&gt;Split-K Matmul&lt;/a&gt;. And just like RMSNorm, using this strategy breaks batch invariance.
&lt;label class="sidenote-number" for="sn10"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;
Another interesting parallelism strategy for matmuls is stream-k. Stream-k is interesting because it has even &lt;i&gt;less&lt;/i&gt; invariance than typical matmuls. As discussed, most matmul libraries are not batch-invariant, but they’re at least what you could call batch-position-invariant (i.e. changing the position of the element &lt;em&gt;within&lt;/em&gt; the batch does not affect numerics). However, stream-k is not batch-position-invariant either! Its core insight is that you can get cleaner load-balancing by splitting along k in different ways for different output tiles, but taking advantage of this makes our kernel not batch-position-invariant either. &lt;/span&gt;&lt;/p&gt;
&lt;figure class="half-width matmul"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/svgs/matmul-03.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;&lt;b&gt;Split-K Matmul&lt;/b&gt; If our batch dimension is fairly small we may not have enough parallelism and require a split-k matmul. In this example, we split each reduction across two cores, which would accumulate separately and then combine their results at the end. However, splitting each reduction across two cores allows us to still leverage eight cores.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;!-- Todo (lowpri): Clean up writing here a bit more --&gt;
&lt;p&gt;There’s an additional complexity with matmuls — tensor core instructions. Whereas with reductions we could simply operate on one row at a time, efficient matrix multiplication kernels must operate on an entire “tile” at a time.&lt;/p&gt;
&lt;p&gt;Each tensor-core instruction (like say, &lt;a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#asynchronous-warpgroup-level-matrix-instructions-wgmma-mma"&gt;&lt;code&gt;wgmma.mma_async.sync.aligned.m64n128k16&lt;/code&gt;&lt;/a&gt;) may have a different reduction order internally. One reason to use a different tensor-core instruction might be that the batch size is very small. For example, if we use a tensor-core PTX instruction that operates on a tile of length 256 but the batch size is only 32, we’re wasting almost all of that compute! At a batch-size of 1, the fastest kernels usually don’t use tensor cores at all.&lt;/p&gt;
&lt;figure class="half-width matmul"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/svgs/matmul-02.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;&lt;b&gt;Padded Tensor-Core Instructions&lt;/b&gt; If the batch size is too small, we may be in our situation where we can't fit even one of our 2D tiles in the output. In this case, it is most efficient to switch to a smaller tensor-core instruction or eschew tensor-cores altogether! However, both of these options prevent our kernel from being batch-invariant.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;So, the easiest way to ensure batch invariance for matmuls is to compile one kernel configuration and use that for all shapes. Although we will lose some performance, this isn’t typically disastrous in LLM inference. In particular, split-k is most needed when &lt;strong&gt;both&lt;/strong&gt; M and N are small, and luckily in our case, N (i.e. the model dim) is usually pretty large!&lt;/p&gt;
&lt;figure class="chart-line" data-labels='{"CuBLAS":{"x":7700,"y":668,"dx":-4,"dy":-8},"Batch-Invariant":{"x":7700,"y":590,"dx":-4,"dy":-8}}' data-series='[{"lineWidth":1.4},{"lineWidth":1.4}]' data-seriesmeta='{"names":["CuBLAS","Batch-Invariant"],"colors":["var(--blue)","#f59e0b"]}' data-show-points="false" data-smooth="5" data-src="/data/batch-vs-cublas.txt" data-x='{"label":"Batch-size","ticks":[0,1000,2000,3000,4000,5000,6000,7000,8200]}' data-y='{"label":"TFLOPs","ticks":[0,100,200,300,400,500,600,700]}'&gt;
&lt;figcaption&gt;Despite obtaining batch invariance, we only lose about 20% performance compared to cuBLAS. Note that this is not an optimized Triton kernel either (e.g. no TMA). However, some of the patterns in performance are illustrative of where our batch-invariant requirement loses performance. First, note that we lose a significant amount of performance at very small batch sizes due to an overly large instruction and insufficient parallelism. Second, there is a "jigsaw" pattern as we increase the batch-size that is caused by quantization effects (both tile and wave) that are typically ameliorated through changing tile sizes. You can find more on these quantization effects &lt;a href="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications"&gt;here&lt;/a&gt;.&lt;/figcaption&gt;
&lt;svg height="auto" viewbox="0 0 800 520" width="100%"&gt;&lt;/svg&gt;
&lt;/figure&gt;
&lt;h3 id="batch-invariant-attention"&gt;Batch-invariant attention&lt;/h3&gt;
&lt;figure class="half-width"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/svgs/attention-01.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;&lt;b&gt;FlashAttention2 Strategy&lt;/b&gt; We parallelize along Q, and reduce along K/V simultaneously. This means that our entire reduction can be kept within a single core, making it another data-parallel strategy.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;After obtaining batch invariance for matmuls, attention introduces two additional wrinkles — fittingly,  because it contains two matmuls.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;As opposed to only reducing over the feature dimension like both RMSNorm and matmuls, we now reduce over the feature dimension &lt;em&gt;and&lt;/em&gt; sequence dimension.&lt;/li&gt;
&lt;li&gt;Due to the above, attention must deal with a variety of inference optimizations that affect how sequences get processed (chunked prefill, prefix caching, etc.).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thus, to achieve determinism in LLM inference our numerics must be invariant to both how many requests are processed at once &lt;strong&gt;and&lt;/strong&gt; how each request gets sliced up in the inference engine.&lt;/p&gt;
&lt;p&gt;Let’s first walk through the standard parallelism strategy for attention, first introduced in FlashAttention2. Similar to RMSNorm and Matmul, the default strategy is a “data-parallel” strategy. Since we reduce along the key/value tensors, a data-parallel strategy can only parallelize along the query tensor.&lt;/p&gt;
&lt;p&gt;For example, depending on the inference engine’s choices, it’s possible that a sequence might get processed in several parts (such as in chunked prefill) or perhaps all at once (if the prefill isn’t split up). In order to achieve “batch invariance”, it’s necessary that the &lt;em&gt;reduction order for a given token does not depend on how many other tokens from its sequence are being simultaneously processed&lt;/em&gt;. If you reduce over the K/V values in the KV cache separately from the K/V values in the current tokens being processed (like in vLLM’s &lt;a href="https://github.com/vllm-project/vllm/blob/0ae43dbf8cb28a299ae724fc742b0c5bcddea868/vllm/attention/ops/prefix_prefill.py#L36"&gt;Triton attention kernel&lt;/a&gt;), this can’t be achieved. For example, when processing the 1000th query token in a sequence, the reduction order must be identical regardless of whether 0 tokens are in the KV cache (prefill) or 999 tokens are in the KV cache (decoding).&lt;/p&gt;
&lt;figure class="half-width"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/svgs/attention-02.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;&lt;b&gt;FlashAttention with a KV Cache&lt;/b&gt; The reason why explicitly handling the KV cache separately from the current KV values breaks batch invariance is a bit subtle and is related to "boundary conditions". In particular, imagine your block size is 32 but we currently have 80 elements in our KV cache. We then compute an additional 48 elements that aren't cached. In this case, we need three blocks (two full and one masked) to compute "P cache" and another two blocks (one full and one masked) to compute "P". This is therefore five total blocks to compute our reduction when we only have four total blocks (i.e. 128) of elements to compute, which will definitely change our reduction order. 
&lt;p&gt;For example, if we instead had no elements in our KV Cache and were processing 128 elements altogether, we need to have identical numerics in both of these situations to ensure “batch invariance” for attention.&lt;/p&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To resolve this, we can just update the KV cache and page table before the attention kernel itself, ensuring that our keys and values are always consistently laid out regardless of how many tokens are being processed.&lt;/p&gt;
&lt;p&gt;With this additional detail (as well as all the things mentioned in the previous section, like consistent tile sizes), we are able to achieve a batch-invariant attention implementation!&lt;/p&gt;
&lt;p&gt;However, there is a significant problem here. Unlike with matrix multiplication, the attention shapes we see in LLM inference often do require a split-reduction kernel, often known as Split-KV or FlashDecoding. This is because if we don’t parallelize along the reduction, we can only parallelize along the batch dimension, head dimension, and “query length” dimension. In the decode stage of attention, query length is very small, and so unless we have a very large batch size we are often unable to saturate the GPU.&lt;/p&gt;
&lt;p&gt;Unfortunately, it’s not as easy to ignore this case as it was for RMSNorm and Matmuls. For example, if you have a very long KV cache, the attention kernel may take a very long time despite only processing one request.&lt;/p&gt;
&lt;figure class="half-width"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/svgs/attention-03.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;&lt;b&gt;Fixed # Split-KV Strategy (i.e. FlashDecode)&lt;/b&gt; If our query length becomes very small (like it does during decoding), we may end up in a situation where there is very little parallelism in our kernel at all. In these cases, we'll need to once again split along the reduction dimension --- the KV dimension this time. The typical strategy for how to split along the KV dimension is to figure out how much parallelism we need and then divide the KV dimension evenly. For example, if our KV length was 1000 and we needed 4 splits, each core would handle 250 elements.
&lt;p&gt;This unfortunately also breaks batch invariance, as our precise reduction strategy depends on how many query tokens from the sequence we’re processing in any given request.&lt;/p&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Furthermore, the split-reduction strategies commonly used for attention also pose challenges for batch invariance. For example, FlashInfer’s “balanced scheduling algorithm” chooses the largest split-size that can still saturate all the GPU’s cores, thus making the reduction strategy not “batch-invariant”. However, unlike with RMSNorm/Matmuls, it’s not sufficient to choose a fixed number of splits regardless of the batch size.&lt;/p&gt;
&lt;p&gt;Instead, to achieve batch invariance, we must adopt a “fixed split-size” strategy. In other words, instead of fixing the # of splits, we fix the size of each split and then end up with a varying number of splits. In this manner, we can guarantee that regardless of how many tokens we’re processing, we always perform the identical reduction order. &lt;label class="sidenote-number" for="sn11"&gt;&lt;/label&gt;&lt;span class="sidenote"&gt;This requires some internal FlexAttention changes that are not included in our code release. We will upstream them in the near future!&lt;/span&gt;&lt;/p&gt;
&lt;figure class="half-width"&gt;
&lt;img alt="" data-zoomable="" loading="lazy" src="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/svgs/attention-04.svg" style="display: block; width: 100%; max-width: 800px; height: auto; margin: 0 auto;"/&gt;
&lt;figcaption&gt;&lt;b&gt;Fixed Size Split-KV Strategy&lt;/b&gt; 
The only difference between this strategy and the previous strategy is that our splits are now "fixed size". For example, if our KV length was 1000, instead of splitting it into four even length 250 splits, we would split it into three fixed-size length 256 splits and one length 232 split.
&lt;p&gt;This allows us to &lt;em&gt;preserve&lt;/em&gt; batch invariance as our reduction strategy is no longer dependent on how many query tokens we’re processing at once!&lt;/p&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id="implementation"&gt;Implementation&lt;/h2&gt;
&lt;p&gt;We provide a demonstration of deterministic inference on top of vLLM by leveraging its FlexAttention backend as well as torch.Library.
Through torch.Library, we’re able to substitute out most of the relevant PyTorch operators in an unintrusive way. You can find the library of “batch-invariant” kernels at &lt;a href="https://github.com/thinking-machines-lab/batch_invariant_ops"&gt;thinking-machines-lab/batch-invariant-ops&lt;/a&gt;, as well as the vLLM example of running in “deterministic” mode.&lt;/p&gt;
&lt;h2 id="experiments"&gt;Experiments&lt;/h2&gt;
&lt;h3 id="how-nondeterministic-are-completions"&gt;How nondeterministic are completions?&lt;/h3&gt;
&lt;p&gt;We use &lt;code&gt;Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/code&gt; and sample 1000 completions at temperature 0 with the prompt “Tell me about Richard Feynman” (non-thinking mode), generating 1000 tokens each. Surprisingly, we generate &lt;em&gt;80&lt;/em&gt; unique completions, with the most common of these occuring 78 times.&lt;/p&gt;
&lt;p&gt;Looking at where the completions differ, we see that the completions are actually identical for the first 102 tokens! The first instance of diverging completions occurs at the 103rd token. All completions generate the sequence “Feynman was born on May 11, 1918, in” However, 992 of the completions go on to generate “Queens, New York” whereas 8 of the completions generate “New York City”.&lt;/p&gt;
&lt;!-- At this point, we can also inspect the logprobs of each generation. # TODO: Fill this out a little more. --&gt;
&lt;p&gt;On the other hand, when we enable our batch-invariant kernels, all of our 1000 completions are identical. This is what we would mathematically expect from our sampler, but we aren’t able to achieve deterministic results without our batch-invariant kernels.&lt;/p&gt;
&lt;!-- ### Deterministic evals
One implication of deterministic completions is deterministic evals.
One benefit we’ve found of fully deterministic LLM inference is that we can also get deterministic evals. This can be advantageous for experiment reproducibility, validating small performance improvements, or simply writing reliable unit tests.

Running X eval, we can see that there is a Y variance in eval scores normally. However, when run in deterministic mode, we get exactly the same eval scores every time. --&gt;
&lt;h3 id="performance"&gt;Performance&lt;/h3&gt;
&lt;p&gt;We have not put a significant effort into optimizing the performance of the batch-invariant kernels here. However, let’s run some experiments to verify that our performance remains usable.&lt;/p&gt;
&lt;p&gt;We will set up an API server with one GPU running Qwen-3-8B, and request 1000 sequences with an output length of between 90 and 110.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th style="text-align: center"&gt;Time (seconds)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;vLLM default&lt;/td&gt;
&lt;td style="text-align: center"&gt;26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Unoptimized Deterministic vLLM&lt;/td&gt;
&lt;td style="text-align: center"&gt;55&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;+ Improved Attention Kernel&lt;/td&gt;
&lt;td style="text-align: center"&gt;42&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Much of the slowdown comes from the fact that the FlexAttention integration in vLLM has not been heavily optimized yet. Nevertheless, we see that performance is not &lt;em&gt;disastrous&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id="true-on-policy-rl"&gt;True on-policy RL&lt;/h3&gt;
&lt;p&gt;As &lt;a href="https://fengyao.notion.site/off-policy-rl"&gt;researchers have noted&lt;/a&gt;, the different numerics between training and inference implicitly turns our on-policy RL into off-policy RL.&lt;/p&gt;
&lt;p&gt;Of course, it is impossible to get bitwise identical results between training and inference if we can’t even get bitwise identical results from two identical inference requests. Then, deterministic inference enables us to also modify our training stack to obtain bitwise identical results between sampling and training, thus resulting in true on-policy RL.&lt;/p&gt;
&lt;p&gt;We run experiments in a RLVR setup on &lt;a href="https://arxiv.org/abs/2502.17387"&gt;Bigmath&lt;/a&gt; with the RL policy initialized from the Qwen 2.5-VL instruct 8B with a max rollout length of 4096.&lt;/p&gt;
&lt;p&gt;If we train without off-policy correction (i.e. importance weighting), our reward collapses partway through training, whereas adding an off-policy correction term  allows training to proceed smoothly. But, if we achieve bitwise identical results between our sampler and trainer, we are fully on policy (i.e. 0 KL divergence) and can also train smoothly.&lt;/p&gt;
&lt;p&gt;We can also plot the KL-divergence in logprobs between our sampler and trainer, where all 3 runs have notably different behavior. When running with importance weighting, it stays around 0.001 with occasional spikes. However, running &lt;em&gt;without&lt;/em&gt; importance weighting eventually leads to a spike in KL-divergence around the same time that reward crashes. And, of course, when running “True On-Policy RL”, our KL-divergence stays flat at 0, indicating that there is &lt;em&gt;no&lt;/em&gt; divergence between the training policy and sampling policy.&lt;/p&gt;
&lt;!-- &lt;figure class="chart-line"
        data-src="/data/wandb_export_2025-09-08T11_16_51.917-07_00.csv"
        data-seriesmeta='{"names":["With importance weighting","No importance weighting"],"colors":["#8ac926","#007f86"]}'
        data-show-points="false"
        data-smooth="35"
        data-x='{"label":"Step","ticks":[0,100,200,400,600,800,900,1000,1100]}'
        data-y='{"label":"Mean_reward","ticks":[-0.1,0.0,0.1,0.2,0.3,0.4,0.5]}'
        data-labels='{"With Importance Weighting":{"x":850,"y":0.5,"dx":-8,"dy":-6},"No Importance Weighting":{"x":850,"y":-0.03,"dx":-8,"dy":-6}}'
        data-series='[{"lineWidth":1.6},{"lineWidth":1.6}]'&gt;
  &lt;figcaption&gt;Without off-policy correction like importance weighting, RL is prone to catastrophic failure due to the "off-policyness" induced by numerical mismatches between the trainer and the sampler. However, if we obtain bitwise equivalence between the trainer and the sampler, we don't need any off-policy correction to train smoothly.&lt;/figcaption&gt;
  &lt;svg viewBox="0 0 800 520" width="100%" height="auto"&gt;&lt;/svg&gt;
  
&lt;/figure&gt; --&gt;
&lt;!-- &lt;figure class="chart-line"
        data-srcs='["/data/reward.csv","/data/kl.csv"]'
        data-seriesmeta='{"colors":["#1f78b4","#e31a1c","#33a02c"],"names":["True On-Policy","No Importance Weighting","With Importance Weighting"]}'
        data-show-points="false"
        data-smooth="0.1"
        data-x='{"label":"Step","ticks":[0,50,100,150,200,250,300,318],"domain":[0,319]}'
        data-y='{"label":"Mean reward","ticks":[0.0,0.1,0.2,0.3,0.4,0.5,0.6], "domain":[-0.1,0.6]}'
        data-y2='{"label":"KL-Divergence","ticks":[0.0,0.001,0.002,0.003,0.004,0.005,0.0055],"domain":[-0.001,0.0055]}'
        data-labels='{"True On-Policy":{"x":215,"y":0.32,"dx":-16,"dy":-4},"No Importance Weighting":{"x":215,"y":0.27,"dx":-16,"dy":-4},"With Importance Weighting":{"x":215,"y":0.22,"dx":-16,"dy":-4}}'
        data-series='[{"lineWidth":1.5},{"lineWidth":1.4},{"lineWidth":2.4}]'&gt;
  &lt;figcaption&gt;Note that the run without importance weighting has a significant loss spike around Step 318, and this comes with a corresponding spike in KL-divergence of logprobs. Meanwhile, either using an off-policy correction or running with "True On-Policy" allows RL to continue smoothly. The blue line showing "True On-Policy" is not a bug --- it's just a flat line at 0. &lt;/figcaption&gt;
  &lt;svg viewBox="0 0 800 520" width="100%" height="auto"&gt;&lt;/svg&gt;
  
&lt;/figure&gt; --&gt;
&lt;figure class="chart-line-2pane" data-bottom-series='[{"lineWidth":1.5},{"lineWidth":1.4},{"lineWidth":2.4}]' data-bottom-y='{"label":"KL-Divergence","ticks":[0.0,0.001,0.002,0.003,0.004,0.005],"domain":[-0.001,0.0055]}' data-labels='{"True on-policy":{"x":215,"y":0.32,"dx":-16,"dy":-4},"No importance weighting":{"x":215,"y":0.27,"dx":-16,"dy":-4},"With importance weighting":{"x":215,"y":0.22,"dx":-16,"dy":-4}}' data-seriesmeta='{"colors":["#f4a261","#007f86","#8ac926"],"names":["True on-policy","No importance weighting","With importance weighting"]}' data-show-points="false" data-smooth="0.1" data-srcs='["/data/reward.csv","/data/kl.csv"]' data-top-series='[{"lineWidth":1.5},{"lineWidth":1.4},{"lineWidth":2.4}]' data-top-y='{"label":"Mean reward","ticks":[0.0,0.1,0.2,0.3,0.4,0.5,0.6], "domain":[-0.1,0.6]}' data-x='{"label":"Step","ticks":[0,50,100,150,200,250,300,318],"domain":[0,319]}'&gt;
&lt;figcaption&gt;Note that the run without importance weighting has a significant loss spike around Step 318, and this comes with a correspond ing spike in KL-divergence of logprobs. Meanwhile, either using an off-policy correction or running with "True On-Policy" allows RL to continue smoothly. The blue line showing "True On-Policy" is not a bug - it's just a flat line at 0. &lt;/figcaption&gt;
&lt;svg height="auto" viewbox="0 0 800 520" width="100%"&gt;&lt;/svg&gt;
&lt;/figure&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Modern software systems contain many layers of abstractions. In machine learning, when we run into nondeterminism and subtle numerical differences it can often be tempting to paper over them. After all, our systems are already “probabilistic”, so what’s wrong with a little more nondeterminism? What’s wrong with bumping up the atol/rtol on the failing unit test? The difference in logprobs between the trainer and the sampler probably isn’t a real bug, right?&lt;/p&gt;
&lt;p&gt;We reject this defeatism. With a little bit of work, we &lt;em&gt;can&lt;/em&gt; understand the root causes of our nondeterminism and even solve them! We hope that this blog post provides the community with a solid understanding of how to resolve nondeterminism in our inference systems and inspires others to obtain a full understanding of their systems.&lt;/p&gt;
&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;
&lt;p&gt;Please cite this work as:&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;He, Horace and Thinking Machines Lab, "Defeating Nondeterminism in LLM Inference", 
Thinking Machines Lab: Connectionism, Sep 2025.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Or use the BibTeX citation:&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;@article{he2025nondeterminism,
  author = {Horace He and Thinking Machines Lab},
  title = {Defeating Nondeterminism in LLM Inference},
  journal = {Thinking Machines Lab: Connectionism},
  year = {2025},
  note = {https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/},
  doi = {10.64434/tml.20250910}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/</guid>
      <category>Research</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
