<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Hacker News</title>
    <link>https://news.ycombinator.com/</link>
    <description>Hacker News front-page links with full article content.</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>en</language>
    <lastBuildDate>Tue, 23 Dec 2025 19:03:44 +0000</lastBuildDate>
    <item>
      <title>We replaced H.264 streaming with JPEG screenshots (and it worked better)</title>
      <link>https://blog.helix.ml/p/we-mass-deployed-15-year-old-screen</link>
      <description>Part 2 of our video streaming saga. Read Part 1: How we replaced WebRTC with WebSockets â†’</description>
      <content:encoded>&lt;article class="typography newsletter-post post"&gt;&lt;h1&gt;We Mass-Deployed 15-Year-Old Screen Sharing Technology and It's Actually Better&lt;/h1&gt;&lt;h3&gt;Or: How JPEG Screenshots Defeated Our Beautiful H.264 WebCodecs Pipeline&lt;/h3&gt;&lt;a href="https://substack.com/@lewq"&gt;&lt;img alt="Luke Marsden's avatar" src="https://substackcdn.com/image/fetch/$s_!fz2p!,w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe83a5474-4943-4f2d-ae84-070ba6dd2042_400x400.jpeg"/&gt;&lt;/a&gt;&lt;a href="https://substack.com/@lewq"&gt;Luke Marsden&lt;/a&gt;Dec 18, 2025311Share&lt;p&gt;&lt;em&gt;Part 2 of our video streaming saga. &lt;a href="https://blog.helix.ml/p/we-killed-webrtc-and-nobody-noticed"&gt;Read Part 1: How we replaced WebRTC with WebSockets â†’&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;The Year is 2025 and Weâ€™re Sending JPEGs&lt;/h2&gt;&lt;p&gt;Let me tell you about the time we spent three months building a gorgeous, hardware-accelerated, WebCodecs-powered, 60fps H.264 streaming pipeline over WebSockets...&lt;/p&gt;&lt;p&gt;Thanks for reading HelixML! Subscribe for free to receive new posts and support my work.&lt;/p&gt;&lt;p&gt;...and then replaced it with &lt;code&gt;grim | curl&lt;/code&gt; when the WiFi got a bit sketchy.&lt;/p&gt;&lt;p&gt;I wish I was joking.&lt;/p&gt;&lt;h2&gt;Act I: Hubris (Also Known As â€œEnterprise Networking Existsâ€)&lt;/h2&gt;&lt;p&gt;Weâ€™re building &lt;a href="https://github.com/helixml/helix"&gt;Helix&lt;/a&gt;, an AI platform where autonomous coding agents work in cloud sandboxes. Users need to watch their AI assistants work. Think â€œscreen share, but the thing being shared is a robot writing code.â€&lt;/p&gt;&lt;p&gt;Last week, we explained how we replaced WebRTC with a custom WebSocket streaming pipeline. This week: why that wasnâ€™t enough.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The constraint that ruined everything:&lt;/strong&gt; It has to work on enterprise networks.&lt;/p&gt;&lt;p&gt;You know what enterprise networks love? HTTP. HTTPS. Port 443. Thatâ€™s it. Thatâ€™s the list.&lt;/p&gt;&lt;p&gt;You know what enterprise networks hate?&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;UDP&lt;/strong&gt; â€” Blocked. Deprioritized. Dropped. â€œSecurity risk.â€&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;WebRTC&lt;/strong&gt; â€” Requires TURN servers, which requires UDP, which is blocked&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Custom ports&lt;/strong&gt; â€” Firewall says no&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;STUN/ICE&lt;/strong&gt; â€” NAT traversal? In &lt;em&gt;my&lt;/em&gt; corporate network? Absolutely not&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Literally anything fun&lt;/strong&gt; â€” Denied by policy&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We tried WebRTC first. Worked great in dev. Worked great in our cloud. Deployed to an enterprise customer.&lt;/p&gt;&lt;p&gt;â€œThe video doesnâ€™t connect.â€&lt;/p&gt;&lt;p&gt;&lt;em&gt;checks network&lt;/em&gt; â€” Outbound UDP blocked. TURN server unreachable. ICE negotiation failing.&lt;/p&gt;&lt;p&gt;We could fight this. Set up TURN servers. Configure enterprise proxies. Work with IT departments.&lt;/p&gt;&lt;p&gt;Or we could accept reality: &lt;strong&gt;Everything must go through HTTPS on port 443.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;So we built a &lt;strong&gt;pure WebSocket video pipeline&lt;/strong&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;H.264 encoding via GStreamer + VA-API (hardware acceleration, baby)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Binary frames over WebSocket (L7 only, works through any proxy)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;WebCodecs API for hardware decoding in the browser&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;60fps at 40Mbps with sub-100ms latency&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We were so proud. We wrote Rust. We wrote TypeScript. We implemented our own binary protocol. We measured things in microseconds.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Then someone tried to use it from a coffee shop.&lt;/strong&gt;&lt;/p&gt;&lt;h2&gt;Act II: Denial&lt;/h2&gt;&lt;p&gt;â€œThe video is frozen.â€&lt;/p&gt;&lt;p&gt;â€œYour WiFi is bad.â€&lt;/p&gt;&lt;p&gt;â€œNo, the video is definitely frozen. And now my keyboard isnâ€™t working.â€&lt;/p&gt;&lt;p&gt;&lt;em&gt;checks the video&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Itâ€™s showing what the AI was doing 30 seconds ago. And the delay is growing.&lt;/p&gt;&lt;p&gt;Turns out, 40Mbps video streams donâ€™t appreciate 200ms+ network latency. Who knew.&lt;/p&gt;&lt;p&gt;When the network gets congested:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Frames buffer up in the TCP/WebSocket layer&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;They arrive in-order (thanks TCP!) but increasingly delayed&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Video falls further and further behind real-time&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Youâ€™re watching the AI type code from 45 seconds ago&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;By the time you see a bug, the AI has already committed it to main&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Everything is terrible forever&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;â€œJust lower the bitrate,â€ you say. Great idea. Now itâ€™s 10Mbps of blocky garbage thatâ€™s &lt;em&gt;still&lt;/em&gt; 30 seconds behind.&lt;/p&gt;&lt;h2&gt;Act III: Bargaining&lt;/h2&gt;&lt;p&gt;We tried everything:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;â€œWhat if we only send keyframes?â€&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This was our big brain moment. H.264 keyframes (IDR frames) are self-contained. No dependencies on previous frames. Just drop all the P-frames on the server side, send only keyframes, get ~1fps of corruption-free video. Perfect for low-bandwidth fallback!&lt;/p&gt;&lt;p&gt;We added a &lt;code&gt;keyframes_only&lt;/code&gt; flag. We modified the video decoder to check &lt;code&gt;FrameType::Idr&lt;/code&gt;. We set GOP to 60 (one keyframe per second at 60fps). We tested.&lt;/p&gt;&lt;p&gt;We got exactly ONE frame.&lt;/p&gt;&lt;p&gt;One single, beautiful, 1080p IDR frame. Then silence. Forever.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;code&gt;[WebSocket] Keyframe received (frame 121), sending
[WebSocket] ...
[WebSocket] ...
[WebSocket] It's been 14 seconds why is nothing else coming
[WebSocket] Failed to send audio frame: Closed&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;checks Wolf logs&lt;/em&gt; â€” encoder still running&lt;/p&gt;&lt;p&gt;&lt;em&gt;checks GStreamer pipeline&lt;/em&gt; â€” frames being produced&lt;/p&gt;&lt;p&gt;&lt;em&gt;checks Moonlight protocol layer&lt;/em&gt; â€” &lt;strong&gt;nothing coming through&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Weâ€™re using &lt;a href="https://games-on-whales.github.io/wolf/stable/"&gt;Wolf&lt;/a&gt;, an excellent open-source game streaming server (seriously, the documentation is great). But our WebSocket streaming layer sits on top of the Moonlight protocol, which is reverse-engineered from NVIDIA GameStream. Somewhere in that protocol stack, &lt;em&gt;something&lt;/em&gt; decides that if youâ€™re not consuming P-frames, youâ€™re not ready for more frames. Period.&lt;/p&gt;&lt;p&gt;We poked around for an hour or two, but without diving deep into the Moonlight protocol internals, we werenâ€™t going to fix this. The protocol wanted all its frames, or no frames at all.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;â€œWhat if we implement proper congestion control?â€&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;looks at TCP congestion control literature&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;closes tab&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;â€œWhat if we just... donâ€™t have bad WiFi?â€&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;stares at enterprise firewall thatâ€™s throttling everything&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;Act IV: Depression&lt;/h2&gt;&lt;p&gt;One late night, while debugging why the stream was frozen again, I opened our screenshot debugging endpoint in a browser tab:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;code&gt;GET /api/v1/external-agents/abc123/screenshot?format=jpeg&amp;amp;quality=70&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The image loaded instantly.&lt;/p&gt;&lt;p&gt;A pristine, 150KB JPEG of the remote desktop. Crystal clear. No artifacts. No waiting for keyframes. No decoder state. Just... pixels.&lt;/p&gt;&lt;p&gt;I refreshed. Another instant image.&lt;/p&gt;&lt;p&gt;I mashed F5 like a degenerate. 5 FPS of perfect screenshots.&lt;/p&gt;&lt;p&gt;I looked at my beautiful WebCodecs pipeline. I looked at the JPEGs. I looked at the WebCodecs pipeline again.&lt;/p&gt;&lt;p&gt;No.&lt;/p&gt;&lt;p&gt;No, we are not doing this.&lt;/p&gt;&lt;p&gt;We are professionals. We implement proper video codecs. We donâ€™t spam HTTP requests for individual frames like itâ€™s 2009.&lt;/p&gt;&lt;h2&gt;Act V: Acceptance&lt;/h2&gt;&lt;p&gt;typescript&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;code&gt;// Poll screenshots as fast as possible (capped at 10 FPS max)
const fetchScreenshot = async () =&amp;gt; {
  const response = await fetch(`/api/v1/external-agents/${sessionId}/screenshot`)
  const blob = await response.blob()
  screenshotImg.src = URL.createObjectURL(blob)
  setTimeout(fetchScreenshot, 100) // yolo
}&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We did it. Weâ€™re sending JPEGs.&lt;/p&gt;&lt;p&gt;And you know what? &lt;strong&gt;It works perfectly.&lt;/strong&gt;&lt;/p&gt;&lt;h2&gt;Why JPEGs Actually Slap&lt;/h2&gt;&lt;p&gt;Hereâ€™s the thing about our fancy H.264 pipeline:&lt;/p&gt;&lt;p&gt;PropertyH.264 StreamJPEG SpamBandwidth40 Mbps (constant)100-500 Kbps (varies with complexity)StateStateful (corrupt = dead)Stateless (each frame independent)Latency sensitivityVery highDoesnâ€™t careRecovery from packet lossWait for keyframe (seconds)Next frame (100ms)Implementation complexity3 months of Rust&lt;code&gt;fetch()&lt;/code&gt; in a loop&lt;/p&gt;&lt;p&gt;A JPEG screenshot is &lt;strong&gt;self-contained&lt;/strong&gt;. It either arrives complete, or it doesnâ€™t. Thereâ€™s no â€œpartial decode.â€ Thereâ€™s no â€œwaiting for the next keyframe.â€ Thereâ€™s no â€œdecoder state corruption.â€&lt;/p&gt;&lt;p&gt;When the network is bad, you get... fewer JPEGs. Thatâ€™s it. The ones that arrive are perfect.&lt;/p&gt;&lt;p&gt;And the size! A 70% quality JPEG of a 1080p desktop is like &lt;strong&gt;100-150KB&lt;/strong&gt;. A single H.264 keyframe is 200-500KB. Weâ€™re sending LESS data per frame AND getting better reliability.&lt;/p&gt;&lt;h2&gt;The Hybrid: Have Your Cake and Eat It Too&lt;/h2&gt;&lt;p&gt;We didnâ€™t throw away the H.264 pipeline. Weâ€™re not &lt;em&gt;complete&lt;/em&gt; animals.&lt;/p&gt;&lt;p&gt;Instead, we built adaptive switching:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Good connection&lt;/strong&gt; (RTT &amp;lt; 150ms): Full 60fps H.264, hardware decoded, buttery smooth&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Bad connection detected&lt;/strong&gt;: Pause video, switch to screenshot polling&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Connection recovers&lt;/strong&gt;: User clicks to retry video&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The key insight: &lt;strong&gt;we still need the WebSocket for input&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;Keyboard and mouse events are tiny. Like, 10 bytes each. The WebSocket handles those perfectly even on a garbage connection. We just needed to stop sending the massive video frames.&lt;/p&gt;&lt;p&gt;So we added one control message:&lt;/p&gt;&lt;p&gt;json&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;code&gt;{"set_video_enabled": false}&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Server receives this, stops sending video frames. Client polls screenshots instead. Input keeps flowing. Everyoneâ€™s happy.&lt;/p&gt;&lt;p&gt;15 lines of Rust. I am not joking.&lt;/p&gt;&lt;p&gt;rust&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;code&gt;if !video_enabled.load(Ordering::Relaxed) {
    continue; // skip frame, it's screenshot time baby
}&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;The Oscillation Problem (Lol)&lt;/h2&gt;&lt;p&gt;We almost shipped a hilarious bug.&lt;/p&gt;&lt;p&gt;When you stop sending video frames, the WebSocket becomes basically empty. Just tiny input events and occasional pings.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The latency drops dramatically.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Our adaptive mode sees low latency and thinks: â€œOh nice! Connection recovered! Letâ€™s switch back to video!â€&lt;/p&gt;&lt;p&gt;Video resumes. 40Mbps floods the connection. Latency spikes. Mode switches to screenshots.&lt;/p&gt;&lt;p&gt;Latency drops. Mode switches to video.&lt;/p&gt;&lt;p&gt;Latency spikes. Mode switches to screenshots.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Forever. Every 2 seconds.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The fix was embarrassingly simple: once you fall back to screenshots, &lt;strong&gt;stay there until the user explicitly clicks to retry&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;typescript&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;code&gt;setAdaptiveLockedToScreenshots(true) // no oscillation for you
```

We show an amber icon and a message: "Video paused to save bandwidth. Click to retry."

Problem solved. User is in control. No infinite loops.

---

## Ubuntu Doesn't Ship JPEG Support in grim Because Of Course It Doesn't

Oh, you thought we were done? Cute.

`grim` is a Wayland screenshot tool. Perfect for our needs. Supports JPEG output for smaller files.

Except Ubuntu compiles it without libjpeg.
```
$ grim -t jpeg screenshot.jpg
error: jpeg support disabled&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;incredible&lt;/em&gt;&lt;/p&gt;&lt;p&gt;So now our Dockerfile has a build stage that compiles grim from source:&lt;/p&gt;&lt;p&gt;dockerfile&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;code&gt;FROM ubuntu:25.04 AS grim-build
RUN apt-get install -y meson ninja-build libjpeg-turbo8-dev ...
RUN git clone https://git.sr.ht/~emersion/grim &amp;amp;&amp;amp; \
    meson setup build -Djpeg=enabled &amp;amp;&amp;amp; \
    ninja -C build
```

We're building a screenshot tool from source so we can send JPEGs in 2025. This is fine.

---

## The Final Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User's Browser                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  WebSocket (always connected)                               â”‚
â”‚  â”œâ”€â”€ Video frames (H.264) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ when RTT &amp;lt; 150ms    â”‚
â”‚  â”œâ”€â”€ Input events (keyboard/mouse) â”€â”€ always               â”‚
â”‚  â””â”€â”€ Control messages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ {"set_video_enabled"} â”‚
â”‚                                                              â”‚
â”‚  HTTP (screenshot polling) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ when RTT &amp;gt; 150ms    â”‚
â”‚  â””â”€â”€ GET /screenshot?quality=70                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Good connection:&lt;/strong&gt; 60fps H.264, hardware accelerated, beautiful &lt;strong&gt;Bad connection:&lt;/strong&gt; 2-10fps JPEGs, perfectly reliable, works everywhere&lt;/p&gt;&lt;p&gt;The screenshot quality adapts too:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Frame took &amp;gt;500ms? Drop quality by 10%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Frame took &amp;lt;300ms? Increase quality by 5%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Target: minimum 2 FPS, always&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;Lessons Learned&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Simple solutions often beat complex ones.&lt;/strong&gt; Three months of H.264 pipeline work. One 2am hacking session the night before production deployment: â€œwhat if we just... screenshots?â€&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Graceful degradation is a feature.&lt;/strong&gt; Users donâ€™t care about your codec. They care about seeing their screen and typing.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;WebSockets are for input, not necessarily video.&lt;/strong&gt; The input path staying responsive is more important than video frames.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ubuntu packages are missing random features.&lt;/strong&gt; Always check. Or just build from source like itâ€™s 2005.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Measure before optimizing.&lt;/strong&gt; We assumed video streaming was the only option. It wasnâ€™t.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;Try It Yourself&lt;/h2&gt;&lt;p&gt;Helix is open source: &lt;a href="https://github.com/helixml/helix"&gt;github.com/helixml/helix&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The shameful-but-effective screenshot code:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;api/cmd/screenshot-server/main.go&lt;/code&gt; â€” 200 lines of Go that changed everything&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;MoonlightStreamViewer.tsx&lt;/code&gt; â€” React component with adaptive logic&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;websocket-stream.ts&lt;/code&gt; â€” WebSocket client with &lt;code&gt;setVideoEnabled()&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The beautiful H.264 pipeline weâ€™re still proud of:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;moonlight-web-stream/&lt;/code&gt; â€” Rust WebSocket server&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Still used when your WiFi doesnâ€™t suck&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;em&gt;Weâ€™re building Helix, open-source AI infrastructure that works in the real world â€” even on terrible WiFi. We started by &lt;a href="https://blog.helix.ml/p/LINK-TO-PART-1"&gt;killing WebRTC&lt;/a&gt;, then we killed our replacement. Sometimes the 15-year-old solution is the right one.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Star us on GitHub: &lt;a href="https://github.com/helixml/helix"&gt;github.com/helixml/helix&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Thanks for reading HelixML! Subscribe for free to receive new posts and support my work.&lt;/p&gt;311Share&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://blog.helix.ml/p/we-mass-deployed-15-year-old-screen</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 18:00:31 +0000</pubDate>
    </item>
    <item>
      <title>Towards a secure peer-to-peer app platform for Clan</title>
      <link>https://clan.lol/blog/towards-app-platform-vmtech/</link>
      <description>While most of the existing Clan framework is dedicated to machine and service management, thereâ€™s more on the horizon. Our mission is to make sure peer-to-peer, user-controlled, community software can beat Big Tech solutions. Thatâ€™s why weâ€™re working on platform fundamentals that would open the way for our FOSS stack to match the usability and convenience of proprietary platforms.</description>
      <content:encoded>&lt;article class="article"&gt;


&lt;img src="https://clan.lol/blog/towards-app-platform-vmtech/post-hero-image_hu_300d99c55181014e.webp"/&gt;


&lt;p&gt;While most of the existing Clan framework is dedicated to machine and service management, thereâ€™s more on the horizon. Our mission is to make sure peer-to-peer, user-controlled, community software can beat Big Tech solutions. Thatâ€™s why weâ€™re working on platform fundamentals that would open the way for our FOSS stack to match the usability and convenience of proprietary platforms.&lt;/p&gt;
&lt;p&gt;Unfortunately, the FOSS world is still lagging behind commercial platforms in some important aspects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Web and mobile apps are strongly sandboxed, so while they can get very aggressive in snooping on the data they &lt;em&gt;are&lt;/em&gt; allowed to access, the enforcement of the isolation model is very robust â€” and there is a model for &lt;em&gt;sharing&lt;/em&gt; data that makes the isolated applications actually useful..
&lt;ul&gt;
&lt;li&gt;Meanwhile in the FOSS world, itâ€™s still extremely common to run software with full access to the userâ€™s account. The only project that has built anything close to a similar platform for local software is Flatpak, which is still not perfect and its main repo has a very lax policy;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Centralized Web services can have â€œmultiple instancesâ€ simply by switching accounts; self-hosting Web services is trivially multi-instance; even Android now provides a multi-instance facility..
&lt;ul&gt;
&lt;li&gt;Meanwhile local software often doesnâ€™t have a global database, but when it does, it can be impossible to make it multi-instance without advanced knowledge;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Commercial apps come with their own always-online remote servers. Users donâ€™t have to think about connecting the clients to the servers, itâ€™s all pre-connected!
&lt;ul&gt;
&lt;li&gt;Meanwhile decentralized community software is stuck between various bad options. Supporting multiple commercial backends is tedious and defeats the point anyway. Self-hosting traditional web servers can get complex and unreliable, and exposes attack surface to the public Web. Direct peer-to-peer connections can be hard to set up and unreliable too, and typically donâ€™t provide asynchronous communication.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Soâ€¦ What do we need to make it possible for communities to share apps install and load &lt;strong&gt;quickly&lt;/strong&gt;, already &lt;strong&gt;pre-connected&lt;/strong&gt; to network services; are isolated to a &lt;strong&gt;worry-free&lt;/strong&gt; level of security, and yet allow for enough &lt;strong&gt;sharing&lt;/strong&gt; via explicit permissions to make them useful?&lt;/p&gt;
&lt;p&gt;The first piece of the puzzle is, unsurprisingly, Nix. The entire Clan project is built on Nix, and the future app platform is no exception. Nix makes it possible to quickly fetch and run any software â€“ thanks to caching, as long as we steer everyone towards using very few common versions of the nixpkgs tree, most downloads could be almost as fast as web app loads.&lt;/p&gt;
&lt;p&gt;Then we have to add a &lt;strong&gt;microVM hypervisor&lt;/strong&gt; with &lt;strong&gt;Wayland and GPU virtualization&lt;/strong&gt; and a side of &lt;strong&gt;D-Bus portals&lt;/strong&gt;â€¦ and we can finally get a glimpse of the future!&lt;/p&gt;





&lt;h2&gt;microVMs&lt;/h2&gt;
&lt;p&gt;Secure isolation is essential for any modern app platform. Hardware-based virtualization is a lot more confidence-inspiring than shared-kernel isolation mechanisms like Linux namespaces. But itâ€™s not &lt;em&gt;only&lt;/em&gt; a security measure. Running apps in VMs also improves environment consistency/reproducibility by ensuring everyone runs the same kernel â€” which can also give us portability, since it enables running on completely different host OSes as well.&lt;/p&gt;
&lt;p&gt;If your experience with virtualization on desktop has only been with booting entire Linux distros under something like VirtualBox, you might be very skeptical of the same technology being involved in launching applications all the time. But thatâ€™s not at all inherent to the use of KVM!&lt;/p&gt;
&lt;p&gt;Conventional VMs feel â€œheavyâ€ â€”slow to launch, big RAM footprint, extra background CPU usage, fixed storage allocation, usually not very well integrated with the host desktopâ€” only because their goal is to simulate a whole another computer within your existing computer. For app isolation, we donâ€™t need that, so the whole stack can be vastly simplified and optimized for high performance and low overhead. The microVM idea was first popularized by AWSâ€™s &lt;a href="https://firecracker-microvm.github.io/"&gt;Firecracker&lt;/a&gt; on the server side, powering instantly-launching event/request handlers in Lambda. A microVM boots directly into the kernel (skipping firmware) and does not emulate any legacy PC hardware, which results in &lt;em&gt;very&lt;/em&gt; fast boot times, on the order of a couple hundred milliseconds.&lt;/p&gt;
&lt;p&gt;Now, has this been used on the client side already? Yes, most prominently by Asahi Linux, motivated by a technical restriction that was preventing Apple machines from playing legacy Windows games. Thatâ€™s the &lt;a href="https://github.com/AsahiLinux/muvm"&gt;muvm&lt;/a&gt; project, powered by &lt;a href="https://github.com/containers/libkrun"&gt;libkrun&lt;/a&gt; â€“ a Firecracker-like VMM provided as a dynamic library so that different frontends could be built. For our platform development, we have indeed adopted muvm (after submitting &lt;a href="https://github.com/AsahiLinux/muvm/pull/192"&gt;some changes&lt;/a&gt; that make it more useful for us), combining it with namespace-based &lt;a href="https://github.com/containers/bubblewrap"&gt;Bubblewrap&lt;/a&gt; to make a script that &lt;a href="https://git.clan.lol/clan/munix"&gt;runs NixOS system closures&lt;/a&gt; in microVMs.&lt;/p&gt;
&lt;p&gt;â€¦Wait, did someone mention playing gamesâ€“ like, highly GPU-demanding games? In a VM? Without a dedicated GPU?&lt;/p&gt;
&lt;h2&gt;Desktop and GPU support&lt;/h2&gt;





&lt;p&gt;In the Beginning (of virtio-gpu), there was the Framebuffer. An emulated computer monitor, a single rectangle representing the entire graphical output of the VM. Then there was &lt;a href="https://docs.mesa3d.org/drivers/virgl.html"&gt;VirGL&lt;/a&gt;, a way to forward the OpenGL API across the VM boundary to make the host render on its GPU on behalf of the VM, so that 3D graphics could be displayed on the emulated monitor. It wasnâ€™t super fast, it wasnâ€™t compatible with the latest GL extensions, it wasnâ€™t very secure, but it was something. With the advent of Vulkan, &lt;a href="https://docs.mesa3d.org/drivers/venus.html"&gt;Venus&lt;/a&gt; was started as the Vulkan version of the same thing.&lt;/p&gt;
&lt;p&gt;Meanwhile, the Chrome OS team was working on adding support for Linux apps. While it was initially based on namespaces, they quickly started working on switching to hardware virtualization. The virtio-gpu device was extended to support arbitrary â€œcross-domainâ€ protocols, making it possible â€”with some wrapping-unwrappingâ€” to forward Unix domain sockets that pass certain types of file descriptors (shared memory and DMA-BUF) to the guest. (Well, initially it was a whole separate virtual device but letâ€™s skip over that.) Googleâ€™s crosvm supports &lt;a href="https://crosvm.dev/book/devices/wayland.html"&gt;connecting to the host Wayland socket&lt;/a&gt; to that facility, and the team wrote &lt;a href="https://chromium.googlesource.com/chromiumos/platform2/+/refs/heads/main/vm_tools/sommelier/README.md"&gt;Sommelier&lt;/a&gt; as the guest-side proxy that exposes a normal Wayland socket to guest apps.&lt;/p&gt;
&lt;p&gt;The part of crosvm responsible for handling the virtio-gpu device was written as a reusable library called Rutabaga (now &lt;a href="https://github.com/magma-gpu/rutabaga_gfx"&gt;living outside of the CrOS repos&lt;/a&gt;), and integrated into other VMMs such as good old Qemu. Sommelier was packaged by various Linux distros as well, and one enthusiast wrote &lt;a href="https://roscidus.com/blog/blog/2021/03/07/qubes-lite-with-kvm-and-wayland/"&gt;an entire alternative to Sommelier&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Meanwhile, there was also a lot to improve in terms of accessing the GPU. As weâ€™ve mentioned already, API forwarding solutions like VirGL/Venus leave a lot to be desired. PCIe passthrough requires a dedicated GPU, or SR-IOV support which GPU vendors have mostly restricted to enterprise models. Howeverâ€¦ of course a better way was possible! Rob Clark presented &lt;a href="https://indico.freedesktop.org/event/2/contributions/53/attachments/76/121/XDC2022_%20virtgpu%20drm%20native%20context.pdf"&gt;DRM native contexts&lt;/a&gt; at XDC 2022: this approach essentially paravirtualizes the kernel-space GPU driver, letting the guest submit hardware-specific commands that the host would run in separate contexts (relying on the same separation as between programs on the host). Thatâ€™s the approach that was picked up by the Asahi Linux project for gaming because of the amazing performance it allows for, but itâ€™s also intended to be a stronger security boundary due to providing way less attack surface on the host (itâ€™s all I/O management rather than implementing complex APIs).&lt;/p&gt;
&lt;p&gt;So, was it possible to take all of this technology and use it? Wellâ€¦ it required quite a bit of debugging and fixing everywhere â€“ but thatâ€™s exactly why I joined! So far Iâ€™ve discovered that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the rutabaga_gfx integration in QEMU (which was the thing we tried to use initially) and other C consumers was broken with the latest versions due to &lt;a href="https://issuetracker.google.com/issues/440386997"&gt;an &lt;code&gt;ifdef&lt;/code&gt; mistake&lt;/a&gt; (&lt;a href="https://github.com/magma-gpu/rutabaga_gfx/pull/9"&gt;fixed&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;itâ€™s not documented everywhere that &lt;a href="https://gitlab.com/qemu-project/qemu/-/issues/2574"&gt;kernel &amp;gt;= 6.13 is required&lt;/a&gt; to be able to touch AMD GPU memory from KVM guests in any way&lt;/li&gt;
&lt;li&gt;Sommelier was &lt;a href="https://issuetracker.google.com/u/2/issues/441537635"&gt;assuming Chromium OS kernel patches&lt;/a&gt; and misinterpreting &lt;code&gt;ioctl&lt;/code&gt; responses on regular mainline Linux&lt;/li&gt;
&lt;li&gt;libkrunâ€™s internal version of rutabaga_gfx contained a tiny strange API modification incompatible with Sommelier/proxy-virtwl and didnâ€™t handle memfd seals (&lt;a href="https://github.com/containers/libkrun/pull/407"&gt;fixed&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;RADV (Radeon Vulkan driver in Mesa) only recognized PCI devices including for virtgpu, ignoring the &lt;code&gt;virtio-mmio&lt;/code&gt; setup used by libkrun (&lt;a href="https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/37281"&gt;fixed&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And weâ€™re continuing with more work in this area.&lt;/p&gt;
&lt;h2&gt;D-Bus / XDG Desktop Portals&lt;/h2&gt;





&lt;p&gt;Application isolation is great, but completely isolated applications tend to have limited usefulness. Thatâ€™s why weâ€™re also integrating &lt;a href="https://flatpak.github.io/xdg-desktop-portal/"&gt;desktop portals&lt;/a&gt; that Flatpak uses â€”at least the file-opening / document portalâ€” into the microVM-based platform.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://git.clan.lol/clan/sidebus"&gt;sidebus&lt;/a&gt; project is inspired by &lt;a href="https://spectrum-os.org/"&gt;Spectrum&lt;/a&gt;â€™s setup for using the document portal with virtiofs to dynamically expose chosen files to the guest, using vsock as the D-Bus transport. It is based on the &lt;a href="https://github.com/dbus2/busd"&gt;busd&lt;/a&gt; broker library, and uses the portal frontend on the host for perfect integration with arbitrary desktop environments.&lt;/p&gt;
&lt;p&gt;With the switch to libkrun however, we are looking at the possibility of making the Camera and Screencast portals working, with full hardware acceleration â€“ by switching to virtgpu cross-domain as the transport instead of vsock. Currently libkrun already has added some PipeWire support to its copy of rutabaga_gfx, however thatâ€™s fixed to one system-wide socket. How these portals work is that for every request they pass a new restricted PipeWire remote socket over D-Bus. So weâ€™re looking to make rutabagaâ€™s cross-domain sockets more generic, to be able to just pass through that whole chain of file descriptor passing.&lt;/p&gt;
&lt;p&gt;(And yes, lots of people are worried about PipeWire attack surface â€” itâ€™s definitely possible to mitigate that with a proxy on the host that would only allow a small validated subset of the PipeWire protocol.)&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Weâ€™re looking to finally make a peer-to-peer community software platform thatâ€™s competitive with commercial ones in terms of security, usability and convenience.
If you want to try it out now, you can! Just follow the installation instructions on our &lt;a href="https://git.clan.lol/clan/munix"&gt;munix project&lt;/a&gt;.
Note that itâ€™s still actively being developed, so if you find any issues, please open up a bug report!&lt;/p&gt;

&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://clan.lol/blog/towards-app-platform-vmtech/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 17:34:22 +0000</pubDate>
    </item>
    <item>
      <title>Fabrice Bellard Releases MicroQuickJS</title>
      <link>https://github.com/bellard/mquickjs/blob/main/README.md</link>
      <description>mquickjs/README.md at main Â· bellard/mquickjs Â· GitHub</description>
      <content:encoded>&lt;main id="js-repo-pjax-container"&gt;






&lt;a href="https://github.com/bellard"&gt;
        bellard
&lt;/a&gt; 
/
&lt;strong&gt;
&lt;a href="https://github.com/bellard/mquickjs"&gt;mquickjs&lt;/a&gt;
&lt;/strong&gt;
Public



&lt;ul&gt;
&lt;li&gt;
&lt;a href="https://github.com/login?return_to=%2Fbellard%2Fmquickjs"&gt; Notifications
&lt;/a&gt; You must be signed in to change notification settings
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/login?return_to=%2Fbellard%2Fmquickjs"&gt; Fork
    11
&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;

&lt;a href="https://github.com/login?return_to=%2Fbellard%2Fmquickjs"&gt; 
          Star
 539
&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;













&lt;/main&gt;</content:encoded>
      <guid isPermaLink="false">https://github.com/bellard/mquickjs/blob/main/README.md</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 17:33:42 +0000</pubDate>
    </item>
    <item>
      <title>Meta is using the Linux scheduler designed for Valve's Steam Deck on its servers</title>
      <link>https://www.phoronix.com/news/Meta-SCX-LAVD-Steam-Deck-Server</link>
      <description>Meta Is Using The Linux Scheduler Designed For Valve's Steam Deck On Its Servers - Phoronix</description>
      <content:encoded>&lt;article class="full"&gt;
&lt;h1&gt;Meta Is Using The Linux Scheduler Designed For Valve's Steam Deck On Its Servers&lt;/h1&gt;
Written by &lt;a href="https://www.michaellarabel.com/"&gt;Michael Larabel&lt;/a&gt; in &lt;a href="https://www.phoronix.com/linux/Linux+Kernel"&gt;Linux Kernel&lt;/a&gt; on 23 December 2025 at 06:10 AM EST. &lt;a href="https://www.phoronix.com/forums/node/1601261"&gt;13 Comments&lt;/a&gt;

&lt;img alt="LINUX KERNEL" src="https://www.phoronix.com/assets/categories/linuxkernel.webp"/&gt;
An interesting anecdote from this month's Linux Plumbers Conference in Tokyo is that Meta (Facebook) is using the Linux scheduler originally designed for the needs of Valve's Steam Deck... On Meta Servers. Meta has found that the scheduler can actually adapt and work very well on the hyperscaler's large servers.&#13;
&lt;br/&gt;
&lt;br/&gt;SCX-LAVD as the Latency-criticality Aware Virtual Deadline scheduler has &lt;a href="https://www.phoronix.com/news/LAVD-Scheduler-Linux-Gaming"&gt;worked out very well for the needs of Valve's Steam Deck&lt;/a&gt; with similar or better performance than &lt;a href="https://www.phoronix.com/search/EEVDF"&gt;EEVDF&lt;/a&gt;. SCX-LAVD has been worked on by Linux consulting firm Igalia under contract for Valve. SCX-LAVD has also seen varying use by the CachyOS Handheld Edition, Bazzite, and other Linux gaming software initiatives.&#13;
&lt;br/&gt;&lt;p&gt;&lt;a href="https://www.phoronix.com/image-viewer.php?id=2025&amp;amp;image=meta_lavd_1_lrg"&gt;&lt;img alt="Steam Deck and Server" src="https://www.phoronix.net/image.php?id=2025&amp;amp;image=meta_lavd_1_med"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;br/&gt;It turns out that besides working well on handhelds, SCX-LAVD can also end up working well on large servers too. The presentation at LPC 2025 by Meta engineers was in fact titled "&lt;em&gt;How do we make a Steam Deck scheduler work on large servers&lt;/em&gt;." At Meta they have explored SCX_LAVD as a "default" fleet scheduler for their servers that works for a range of hardware and use-cases for where they don't need any specialized scheduler.&#13;
&lt;br/&gt;&lt;p&gt;&lt;img alt="Meta SCX LAVD" src="https://www.phoronix.net/image.php?id=2025&amp;amp;image=meta_lavd_2_med"/&gt;&lt;/p&gt;
&lt;br/&gt;They call this scheduler built atop sched_ext as "Meta's New Default Scheduler". LAVD they found to work well across the growing CPU and memory configurations of their servers, nice load balancing between CCX/LLC boundaries, and more. Those wishing to learn more about Meta's use and research into SCX-LAVD can find the Linux Plumbers Conference presentation embedded below along with the &lt;a href="https://lpc.events/event/19/contributions/2099/attachments/1875/4020/lpc-2025-lavd-meta.pdf"&gt;slide deck&lt;/a&gt;.&#13;
&lt;br/&gt;&lt;p&gt;&lt;/p&gt;









&lt;a href="https://www.phoronix.com/forums/node/1601261"&gt;13 Comments&lt;/a&gt; 

&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://www.phoronix.com/news/Meta-SCX-LAVD-Steam-Deck-Server</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 17:08:34 +0000</pubDate>
    </item>
    <item>
      <title>When irate product support customers demand to speak to Bill Gates</title>
      <link>https://devblogs.microsoft.com/oldnewthing/20251223-00/?p=111896</link>
      <description>A colleague of mine who used to work in product support told me that they had a procedure if a customer became irate and demanded to speak with Bill Gates. (This was, of course, back in the days when Bill Gates still ran the company.)</description>
      <content:encoded>&lt;article class="middle-column pe-xl-198" data-clarity-region="article" id="post-111896"&gt;

&lt;p&gt;A colleague of mine who used to work in product support told me that they had a procedure if a customer became irate and demanded to speak with Bill Gates. (This was, of course, back in the days when Bill Gates still ran the company.)&lt;/p&gt;
&lt;p&gt;The product support technician would apologize for not resolving the problem to the customerâ€™s satisfaction, but if the customer continued to demand to speak with The Boss, the technician would indeed transfer the customer.&lt;/p&gt;
&lt;p&gt;The customer was transferred to a special internal phone number, and when the operators saw a call on that line, they took the call and said, â€œBill Gatesâ€™s office.â€ They werenâ€™t actually in Bill Gatesâ€™s office. They were just pretending to be Bill Gatesâ€™s secretary. Their job was to tell the caller that Mr. Gates is currently unavailable, but if the customer leaves a message and their contact information, they will pass the information to Mr. Gates.&lt;/p&gt;
&lt;p&gt;Of course, the information was never actually passed along to Bill. The information went back into the product support channel with a note that the customer was escalated to â€œBill Gatesâ€™s office.â€ The technician who returned the call would probably say something like â€œBill Gates asked me to contact you to follow up on an issue you had earlier.â€&lt;/p&gt;
&lt;!-- .entry-content --&gt;
&lt;!-- AI Disclaimer --&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://devblogs.microsoft.com/oldnewthing/20251223-00/?p=111896</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 16:43:03 +0000</pubDate>
    </item>
    <item>
      <title>Stop Slopware</title>
      <link>https://stopslopware.net/</link>
      <description>Stop Slopware</description>
      <guid isPermaLink="false">https://stopslopware.net/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 15:51:17 +0000</pubDate>
    </item>
    <item>
      <title>Test, don't (just) verify</title>
      <link>https://alperenkeles.com/posts/test-dont-verify/</link>
      <description>AI is making formal verification go mainstream.</description>
      <content:encoded>&lt;article&gt;


        Test, don't (just) verify.


                    
                        Posted on 2025-12-22
                    

                    

                    :: 
  
  
  
  13 min read


                    
                    
                             :: Tags:

&lt;a href="https://alperenkeles.com/tags/software-engineering/"&gt;ğŸ·software engineering&lt;/a&gt;, 
                                
                                    &lt;a href="https://alperenkeles.com/tags/testing/"&gt;ğŸ·testing&lt;/a&gt;, 
                                
                                    &lt;a href="https://alperenkeles.com/tags/formal-methods/"&gt;ğŸ·formal methods&lt;/a&gt;




&lt;p&gt;AI &lt;em&gt;is&lt;/em&gt; making formal verification go mainstream.&lt;/p&gt;
&lt;p&gt;AI-assisted mechnical proving companies are raising funds on billion dollar &lt;a href="https://www.reuters.com/business/robinhood-ceos-math-focused-ai-startup-harmonic-valued-145-billion-latest-2025-11-25/"&gt;valuations&lt;/a&gt;, new people are trying proof assistants,
overwhelmingly Lean, at unprecedented rates. Models achieve fascinating results in competitions previously considered to contain
some of the hardest problems in the world, such as IMO, ICPC, Putnam; as well as open problems in mathematics such as ErdÃ¶s Problems.
It's not just the hobbyists that are excited about AI-assisted proofs, from &lt;a href="https://terrytao.wordpress.com/wp-content/uploads/2024/03/machine-jan-3.pdf"&gt;Terry Tao&lt;/a&gt;, to &lt;a href="https://martin.kleppmann.com/2025/12/08/ai-formal-verification.html"&gt;Martin Kleppman&lt;/a&gt;, to &lt;a href="https://x.com/ilyasergey/status/1989053674552004749"&gt;Ilya Sergey&lt;/a&gt;, prominent researchers around the world are excited and hopeful about the effects.&lt;/p&gt;
&lt;h2&gt;&lt;a href="#formal-verification-the-goods"&gt;Formal Verification: The Goods&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Let me quickly give you a run down of the argument:&lt;/p&gt;
&lt;p&gt;There are multiple complex challenges in formal verification. The first one, and the one that is very hard to solve technically,
is that most software in the world does not have a formal specification. A formal specification is a simpler mathematical description
of the system we build. Algorithms have formal specifications. Data structures, protocols, data formats, safety-critical systems typically
have formal specifications. The majority of the programs in the world doesn't have a formal specification, hell, most of them don't even have
informal specifications. At the limit, which is where we actually are, the specification of a program is itself, the implementation is the
specification. The lack of a formal specification makes it very hard to formally verify some piece of software, because what would you even
verify?&lt;/p&gt;
&lt;p&gt;The second issue is, proof engineering, the practice of writing proofs for theorems about your systems, is very hard. The proofs have many domain
specific elements to them, a proof of a mathematical theorem will be very different from a proof about a programming language, and a proof about the programming
language will highly depend on the underlying constructs of its theoretical framework. The widest taught proof engineering book
is &lt;a href="https://softwarefoundations.cis.upenn.edu"&gt;Software Foundations&lt;/a&gt;, and every chapter has a different style of proofs. Someone that went through
Volume 2: Programming Language Foundations will not find the problems in Volume 6: Separation Logic Foundations intuitive or obvious. There are other problems
such as the tooling for proof automation, brittleness of proofs, reusability of proofs etc. but I don't find them particularly fundamental to proof engineering
itself but rather problems of the current generation, so we can leave those aside for now.&lt;/p&gt;
&lt;p&gt;The rise of LLMs in programming vastly affects both of these points. It affects point number 1 because AI-assisted programming is a very natural fit
fot specification-driven development. AI-assisted programming pushes the limits of programming from what you can implement to what you can specify and
what you can &lt;a href="https://alperenkeles.com/posts/verifiability-is-the-limit/"&gt;verify&lt;/a&gt;. This is a great incentive for writing executable specifications,
because then you can put the LLM inside a loop until it achieves the said objective, irrespective of the means of the achievement.
&lt;a href="https://alperenkeles.com/posts/verifiable-abstractions/"&gt;I predict that&lt;/a&gt; this will give rise to program optimizers and translators that will
be transformative of our work in those domains. However, tests are, as infamously they are, incomplete. Tests are great at finding bugs (actually
they are not so great most of the time, but that's another discussion), but they &lt;strong&gt;cannot&lt;/strong&gt; prove the absence of bugs. SQLite famously has
millions of tests, but researchers still find bugs in SQLite, similar situations arise in almost all software.&lt;/p&gt;
&lt;p&gt;The only way to prove the absence of bugs is formal verification, and industry has seen great examples of this. Highly cited formal verification
projects include CompCert C Compiler, &lt;a href="https://users.cs.utah.edu/~regehr/papers/pldi11-preprint.pdf"&gt;the random testing of which against GCC and Clang&lt;/a&gt;
has led to finding 79 bugs in GCC and 202 bugs in Clang, and only 2 bugs in CompCert in its &lt;em&gt;unverified&lt;/em&gt; parser, finding no bugs in its verified compilation
pass, a striking win for formal verification. (Thanks to &lt;a href="https://emptysqua.re/blog/"&gt;A. Jesse Jiryu Davis&lt;/a&gt; informing me, I learned that researchers have
&lt;a href="https://www.cs.purdue.edu/homes/pfonseca/papers/eurosys2017-dsbugs.pdf"&gt;studied&lt;/a&gt; the source of failures in formally verified distributed systems, and
found that wrong assumptions about the unverified parts of the system are the likely culprit.)&lt;/p&gt;
&lt;p&gt;This makes formal verification a prime target for AI-assisted programming. Given that we have a formal specification, we can just let the machine wander around
for hours, days, even weeks. If it comes back with a solution, we shall trust not to the probabilistic predictions of the so-called artificial intelligence,
but the symbolic proof checker that verifies the solution. This idea has always been at the core of formal verification, the ability to have unsound proof
generation coupled with sound proof checking has given rise to complex tactics, algorithms that produce proofs by searching them, to enable proof engineers
in great capacity.&lt;/p&gt;
&lt;p&gt;This is not the end of the good news. AI is also very good at writing proofs, at least much better than the average software engineer. Given that we have
a perfect oracle, we can also turn the problem into an RLVR (Reinforcement Learning with Verifiable Rewards), which means we should expect the models to get
even better at it as time goes on as they did in chess, go, and many other similar problems. This is the promise behind the billion dollar valuations,
the companies started with impressive autoformalization techniques and automated provers for tackling competition problems and open conjectures, but
the real industrial value is at the core of automating software engineering by letting the engineer write a verbal description, autoformalized into a Lean
theorem, proven by the automated prover, and voila, we have our program that we can fully trust.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;or do we?&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a href="#formal-verification-the-bads"&gt;Formal Verification: The Bads&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I see the appeal, I like the idea, I trust the researchers building these systems, but I don't love the overarching promises. This blog post
is my attempt at building a reasonable middle ground by laying out the goods (as I already did), and the bads (as I now will), and make my closing
remarks by reiterating the position of testing in this space, and in the future.&lt;/p&gt;
&lt;h3&gt;&lt;a href="#autoformalization-is-a-shaky-ground"&gt;Autoformalization is a shaky ground&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I started this post by stating most programs in the world do not have formal specifications, followed by how AI is incentivizing us to write specifications,
and autoformalization takes the specifications, and makes them &lt;strong&gt;formal&lt;/strong&gt;. In formal verification, there's this concept of a &lt;em&gt;trusted computing base (TCB)&lt;/em&gt;. TCB
is your Achilles' Heel, it's the bottom of the ladder, where layers over layers of verification is built on, trusting a small core without verifying it, because
there must be a bottom of the ladder, we cannot build a circular verification argument, and the system cannot verify itself. (please fact check me on this
if I'm wrong, which is possible)&lt;/p&gt;
&lt;p&gt;Autoformalization is part of the TCB in this AI-assisted verified programming paradigm, because one cannot mechanically verify that the verbally stated
specification indeed corresponds to the formalized one. There are several modes of usage, one issue is soundness, there might be mechanically verified scenarios
that would be rejected by the verbal specification. Another issue is completeness, the formalized model might reject valid scenarios in our descriptions.
Autoformalization part of the process requires and deserves our special attention as it's one of the crucial points of failure in this verification process.&lt;/p&gt;
&lt;h3&gt;&lt;a href="#proof-assistants-are-slow"&gt;Proof assistants are slow&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In a proof assistant, the primary goal is reasoning about programs and ease of verification. For instance, proof assistants, traditionally, don't use our classic
two's complement integers packed into words in our memory, they use Peano numbers, an encoding of natural numbers as inductive data structures:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Inductive Nat : Type :=
| zero : Nat
| succ : Nat -&amp;gt; Nat.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This encoding doesn't possess the concept of an integer overflow. Its inductive structure allows us to build theorems that hold for all natural numbers, not
just the ones that can fit in a word. It is also painfully slow, the computational complexity of &lt;code&gt;a + b&lt;/code&gt;, an operation so fast in CPU that it's literally
an instant, is &lt;code&gt;O(a + b)&lt;/code&gt;, addition is linear in time to the added values instead of a constant operation. However, we would like to run verified code on
real life workloads, so we cannot wait for a million cycles to add 1M + 1M = 2M. There are 2 solutions to this problem, the first one is that you build a
more efficient encoding in the proof assistant, prove equivalence of the efficient but hard to reason encoding to the inefficient but simpler to reason one,
and use the efficient one in your computations. The other is once again, axiomatization, or broadening the TCB. Proof assistants offer a mechanism called
&lt;strong&gt;extraction&lt;/strong&gt; that allows for extracting a piece of code in the language of the proof assistant (e.g Rocq) to a language with a larger ecosystem optimized
for running production workloads (e.g OCaml). Much of the extraction is a one-to-one correspondence between the languages via syntactic transformations,
but we can hijack the extraction to axiomatize our own types. We can turn Nat into unsigned integers, where &lt;code&gt;Nat.add&lt;/code&gt; becomes &lt;code&gt;u64 + u64&lt;/code&gt;. For instance,
in Rocq, Nats are extracted to BigInts, which should have the same semantics, but the "should" in this sentence carries the heavyweight. Without an accompanying
proof of correctness, we just put BigInt into the TCB.&lt;/p&gt;
&lt;p&gt;Without broadening the TCB, the speed of a proof assistant will be limited by the large amount of pointer chasing that arises naturally with the use of
inductive types and their tree-inducing characteristics. There are many domains in which speed isn't that big of a deal, but I think there's also a
concern between speed and the requirement for correctness, as faster code tends to involve more complex language constructs such as concurrency,
bit manipulation, layout awareness, which leads to more bugs than their simpler counterparts. If we cannot reason about programs that are more likely
to have bugs, how much of the overarching problem are we tackling?&lt;/p&gt;
&lt;h3&gt;&lt;a href="#verification-requires-models-and-models-are-very-hard-to-come-by"&gt;Verification requires models, and models are very hard to come by&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In order to verify some property of a system, one needs a model of the system. These models don't grow on trees, they are crafted by domain
experts over a number of years. People have been building models for programs with pointers (separation logic), as well as programs with
concurrency, programs with randomness, programs with algebraic effects, and perhaps many more that I haven't even heard of. In proving a property
of the system, we need a foundation for our reasoning process, which these theories give us for their respective domains. However, there are many
domains we don't have good models for, one example is runtime performance. This has been a contentious issue in the computer science curriculum, the
asymptotic complexity does not translate to program behavior in real hardware. Modern CPUs have cache lines, speculative execution, branch prediction
that makes the plain old abstract machine used in asymptotical analysis obsolete for many scenarios. We do not even have a single hardware to conform
our model to, we have tens of different configurations of hardware, each of which will give different results in our measurements. If we tried to prove
that some piece of code has a better result for a specific memory/processor pair than another one, we would have a massive job on our hands, I personally
don't even know where I would start with.&lt;/p&gt;
&lt;p&gt;Contrast this to testing, where you can just spin up the same algorithm on both machines, run our benchmarks, which will be our final ground truth. Testing
is almost universally considered inferior to formal verification, it is what you do when you don't have the resources to justify verification, because if you
had the opportunity to spend the time, proving absence of bugs, or universal facts about your system, would be much more valuable than codifiying the results
of singular measurements. I am here to tell you that there are tests we can write that could not be formally verified, because while building the underlying
models for verification might be hard, just using the real hardware for our measurements can be much easier.&lt;/p&gt;
&lt;h3&gt;&lt;a href="#verification-doesn-t-tell-you-that-you-are-going-down-the-wrong-path"&gt;Verification doesn't tell you that you are going down the wrong path&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In games, there are clear winners and losers. In verification, you can prove that your theorem is correct, you can prove that your theorem is incorrect, but
absence of a proof for a theorem does not imply that the theorem is incorrect, it is possible that you just haven't found the proof. This means the feedback
you get when writing a proof is limited as you cannot rely on your inability to progress as a signal about your theorem, it is plausible that you are the problem.
This is why QuickChick, a testing tool descending from the famous QuickCheck of Haskell that introduced Property-Based Testing to the world, exists in Rocq
ecosystem as one of the three most popular packages. If verification was strictly superior to testing, QuickChick would have no reason to exist, but it serves
a very crucial role that the verification process needs, &lt;strong&gt;falsification&lt;/strong&gt;. I had said that the absence of a proof does not imply that a theorem is wrong, but the witness of a violation of the theorem definitely does. Random testing is the prime suspect for finding such counterexamples, pulling the verifier out of the
rabbit hole of going through many useless paths in the proof search before giving up, because the theorem is ultimately wrong, there is no proof to be found.&lt;/p&gt;
&lt;h2&gt;&lt;a href="#random-testing-and-formal-verification"&gt;Random Testing and Formal Verification&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I have given examples of tests that formal verification is incapable of modeling, and examples of tests that complement formal verification process by creating
a short that falsifies false theorems instead of trying to prove them in vain. The synergy between testing and formal verification doesn't end here, I am a firm
supporter of &lt;a href="https://aws.amazon.com/blogs/opensource/lean-into-verified-software-development/"&gt;Verification-Guided Development (VGD)&lt;/a&gt;, which in addition to leveraging this synergy, solves the problem of proof
assistants being too slow. In verification
guided development, we implement two versions of the same system, one is the simpler to reason, verified version, the other is the complex, production one. We then
test the property that the production system conforms to the reference implementation that is verified by running them with the same inputs and asserting that
the result is the same every single time. VGD &lt;em&gt;lifts&lt;/em&gt; the proof to the production implementation from the slower one implemented in the proof assistant by leveraging
differential random testing, which allows for building a best-of-both-worlds system that is both correct and fast. Below is an image taken from &lt;a href="https://arxiv.org/abs/2407.01688"&gt;the paper&lt;/a&gt;
that (as far as I know) introduced the notion of VGD, explaining their workflow.&lt;/p&gt;
&lt;p&gt;&lt;img alt="cedar vgd" src="https://alperenkeles.com/posts/test-dont-verify/image.png"/&gt;&lt;/p&gt;
&lt;p&gt;VGD doesn't solve all the issues I mention in the rest of the post, but it removes the slowness out of the mix. As we have a production implementation ready to be tested thoroughly,
we can make all kinds of measurements that fall out of the purview of verification, but into the realm of testing. It levels the playing field between the verified realm of computing
and the unverified one, reducing the downsides of the verification leveraging testing.&lt;/p&gt;
&lt;h2&gt;&lt;a href="#closing-remarks"&gt;Closing Remarks&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I would like to state it once more for everyone:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I see the appeal, I like the idea, I trust the researchers building these systems, but I don't love the overarching promises. This blog post
is my attempt at building a reasonable middle ground by laying out the goods, and the bads, and make my closing
remarks by reiterating the position of testing in this space, and in the future.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I believe random testing plays as important of a role as formal verification in the future of software engineering. We will not have magically formally verified systems
in many domains, but as autoformalization tools get better, we will have many, many more formal specifications. Random testing benefits from these formal specifications
in different ways than formal verification, but both have their places. Proof systems will be incomplete without the accompanying testing tools, and the testing tools
will be incomplete without proofs of correctness, it is only possible via some combination of verification and testing that we can achieve our ideals of the future of
software engineering, live in a world where bugs are considered anomalies instead of the norm, where correctness is a virtue, and the bugs in our systems are as old and
as forgotten as the diseases we learned to cure and put away.&lt;/p&gt;

&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://alperenkeles.com/posts/test-dont-verify/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 12:56:50 +0000</pubDate>
    </item>
    <item>
      <title>Ryanair fined â‚¬256M over â€˜abusive strategyâ€™ to limit ticket sales by OTAs</title>
      <link>https://www.theguardian.com/business/2025/dec/23/ryanair-fined-limit-online-travel-agencies-ticket-sales-ota</link>
      <description>Italyâ€™s competition authority says Irish airline implemented technical obstacles to force sales through its own website</description>
      <content:encoded>&lt;article class="dcr-1xdhyk6" style="background-color:var(--article-background)"&gt;&lt;img alt="Passengers board a Ryanair Boeing 737 MAX in Cagliari, Italy, through its rear entrance; a wing of the plane with gold lettering reading Ryanair looms over the photographer." src="https://i.guim.co.uk/img/media/0c0d402539470b70559e972804fa15cf214418f2/0_0_3780_3024/master/3780.jpg?width=465&amp;amp;dpr=1&amp;amp;s=none&amp;amp;crop=none"/&gt; Ryanair has reached a record valuation of â‚¬31bn (Â£27bn), making it the worldâ€™s second most valuable airline behind Delta. Photograph: John Keeble/Getty Images&lt;a href="#img-1"&gt;View image in fullscreen&lt;/a&gt;Ryanair has reached a record valuation of â‚¬31bn (Â£27bn), making it the worldâ€™s second most valuable airline behind Delta. Photograph: John Keeble/Getty Images&lt;h1&gt;Ryanair fined â‚¬256m over â€˜abusive strategyâ€™ to limit ticket sales by online travel agencies&lt;/h1&gt;&lt;p&gt;Italyâ€™s competition authority says Irish airline implemented technical obstacles to force sales through its own website&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.theguardian.com/business/live/2025/dec/23/tesla-sales-fall-europe-byd-surges-car-industry-stock-market-ftse-business-live-news-updates"&gt;Business live â€“ latest updates&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Ryanair has been fined â‚¬256m (Â£223m) by Italyâ€™s competition authority for abusing its dominant market position to limit sales of tickets by online travel agents.&lt;/p&gt;&lt;p&gt;The authority said Europeâ€™s largest airline had â€œimplemented an abusive strategy to hinder travel agenciesâ€ via an â€œelaborate strategyâ€ of technical obstacles for agents and passengers to make it difficult for online travel agents to sell Ryanair tickets and instead force sales through its own website.&lt;/p&gt;&lt;p&gt;The fine related to Ryanairâ€™s conduct between April 2023 and at least until April 2025, the authority said on Tuesday. It said Ryanair had prevented online travel agents from selling tickets on its flights in combination with other airlines and services, weakening competition.&lt;/p&gt;&lt;p&gt;Ryanair said it would immediately appeal against the â€œlegally flawedâ€ ruling.&lt;/p&gt;&lt;p&gt;The Ryanair chief executive, &lt;a href="https://www.theguardian.com/business/michael-oleary"&gt;Michael Oâ€™Leary&lt;/a&gt;, had decided to wage war on what he &lt;a href="https://www.theguardian.com/business/2024/jan/29/ryanair-cuts-profits-forecast-flights-lastminute-opodo"&gt;described as â€œpirateâ€ travel agents&lt;/a&gt;, such as Booking.com, Kiwi and Kayak. Oâ€™Leary accused the travel agent industry of scamming and ripping off unsuspecting consumers by charging extra fees and markups on ticket prices.&lt;/p&gt;&lt;p&gt;Oâ€™Leary was prepared to accept lower ticket sales as he tried to prevent travel agents from selling tickets, forcing their passengers to fill out extra forms supposedly as a security measure. The abrupt removal of Ryanair flights from agentsâ€™ websites in late 2023 &lt;a href="https://www.theguardian.com/business/2024/jan/03/ryanair-ticket-sales-hit-after-travel-agent-websites-delist-airline"&gt;caused a drop in sales for the airline&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The lower sales &lt;a href="https://www.theguardian.com/business/2024/jan/29/ryanair-cuts-profits-forecast-flights-lastminute-opodo"&gt;dented Ryanairâ€™s profits&lt;/a&gt;, although they have not prevented the Irish airline from rising to a record valuation of â‚¬31bn (Â£27bn). That has made it the worldâ€™s second most valuable airline, behind only the USâ€™s Delta Air Lines.&lt;/p&gt;&lt;p&gt;Oâ€™Leary â€“ who is known for his combative and often sweary criticisms of airports, rivals and regulators â€“ is planning to hand over control of the business to a successor within the next five to 10 years. He will be &lt;a href="https://www.theguardian.com/business/2023/dec/18/ryanair-boss-michael-oleary-heads-for-100m-bonus"&gt;given shares worth â‚¬111m&lt;/a&gt; (Â£97m) if he stays at the airline until the end of July 2028. He was already a billionaire on paper because of his shareholding.&lt;/p&gt;&lt;p&gt;Responding to the ruling, Oâ€™Leary said it was â€œan affront to consumer protection and competition lawâ€.&lt;/p&gt;&lt;p&gt;He added: â€œThe internet and the ryanair.com website have enabled Ryanair to distribute directly to consumers, and Ryanair has passed on these 20% cost savings in the form of the lowest air fares in &lt;a href="https://www.theguardian.com/world/italy"&gt;Italy&lt;/a&gt; and Europe.&lt;/p&gt;&lt;p&gt;â€œRyanair looks forward to successfully overturning this legally flawed ruling and its absurd â‚¬256m fine in the courts.â€&lt;/p&gt;&lt;p&gt;The vast majority of Ryanairâ€™s sales took place through its website even before the battle against online travel agents. However, the Italian authority said Ryanair had been guilty of â€œabuse of a dominant positionâ€ and using its â€œsignificant market powerâ€ in trying to stamp out the business.&lt;/p&gt;&lt;p&gt;Ryanairâ€™s tactics included rolling out facial recognition procedures for people who bought tickets via a third party, claiming that was necessary for security. It then â€œtotally or intermittently blocked booking attempts by travel agenciesâ€, including by blocking payment methods and mass-deleting accounts.&lt;/p&gt;&lt;p&gt;The airline then â€œimposed partnership agreementsâ€ on agencies that banned sales of Ryanair flights in combinations with other carriers, and blocked bookings to force them to sign up. Only in April this year did it allow agenciesâ€™ websites to link up with its own services, allowing effective competition.&lt;/p&gt;&lt;p&gt;The competition authority said Ryanairâ€™s actions had â€œblocked, hindered or made such purchases more difficult and/or economically or technically burdensome when combined with flights operated by other carriers and/or other tourism and insurance servicesâ€.&lt;/p&gt;Explore more on these topics&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/business/ryanair"&gt;Ryanair&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/business/michael-oleary"&gt;Michael O'Leary&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/business/theairlineindustry"&gt;Airline industry&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/world/ireland"&gt;Ireland&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/world/italy"&gt;Italy&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/world/europe-news"&gt;Europe&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/business/regulators"&gt;Regulators&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/tone/news"&gt;news&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;a href="mailto:?subject=Ryanair fined â‚¬256m over â€˜abusive strategyâ€™ to limit ticket sales by online travel agencies&amp;amp;body=https://www.theguardian.com/business/2025/dec/23/ryanair-fined-limit-online-travel-agencies-ticket-sales-ota?CMP=share_btn_url"&gt;Share&lt;/a&gt;&lt;a href="https://syndication.theguardian.com/?url=https%3A%2F%2Fwww.theguardian.com%2Fbusiness%2F2025%2Fdec%2F23%2Fryanair-fined-limit-online-travel-agencies-ticket-sales-ota&amp;amp;type=article&amp;amp;internalpagecode=business/2025/dec/23/ryanair-fined-limit-online-travel-agencies-ticket-sales-ota"&gt;Reuse this content&lt;/a&gt;&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://www.theguardian.com/business/2025/dec/23/ryanair-fined-limit-online-travel-agencies-ticket-sales-ota</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 10:53:07 +0000</pubDate>
    </item>
    <item>
      <title>Font with Built-In Syntax Highlighting (2024)</title>
      <link>https://blog.glyphdrawing.club/font-with-built-in-syntax-highlighting/</link>
      <description>I have been trying to identify practical reasons why hand-coding websites with HTML and CSS is so hard ( by hand-coding, I mean not relying on frameworks, generators or 3rd party scripts that modify the DOM ).</description>
      <content:encoded>&lt;article class="container"&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;a href="https://blog.glyphdrawing.club/tags/Blog"&gt;#Blog&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://blog.glyphdrawing.club/tags/Type Design"&gt;#Type Design&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://blog.glyphdrawing.club/tags/Fonts"&gt;#Fonts&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://blog.glyphdrawing.club/tags/CSS"&gt;#CSS&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://blog.glyphdrawing.club/tags/Typography"&gt;#Typography&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Syntax Highlighting in Hand-Coded Websites&lt;/h2&gt;
&lt;h3&gt;The problem&lt;/h3&gt;
&lt;p&gt;I have been trying to identify practical reasons why hand-coding websites with HTML and CSS is so hard (&lt;em&gt;by hand-coding, I mean not relying on frameworks, generators or 3rd party scripts that modify the DOM&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Let's say, I want to make a blog. What are the &lt;strong&gt;actual&lt;/strong&gt; things that prevent me from makingâ€”and maintainingâ€”it by hand? What would it take to clear these roadblocks?&lt;/p&gt;
&lt;p&gt;There are many, of course, but for a hand-coded programming oriented blog one of these roadblocks is &lt;strong&gt;syntax highlighting&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;When I display snippets of code, I want to make the code easy to read and understand by highlighting it with colors. To do that, I would normally need to use a complex syntax highlighter library, like &lt;a href="https://prismjs.com/"&gt;Prism&lt;/a&gt; or &lt;a href="https://highlightjs.org/"&gt;highlight.js&lt;/a&gt;. These scripts work by scanning and chopping up the code into small language-specific patterns, then wrapping each part in tags with special styling that creates the highlighted effect, and then injecting the resulting HTML back into the page.&lt;/p&gt;
&lt;p&gt;But, I want to write code by hand. I don't want any external scripts to inject things I didn't write myself. Syntax highlighters also add to the overall complexity and bloat of each page, which I'm trying to avoid. I want to keep things as simple as possible.&lt;/p&gt;
&lt;h3&gt;Leveraging OpenType features to build a simple syntax highlighter inside the font&lt;/h3&gt;
&lt;p&gt;This lead me to think: &lt;strong&gt;could it be possible to build syntax highlighting directly into a font&lt;/strong&gt;, skipping JavaScript altogether? Could I somehow leverage OpenType features, by creating colored glyphs with the COLR table, and identifying and substituting code syntax with contextual alternates?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;div class="spoilers"&amp;gt;
  &amp;lt;strong&amp;gt;Yes, it's possible!&amp;lt;/strong&amp;gt;
  &amp;lt;small&amp;gt;...to some extent =)&amp;lt;/small&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The colors in the HTML snippet above &lt;strong&gt;comes from within the font itself&lt;/strong&gt;, the code is &lt;strong&gt;plain text&lt;/strong&gt;, and requires &lt;strong&gt;no JavaScript&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To achieve that, I modified an open source font Monaspace Krypton to include colored versions of each character, and then used OpenType contextual alternates to essentially find &amp;amp; replace specific strings of text based on HTML, CSS and JS syntax. The result is a simple syntax highlighter, &lt;strong&gt;built-in&lt;/strong&gt; to the font itself.&lt;/p&gt;
&lt;p&gt;If you want to try it yourself, download the font: &lt;a href="https://blog.glyphdrawing.club/assets/fonts/FontWithASyntaxHighlighter-Regular.woff2"&gt;FontWithASyntaxHighlighter-Regular.woff2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And include the following bits of CSS:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@font-face {
  font-family: 'FontWithASyntaxHighlighter';
  src: 
    url('/FontWithASyntaxHighlighter-Regular.woff2') 
    format('woff2')
  ;
}
code {
  font-family: "FontWithASyntaxHighlighter", monospace;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that's it!&lt;/p&gt;
&lt;h2&gt;What are the Pros and Cons of this method?&lt;/h2&gt;
&lt;p&gt;This method opens up some interesting possibilities...&lt;/p&gt;
&lt;h3&gt;Pros&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Install is as easy as using any custom font.&lt;/li&gt;
&lt;li&gt;Works without JavaScript.&lt;/li&gt;
&lt;li&gt;Works without CSS themes.&lt;/li&gt;
&lt;li&gt;...but can be themed with CSS.&lt;/li&gt;
&lt;li&gt;It's fast.&lt;/li&gt;
&lt;li&gt;Snippets of code can be put into &lt;code&gt;&amp;lt;pre&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;code&amp;gt;&lt;/code&gt;, with no extra classes or &lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt;s.&lt;/li&gt;
&lt;li&gt;Clean HTML source code.&lt;/li&gt;
&lt;li&gt;Works everywhere that supports OpenType features, like InDesign.&lt;/li&gt;
&lt;li&gt;Doesn't require maintenance or updating.&lt;/li&gt;
&lt;li&gt;Works in &lt;code&gt;&amp;lt;textarea&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt;! Syntax highlighting inside &lt;code&gt;&amp;lt;textarea&amp;gt;&lt;/code&gt; has been &lt;a href="https://css-tricks.com/creating-an-editable-textarea-that-supports-syntax-highlighted-code/"&gt;previously impossible&lt;/a&gt;, because textareas and inputs can only contain plain text. This is where the interesting possibilities lie. As a demo, I made this tiny HTML, CSS &amp;amp; JS sandbox, with native undo and redo, in a single, &lt;a href="https://blog.glyphdrawing.club/assets/webcomponents/tinybox.js"&gt;web component&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;




&lt;!-- Edit the content! --&gt;
&lt;p&gt;
          tiny HTML &amp;amp; CSS sandbox =)
        &lt;/p&gt;




&lt;h3&gt;Cons&lt;/h3&gt;
&lt;p&gt;There are, of course, some limitations to this method. It is not a direct replacement to the more robust syntax highligting libraries, but works well enough for simple needs.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Making modifications to the syntax highligher, like adding more language supports or changing the look of the font, requires modifying the font file. This requires some knowledge of font production, which most people don't have.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It only works where OpenType is supported. Fortunately, that's all major browsers and most modern programs. However, something like PowerPoint doesn't support OpenType.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finding patterns in text with OpenType contextual alternates is quite basic, and is no match for scripts that use regular expressions. For example, words within &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; tags that are JS keywords will be always highlighted: &lt;code&gt;&amp;lt;p&amp;gt;if I throw this Object through the window, catch it, for else itâ€™ll continue to Infinity &amp;amp; break&amp;lt;/p&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multiline highlighting with manual line breaks will sadly not work.&lt;/p&gt;
&lt;p&gt;This is common, for example, in comment blocks and template literals:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; &amp;lt;!-- This line gets highlighted...
 but not this, because I made a manual line break...
 --&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;How does it actually work?&lt;/h2&gt;
&lt;p&gt;Here's roughly how it works. There are two features in OpenType that make this possible: OpenType COLR table and contextual alternates.&lt;/p&gt;
&lt;h3&gt;OpenType COLR table&lt;/h3&gt;
&lt;p&gt;OpenType COLR table makes multi-colored fonts possible. &lt;a href="https://glyphsapp.com/learn/creating-a-microsoft-color-font"&gt;There is a good guide on creating a color font using Glyphs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I made a palette with 8 colors.&lt;/p&gt;
&lt;p&gt;I duplicated letters &lt;code&gt;A&lt;/code&gt;â€‰&lt;code&gt;â€“&lt;/code&gt;â€‰&lt;code&gt;Z&lt;/code&gt;, numbers &lt;code&gt;0&lt;/code&gt;â€‰&lt;code&gt;â€“&lt;/code&gt;â€‰&lt;code&gt;9&lt;/code&gt; and the characters &lt;code&gt;.&lt;/code&gt; &lt;code&gt;#&lt;/code&gt; &lt;code&gt;*&lt;/code&gt; &lt;code&gt;-&lt;/code&gt; and &lt;code&gt;_&lt;/code&gt; four times. Each duplicated character is then suffixed with .alt, .alt2, .alt3 or .alt4, and then assigned a color from the palette. For example, all .alt1 glyphs are &lt;code&gt;this&lt;/code&gt; color.&lt;/p&gt;
&lt;p&gt;I also duplicated all characters twice, and gave them suffices .alt1 and .alt5 and assigned them colors used in &lt;code&gt;&amp;lt;!-- comment blocks --&amp;gt;&lt;/code&gt; and &lt;code&gt;"strings within quotes"&lt;/code&gt;&lt;/p&gt;

&lt;img src="https://blog.glyphdrawing.club/assets/kmFZTjkTcx-320.jpeg"/&gt;
View from Glyps app. Each alternate character has a different color.

&lt;p&gt;The two other colors I used for symbols &lt;code&gt;&amp;amp; | $ + âˆ’ = ~ [] () {} / ; : " @ %&lt;/code&gt; and &lt;code&gt;'&lt;/code&gt;, and they are always in one color. Numbers &lt;code&gt;0 1 2 3 4 5 6 7 8 9&lt;/code&gt; are also always a certain color, unless overriden by other rules.&lt;/p&gt;
&lt;h3&gt;OpenType contextual alternates&lt;/h3&gt;
&lt;p&gt;The second required feature is OpenType contextual alternates. &lt;a href="https://glyphsapp.com/learn/features-part-3-advanced-contextual-alternates"&gt;Here's a great introductory guide to advanced contextual alternates for Glyphs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Contextual alternates makes characters "aware" of their adjacent characters. An example would be fonts that emulate continuous hand writing, where &lt;em&gt;how&lt;/em&gt; a letter connects depends on which letter it connects to. There is a &lt;a href="https://ilovetypography.com/2011/04/01/engaging-contextuality/"&gt;nice article covering possible uses here&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;JavaScript syntax rules&lt;/h4&gt;
&lt;p&gt;The core feature of contextual alternates is substituting glyphs. Here is a simplified code for finding the JavaScript keyword &lt;code&gt;if&lt;/code&gt; and substituting the letters i and f with their colored variant:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sub i' f by i.alt2;
sub i.alt2 f' by f.alt2;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In English:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;When i is followed by f, substitute the default i with an alternate (i.alt2).&lt;/li&gt;
&lt;li&gt;When i.alt2 is followed by f, substitute the default f with an alternate (f.alt2).&lt;/li&gt;
&lt;li&gt;As a result, every "if" in text gets substituted with &lt;code&gt;if&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;OpenType doesn't support many-to-many substitutions directly, but &lt;a href="https://typo.social/@behdad/112967180363218632"&gt;@behdad&lt;/a&gt; on Mastodon had a great suggestion: keywords could be elegantly colored by &lt;em&gt;chaining&lt;/em&gt; contextual substitutions.&lt;/p&gt;
&lt;p&gt;To do this, I made a lookup which substitutes each letter with its colored variant.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lookup ALT_SUBS {
    sub a by a.alt; 
    sub b by b.alt; 
    sub c by c.alt; 
    [etc.]
    sub Y by Y.alt;
    sub Z by Z.alt;
} ALT_SUBS;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I moved this lookup rule to the &lt;a href="https://handbook.glyphsapp.com/layout/standalone-lookups/"&gt;Prefix&lt;/a&gt; section, which just means it doesn't get applied automatically unlike the other lookups.&lt;/p&gt;
&lt;p&gt;Then, I made a lookup rule for each keyword in the contextual alternates section. Here's one for &lt;code&gt;console&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lookup console {
    ignore sub @AllLetters c' o' n' s' o' l' e';
    ignore sub c' o' n' s' o' l' e' @AllLetters;
    sub c' lookup ALT_SUBS
        o' lookup ALT_SUBS
        n' lookup ALT_SUBS
        s' lookup ALT_SUBS
        o' lookup ALT_SUBS
        l' lookup ALT_SUBS
        e' lookup ALT_SUBS;
} console;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First two lines tells it to ignore strings like &lt;code&gt;Xconsole&lt;/code&gt; or &lt;code&gt;consoles&lt;/code&gt;, but not if there's a period like &lt;code&gt;console.log()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The third line starts by replacing the first letter 'c' with its colored variant &lt;code&gt;c&lt;/code&gt;, by using definitions from the other lookup table "ALT_SUBS". This repeats until each letter is replaced by its color variant, and the result is &lt;code&gt;console&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Identifying JavaScript keywords is fairly straightforward. Logic is the same for each keyword, and I used a python script to generate them.&lt;/p&gt;
&lt;h4&gt;HTML &amp;amp; CSS syntax rules&lt;/h4&gt;
&lt;p&gt;But for HTML and CSS... I had to get a bit more creative. There are simply too many keywords for both HTML and CSS combined. Making a separate rule for each keyword would inflate the file size.&lt;/p&gt;
&lt;p&gt;Instead, I came up with this monstrosity. Here's how I find CSS value functions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lookup CssParamCalt useExtension {
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' parenleft by @CssParamAlt4;
} CssParamCalt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;@CssParam is a custom OpenType glyph class I've set up. It includes the characters &lt;code&gt;A&lt;/code&gt;â€‰&lt;code&gt;â€“&lt;/code&gt;â€‰&lt;code&gt;Z&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt;â€‰&lt;code&gt;â€“&lt;/code&gt;â€‰&lt;code&gt;z&lt;/code&gt;, and &lt;code&gt;-&lt;/code&gt;, which are all the possible characters used in CSS value function names. Because the longest possible CSS value function name is &lt;code&gt;repeating-linear-gradient()&lt;/code&gt;, with 25 letters, the first line of the lookup starts with @CssParam repeated 25 times, followed by parenleft (&lt;code&gt;(&lt;/code&gt;). This lookup will match any word up to 25 letters long, if it's immediately followed by an opening parenthesis. When a match occurs, it substitutes the matched text with its alternate color form (@CssParamAlt4).&lt;/p&gt;
&lt;p&gt;This lookup works for both CSS and JavaScript. It will colorize standard CSS functions like &lt;code&gt;rgb()&lt;/code&gt; as well as custom JavaScript functions like &lt;code&gt;myFunction()&lt;/code&gt;. The result is a semi-flexible syntax highlighter that doesn't require complex parsing. The downside is that if you have a really long function name, it stops working midway: &lt;code&gt;aReallyLongFunctionNameStopsWorkingMidway()&lt;/code&gt;. I've repeated the same principle for finding HTML tags and attributes, and for CSS selectors and parameters.&lt;/p&gt;
&lt;h4&gt;Unknown length rules&lt;/h4&gt;
&lt;p&gt;Comment blocks and strings between quotes also required extra care, because their length can be anything. OpenType doesn't support loops or anything resembling regular expressions. For example, I can't just tell it to simply substitute everything it finds between two quotes.&lt;/p&gt;
&lt;p&gt;However, I got a great suggestion from @penteract on &lt;a href="https://news.ycombinator.com/item?id=41259124"&gt;&lt;em&gt;Hacker News&lt;/em&gt;&lt;/a&gt; to use a finite state machine for these kinds of situations. Here our aim is to colorize eveything between /* and */ gray:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lookup CSScomment useExtension {
  // stop if we encounter a colored */
  ignore sub asterisk.alt1 slash.alt1 @All';

  // color first letter after /*
  sub slash asterisk @All' by @AllAlt1;
  sub slash asterisk space @All' by @AllAlt1;
  
  // color /* itself
  sub slash' asterisk by slash.alt1;
  sub slash.alt1 asterisk' by asterisk.alt1;
  
  // finite state machine to color rest of the characters
  // or until ignore condition is met
  sub @AllAlt1 @All' by @AllAlt1;
} CSScomment;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last line is the important one. The lookup will just continue replacing characters if the previous character is already colored.&lt;/p&gt;
&lt;h3&gt;End note&lt;/h3&gt;
&lt;p&gt;The full process is a little bit too convoluted to go into step-by-step, but if you're a type designer who wants to do this with their own font, don't hesitate to contact me.&lt;/p&gt;
&lt;p&gt;I'm also not an OpenType expert, so I'm sure the substitution logics could be improved upon. If you're interested in learning more about OpenType, I recommend reading &lt;a href="https://opentypecookbook.com/"&gt;The OpenType Cookbook&lt;/a&gt; and the complete &lt;a href="https://adobe-type-tools.github.io/afdko/OpenTypeFeatureFileSpecification.html"&gt;OpenTypeâ„¢ Feature File Specification&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you have any ideas, suggestions or feedback, let me know. You can reach me at &lt;code&gt;hlotvonen@gmail.com&lt;/code&gt; or leave a comment on &lt;a href="https://typo.social/@gdc/112959308500800771"&gt;Mastodon&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Changing the color theme&lt;/h2&gt;
&lt;p&gt;You can change the color theme with CSS &lt;a href="https://developer.mozilla.org/en-US/docs/Web/CSS/@font-palette-values/override-colors"&gt;&lt;code&gt;override-colors&lt;/code&gt;&lt;/a&gt;! &lt;a href="https://caniuse.com/?search=font-palette"&gt;Browser support&lt;/a&gt; is great at ~94%.&lt;/p&gt;




&lt;!-- Edit the content! --&gt;
&lt;pre&gt;&lt;code&gt;
        var const let for while
        function() linear-gradient()
        .myDiv{ background-color: pink; }
        console.log("hello", true)
        /* comment */
        &amp;amp; | $ + âˆ’ = ~ [] () {} / ; : " @ % 
        0 1 2 3 4 5 6 7 8 9
      &lt;/code&gt;&lt;/pre&gt;



&lt;h2&gt;Alternative built-in color themes&lt;/h2&gt;
&lt;p&gt;Additionally, two alternative color themes &lt;em&gt;Night Owl&lt;/em&gt;, and &lt;em&gt;Light Owl&lt;/em&gt; were added by &lt;a href="https://typo.social/@niutech@fosstodon.org"&gt;niutech&lt;/a&gt;. You can download them from the &lt;a href="https://github.com/hlotvonen/FontWithASyntaxHighlighter"&gt;FontWithASyntaxHighlighter GitHub page&lt;/a&gt;. &lt;a href="https://github.com/sdras/night-owl-vscode-theme"&gt;Night Owl theme&lt;/a&gt; is made by &lt;a href="https://github.com/sdras"&gt;Sarah Drasner&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In order to modify the built-in color palette, you have to edit the font source file. To do so, you can edit the color palettes values in lines 112-120 of the &lt;a href="https://github.com/hlotvonen/FontWithASyntaxHighlighter/blob/main/FontWithASyntaxHighlighter.glyphs"&gt;FontWithASyntaxHighlighter.glyphs&lt;/a&gt; file and then build the font with &lt;a href="https://github.com/googlefonts/fontmake"&gt;fontmake&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Projects using this font&lt;/h2&gt;
&lt;p&gt;Here's some cool projects that are using or are inspired by this font:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.holograph.so/"&gt;Holograph is a visual coding tool built on tldraw&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/dennishansen/holograph"&gt;its GitHub page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://maxbo.me/celine/"&gt;@celine/celine is library for building reactive HTML notebooks&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/MaxwellBo/celine"&gt;its GitHub page&lt;/a&gt; &amp;amp; &lt;a href="https://maxbo.me/a-html-file-is-all-you-need.html"&gt;blogpost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://chenglou.me/pure-css-shaders-art/"&gt;Shaders art made with pure CSS&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/chenglou/pure-css-shaders-art"&gt;its GitHub Page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mdit.pages.dev/"&gt;Mdit, a simple Markdown previewer&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/roblesdotdev/mdit"&gt;its GitHub Page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/JRJurman/textarea-code-block"&gt;Web Component for making a Textarea element into a syntax highlighted codeblock&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;It might also be used as an example for displaying the potential uses for color fonts in the W3C &lt;a href="https://github.com/w3c/csswg-drafts/tree/main/css-fonts-4"&gt;CSS Fonts Module Level 4 specification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://codepen.io/daviddarnes/pen/poXpaLB?editors=1100"&gt; Web Component with syntax highlighting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tug.org/tug2025/preprints/rajeesh-colorfont-syntax.pdf"&gt;An OpenType font with built-in TEX syntax highlighting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://labelary.com/viewer.html"&gt;Labelary ZPL viewer &amp;amp; editor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://garten.salat.dev/"&gt;garten.salat.dev blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ffoodd.fr/devfest.2024/jeu/"&gt;Lâ€™invasion du HTML mutant&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;(Did you make a project using this font, or know a project that uses it? Let me know please!)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Potential future&lt;/h2&gt;
&lt;p&gt;Many people suggested that this concept could be taken one step further with &lt;a href="https://github.com/harfbuzz/harfbuzz-wasm-examples"&gt;harfbuzz-wasm&lt;/a&gt;. With harfbuzz-wasm a real parser could be used instead of my crazy opentype lookup rules. Essentially, all the cons could be eliminated... Any harfbuzz-wasm experts who wants to take this on?&lt;/p&gt;
&lt;h2&gt;Licence&lt;/h2&gt;
&lt;p&gt;The original font (&lt;a href="https://monaspace.githubnext.com/"&gt;MonaSpace&lt;/a&gt;) has &lt;a href="https://github.com/githubnext/monaspace/blob/main/LICENSE"&gt;SIL open font license v1.1&lt;/a&gt;, which carries over to my modified version. So, you're free to use the font in any way that the SIL v1.1 license permits.&lt;/p&gt;
&lt;p&gt;As for the code examples, they are MIT licensed. The tiny sandbox web component can be found here: &lt;a href="https://github.com/hlotvonen/tinybox"&gt;https://github.com/hlotvonen/tinybox&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Source&lt;/h2&gt;
&lt;p&gt;The original source .glyphs file is &lt;a href="https://github.com/hlotvonen/FontWithASyntaxHighlighter"&gt;hosted in this GitHub repository&lt;/a&gt;. UFO files were kindly added by &lt;a href="https://typo.social/@niutech@fosstodon.org"&gt;niutech&lt;/a&gt;. Or, you can modify the font with &lt;a href="https://forum.glyphsapp.com/t/script-outside-glyphapp/22454"&gt;some scripting&lt;/a&gt; &amp;amp; build with &lt;a href="https://github.com/googlefonts/fontmake"&gt;fontmake&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;More examples&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;as, in, of, if, for, while, finally, var, new, function,
do, return, void, else, break, catch, instanceof, with,
throw, case, default, try, switch, continue, typeof, delete,
let, yield, const, class, get, set, debugger, async, await,
static, import, from, export, extends

true, false, null, undefined, NaN, Infinity

Object, Function, Boolean, Symbol, Math, Date, Number, BigInt, 
String, RegExp, Array, Float32Array, Float64Array, Int8Array, 
Uint8Array, Uint8ClampedArray, Int16Array, Int32Array, Uint16Array, 
Uint32Array, BigInt64Array, BigUint64Array, Set, Map, WeakSet,
WeakMap, ArrayBuffer, SharedArrayBuffer, Atomics, DataView, 
JSON, Promise, Generator, GeneratorFunction, AsyncFunction, 
Reflect, Proxy, Intl, WebAssembly, Error, EvalError, InternalError, 
RangeError, ReferenceError, SyntaxError, TypeError, URIError, 
setInterval, setTimeout, clearInterval, clearTimeout, require, 
exports, eval, isFinite, isNaN, parseFloat, parseInt, decodeURI, 
decodeURIComponent, encodeURI, encodeURIComponent, escape, 
unescape, arguments, this, super, console, window, document, 
localStorage, sessionStorage, module, global
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;!-- this is a comment! --&amp;gt;
/* and this */
// and this
&amp;lt;!-- however...
it breaks when your code goes to a newline.
there's no way to keep context line to line...
--&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;!-- can't disable highlighting JS keywords in between tags --&amp;gt;
&amp;lt;p&amp;gt;
  give me a break...
&amp;lt;/p&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang="en"&amp;gt;
&amp;lt;head&amp;gt;
  &amp;lt;meta charset="UTF-8"&amp;gt;
  &amp;lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&amp;gt;
  &amp;lt;title&amp;gt;Syntax Highlighter Example&amp;lt;/title&amp;gt;
  &amp;lt;style&amp;gt;
    body {
      background-color: rgb(255, 0, 0);
      font-family: 'Arial Narrow', sans-serif;
      line-height: 1.44;
      color: #333;
    }
  &amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
  &amp;lt;header&amp;gt;
    &amp;lt;h1&amp;gt;Welcome to the Syntax Highlighter Test&amp;lt;/h1&amp;gt;
  &amp;lt;/header&amp;gt;
  &amp;lt;nav&amp;gt;
    &amp;lt;ul&amp;gt;
      &amp;lt;li&amp;gt;&amp;lt;a href="#section1"&amp;gt;Section 1&amp;lt;/a&amp;gt;
    &amp;lt;/ul&amp;gt;
  &amp;lt;/nav&amp;gt;
  &amp;lt;main&amp;gt;
    &amp;lt;section id="section1"&amp;gt;
      &amp;lt;h2&amp;gt;Section 1&amp;lt;/h2&amp;gt;
      &amp;lt;p&amp;gt;This is a &amp;lt;span class="highlight"&amp;gt;highlighted&amp;lt;/span&amp;gt; paragraph.&amp;lt;/p&amp;gt;
      &amp;lt;img src="/api/placeholder/300/200" alt="Placeholder image"&amp;gt;
    &amp;lt;/section&amp;gt;
  &amp;lt;/main&amp;gt;
  &amp;lt;script&amp;gt;
    console.log("This is a JavaScript comment");
    function greet(name) {
      return `Hello, ${name}!`;
    }
    document.addEventListener('DOMContentLoaded', () =&amp;gt; {
      console.log(greet('Syntax Highlighter'));
    });
  &amp;lt;/script&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;.crazyBackground {
  /* don't try this at home */
  background:
    radial-gradient(
      100% 50% at 50% 50%,
      hsl(90 90% 45%) 0% 5%,
      hsl(250 70% 40%) 50%,
      hsl(50 50% 50%)
    ),
    radial-gradient(
      100% 100% at 50% 25%,
      hsl(90 40% 85%) 30%,
      hsl(40 80% 20%) 60% 90%,
      transparent
    ),
    linear-gradient(
      90deg,
      hsl(150 90% 90%) 0 10%,
      hsl(10 10% 20%),
      hsl(150 90% 90%) 90% 100%
    )
  ;
  background-size:
    5% 10%,
    10% 200%,
    25% 100%
  ;
  background-blend-mode:
    color-dodge,
    difference,
    normal
  ;
  animation: fire2 60s linear infinite;
}

@keyframes fire2 {
  from {
    background-position: 0% 0%, 0 30%, 0 0;
  }

  to {
    background-position: 0% -100%, -100% 30%, 200% 0%;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;// Variables and constants
let variable = 'Hello';
const CONSTANT = 42;

// Template literals
const name = 'World';
console.log(`${variable}, ${name}!`);

// Function declaration
function greet(name) {
  return `Hello, ${name}!`;
}

// Arrow function
const multiply = (a, b) =&amp;gt; a * b;

// Class definition
class Person {
  constructor(name, age) {
    this.name = name;
    this.age = age;
  }
  sayHello() {
    console.log(`Hello, my name is ${this.name}`);
  }
}

// Object literal
const config = {
  apiKey: 'abc123',
  maxRetries: 3,
  timeout: 5000
};

// Array methods
const numbers = [1, 2, 3, 4, 5];
const doubled = numbers.map(num =&amp;gt; num * 2);
const sum = numbers.reduce((acc, curr) =&amp;gt; acc + curr, 0);

// Async/await
async function fetchData(url) {
  try {
    const response = await fetch(url);
    const data = await response.json();
    return data;
  } catch (error) {
    console.error('Error fetching data:', error);
  }
}

// Destructuring
const { apiKey, maxRetries } = config;
const [first, second, ...rest] = numbers;

// Spread operator
const newArray = [...numbers, 6, 7, 8];

// Conditional (ternary) operator
const isAdult = age &amp;gt;= 18 ? 'Adult' : 'Minor';

// Switch statement
function getDayName(dayNumber) {
  switch (dayNumber) {
    case 0: return 'Sunday';
    case 1: return 'Monday';
    // ... other cases
    default: return 'Invalid day';
  }
}

// Regular expression
const emailRegex = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;

// Symbol
const uniqueKey = Symbol('description');

// Set and Map
const uniqueNumbers = new Set(numbers);
const userRoles = new Map([['admin', 'full'], ['user', 'limited']]);

// Promises
const promise = new Promise((resolve, reject) =&amp;gt; {
  setTimeout(() =&amp;gt; resolve('Done!'), 1000);
});

// Export statement
export { greet, Person };
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;I received a lot of great feedback from the discussions at &lt;a href="https://typo.social/@gdc/112959308500800771"&gt;Mastodon&lt;/a&gt; and &lt;a href="https://news.ycombinator.com/item?id=41245159"&gt;Hacker News&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Thanks to jfk13 on hn, and &lt;a href="https://typo.social/@kizu@front-end.social/112960336521542558"&gt;@pixelambacht&lt;/a&gt; on Mastodon for pointing out that 'calt' is turned on by default, and that 'colr' is not an opentype feature that needs to be "turned on".&lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href="https://news.ycombinator.com/item?id=41259124"&gt;penteract&lt;/a&gt; on hn and &lt;a href="https://typo.social/@behdad/112967180363218632"&gt;@behdad&lt;/a&gt; on Mastodon for suggesting better substitution rules.&lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href="https://typo.social/@kizu@front-end.social/112960336521542558"&gt;@kizu&lt;/a&gt; and &lt;a href="https://typo.social/@kizu@front-end.social/112960336521542558"&gt;@pixelambacht&lt;/a&gt; on Mastodon for suggesting color theming with &lt;code&gt;override-colors&lt;/code&gt; CSS rule.&lt;/p&gt;
&lt;p&gt;As said earlier, if you have any ideas, suggestions or feedback, let me know. You can reach me at &lt;code&gt;hlotvonen@gmail.com&lt;/code&gt; or leave a comment on &lt;a href="https://typo.social/@gdc/112959308500800771"&gt;Mastodon&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thanks to all who sent emails, messages and commented!&lt;/p&gt;

&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://blog.glyphdrawing.club/font-with-built-in-syntax-highlighting/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 10:28:09 +0000</pubDate>
    </item>
    <item>
      <title>Ask HN: What are the best engineering blogs with real-world depth?</title>
      <link>https://news.ycombinator.com/item?id=46363921</link>
      <description>Specifically interested in posts that:
1. Explain technical concepts clearly and concisely
2. Show real implementation details, trade-offs, and failures
3. Are well-structured and readable
4. Tie engineering decisions back to business or product outcomes Any standout blogs, posts, or platforms you regularly learn from?</description>
      <content:encoded>&lt;body&gt;&lt;a href="https://news.ycombinator.com"&gt;&lt;img src="https://news.ycombinator.com/y18.svg"/&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/news"&gt;Hacker News&lt;/a&gt;&lt;a href="https://news.ycombinator.com/newest"&gt;new&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/front"&gt;past&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/newcomments"&gt;comments&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/ask"&gt;ask&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/show"&gt;show&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/jobs"&gt;jobs&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/submit"&gt;submit&lt;/a&gt;&lt;a href="https://news.ycombinator.com/login?goto=item%3Fid%3D46363921"&gt;login&lt;/a&gt;&lt;a href="https://news.ycombinator.com/vote?id=46363921&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/item?id=46363921"&gt;Ask HN: What are the best engineering blogs with real-world depth?&lt;/a&gt;280 points by &lt;a href="https://news.ycombinator.com/user?id=nishilpatel"&gt;nishilpatel&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46363921"&gt;8 hours ago&lt;/a&gt;  | &lt;a href="https://news.ycombinator.com/hide?id=46363921&amp;amp;goto=item%3Fid%3D46363921"&gt;hide&lt;/a&gt; | &lt;a href="https://hn.algolia.com/?query=Ask%20HN%3A%20What%20are%20the%20best%20engineering%20blogs%20with%20real-world%20depth%3F&amp;amp;type=story&amp;amp;dateRange=all&amp;amp;sort=byDate&amp;amp;storyText=false&amp;amp;prefix&amp;amp;page=0"&gt;past&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/fave?id=46363921&amp;amp;auth=e95b7fed59a69774119812f886e8bd69edd1f2ec"&gt;favorite&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/item?id=46363921"&gt;87Â comments&lt;/a&gt;Iâ€™m looking for examples of high-quality engineering blog postsâ€”especially from tech company blogs, that go beyond surface-level explanations.&lt;p&gt;Specifically interested in posts that:
1. Explain technical concepts clearly and concisely
2. Show real implementation details, trade-offs, and failures
3. Are well-structured and readable
4. Tie engineering decisions back to business or product outcomes&lt;p&gt;Any standout blogs, posts, or platforms you regularly learn from?&lt;/p&gt;&lt;/p&gt;&lt;br/&gt;
&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364274&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=pella"&gt;pella&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364274"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
&amp;gt; especially from tech company blogs,&lt;p&gt;&lt;a href="https://engineering.fb.com/"&gt;https://engineering.fb.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://netflixtechblog.com/"&gt;https://netflixtechblog.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://stripe.com/blog/engineering"&gt;https://stripe.com/blog/engineering&lt;/a&gt;&lt;p&gt;&lt;a href="https://eng.uber.com"&gt;https://eng.uber.com&lt;/a&gt;&lt;p&gt;&lt;a href="https://engineering.linkedin.com/"&gt;https://engineering.linkedin.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://engineering.atspotify.com/"&gt;https://engineering.atspotify.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://tailscale.com/blog"&gt;https://tailscale.com/blog&lt;/a&gt;&lt;p&gt;&lt;a href="https://careersatdoordash.com/engineering-blog/"&gt;https://careersatdoordash.com/engineering-blog/&lt;/a&gt;&lt;p&gt;&lt;a href="https://dropbox.tech/"&gt;https://dropbox.tech/&lt;/a&gt;&lt;p&gt;--&lt;p&gt;Aggregators:( &lt;a href="https://engineering.fyi/"&gt;https://engineering.fyi/&lt;/a&gt; ; &lt;a href="https://diff.blog/"&gt;https://diff.blog/&lt;/a&gt; )&lt;p&gt;+ &lt;a href="https://hn.algolia.com/?query=engineering%20blog"&gt;https://hn.algolia.com/?query=engineering%20blog&lt;/a&gt;&lt;p&gt;---&lt;p&gt;create a public engineering-blog SKILL.md.
( ~ collect the writing patterns that work on HN )&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364274&amp;amp;goto=item%3Fid%3D46363921%2346364274"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364336&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=i_k"&gt;i_k&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364336"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;parent&lt;/a&gt; | &lt;a href="#46366609"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
I am quite surprised and a bit disappointed that almost none of them have RSS.&lt;p&gt;But thank you!&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364336&amp;amp;goto=item%3Fid%3D46363921%2346364336"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366119&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=reallydoubtful"&gt;reallydoubtful&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366119"&gt;3 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364336"&gt;parent&lt;/a&gt; | &lt;a href="#46364494"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Most of them have feeds.&lt;p&gt;* &lt;a href="https://engineering.fb.com/feed"&gt;https://engineering.fb.com/feed&lt;/a&gt;&lt;p&gt;* &lt;a href="https://netflixtechblog.com/feed"&gt;https://netflixtechblog.com/feed&lt;/a&gt;&lt;p&gt;* No feed for stripe&lt;p&gt;* &lt;a href="https://www.uber.com/en-GB/blog/london/engineering/rss/"&gt;https://www.uber.com/en-GB/blog/london/engineering/rss/&lt;/a&gt;&lt;p&gt;* No feed for LinkedIn&lt;p&gt;* &lt;a href="https://engineering.atspotify.com/feed"&gt;https://engineering.atspotify.com/feed&lt;/a&gt;&lt;p&gt;* &lt;a href="https://tailscale.com/blog/index.xml"&gt;https://tailscale.com/blog/index.xml&lt;/a&gt;&lt;p&gt;* &lt;a href="https://careersatdoordash.com/engineering-blog/feed"&gt;https://careersatdoordash.com/engineering-blog/feed&lt;/a&gt;&lt;p&gt;* &lt;a href="https://dropbox.tech/feed"&gt;https://dropbox.tech/feed&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366119&amp;amp;goto=item%3Fid%3D46363921%2346366119"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364494&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=petercooper"&gt;petercooper&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364494"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364336"&gt;parent&lt;/a&gt; | &lt;a href="#46366119"&gt;prev&lt;/a&gt; | &lt;a href="#46364511"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Not RSS exactly but this OPML has feeds for several hundred such blogs if you can filter down from there: &lt;a href="https://peterc.org/misc/engblogs.opml"&gt;https://peterc.org/misc/engblogs.opml&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364494&amp;amp;goto=item%3Fid%3D46363921%2346364494"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365958&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=phrotoma"&gt;phrotoma&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365958"&gt;3 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364494"&gt;parent&lt;/a&gt; | &lt;a href="#46364511"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Your website is a work of art. Bravo &amp;lt;3&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365958&amp;amp;goto=item%3Fid%3D46363921%2346365958"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364511&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=onion2k"&gt;onion2k&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364511"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364336"&gt;parent&lt;/a&gt; | &lt;a href="#46364494"&gt;prev&lt;/a&gt; | &lt;a href="#46364514"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Spotify and Tailscale do...&lt;p&gt;&lt;a href="https://engineering.atspotify.com/feed"&gt;https://engineering.atspotify.com/feed&lt;/a&gt;&lt;p&gt;&lt;a href="https://tailscale.com/blog/index.xml"&gt;https://tailscale.com/blog/index.xml&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364511&amp;amp;goto=item%3Fid%3D46363921%2346364511"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364514&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=embedding-shape"&gt;embedding-shape&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364514"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364336"&gt;parent&lt;/a&gt; | &lt;a href="#46364511"&gt;prev&lt;/a&gt; | &lt;a href="#46366609"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
&amp;gt; I am quite surprised and a bit disappointed that almost none of them have RSS.&lt;p&gt;I think it's on purpose. It is to signal that these (those without RSS) aren't really "engineering" blogs at all, they're marketing websites aimed to help with recruiting and making the organization seem "engineering-like".&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364514&amp;amp;goto=item%3Fid%3D46363921%2346364514"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364750&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=zbentley"&gt;zbentley&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364750"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364514"&gt;parent&lt;/a&gt; | &lt;a href="#46366609"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
What? That makes no sense. RSS is beloved and known among engineers. Marketers? Not so much.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364750&amp;amp;goto=item%3Fid%3D46363921%2346364750"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364756&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=embedding-shape"&gt;embedding-shape&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364756"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364750"&gt;parent&lt;/a&gt; | &lt;a href="#46366609"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Exactly, so if the blog doesn't have RSS, you know they're probably made from marketers with no input from engineering, otherwise they'd have RSS on the blogs.&lt;p&gt;Edit: Ah, noticed I made a without/with typo, fixed that, should make about 2% more sense now for the ones who the original meaning was unclear :)&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364756&amp;amp;goto=item%3Fid%3D46363921%2346364756"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364764&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=zbentley"&gt;zbentley&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364764"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364756"&gt;parent&lt;/a&gt; | &lt;a href="#46366609"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Oh, I read your post backwards (thought you said RSS == more likely fluff). My fault, sorry!&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364764&amp;amp;goto=item%3Fid%3D46363921%2346364764"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364858&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=embedding-shape"&gt;embedding-shape&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364858"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364764"&gt;parent&lt;/a&gt; | &lt;a href="#46366609"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
To be fair to you, my original comment did say:&lt;p&gt;&amp;gt; It is to signal that these (those with RSS) aren't really "engineering" blogs at all&lt;p&gt;So now when I corrected that with/without typo, it looks like your previous comment doesn't make sense, but it kind of did, at the time. Sorry about that and thanks for making me realize the typo!&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364858&amp;amp;goto=item%3Fid%3D46363921%2346364858"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366609&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=iancmceachern"&gt;iancmceachern&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366609"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;prev&lt;/a&gt; | &lt;a href="#46367852"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
It's so interesting to me as a Mechanical Engineer and Hardware designer/architect how on HN "Engineering" almost always means "Software engineering" here.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366609&amp;amp;goto=item%3Fid%3D46363921%2346366609"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367920&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=sp4nner"&gt;sp4nner&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367920"&gt;3 minutes ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;parent&lt;/a&gt; | &lt;a href="#46367406"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Agreed, though I understand the YC bias. I'm in biotech and mostly follow HN just to see what the software people are interested in these days.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367920&amp;amp;goto=item%3Fid%3D46363921%2346367920"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367406&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=throwaway4PP"&gt;throwaway4PP&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367406"&gt;47 minutes ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;parent&lt;/a&gt; | &lt;a href="#46367920"&gt;prev&lt;/a&gt; | &lt;a href="#46367412"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
It is funny, almost as funny as an entire cadre of people with â€œengineerâ€ in their title who've never had to draw a free body diagram, learn circuit analysis, understand the basics of thermodynamics, or the mechanics of materials.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367406&amp;amp;goto=item%3Fid%3D46363921%2346367406"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367501&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=p2detar"&gt;p2detar&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367501"&gt;38 minutes ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;root&lt;/a&gt; | &lt;a href="#46367406"&gt;parent&lt;/a&gt; | &lt;a href="#46367412"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
I hold a CS master degree from an Eastern European university and everything you listed was in our Bachelor degree program. Itâ€™s pretty funny because while studying material properties back then I always wondered how and when am I gonna use that. It kind of makes sense now that I think about it - some students preferred branching out to hardware.&lt;p&gt;edit: typo&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367501&amp;amp;goto=item%3Fid%3D46363921%2346367501"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367412&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=jupin"&gt;jupin&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367412"&gt;47 minutes ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;parent&lt;/a&gt; | &lt;a href="#46367406"&gt;prev&lt;/a&gt; | &lt;a href="#46366717"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
I thought the same. Check out this mechanical engineering channel - &lt;a href="https://youtu.be/8yUsDnBXo_g?si=CXzWV9D5OvHcCBm3"&gt;https://youtu.be/8yUsDnBXo_g?si=CXzWV9D5OvHcCBm3&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367412&amp;amp;goto=item%3Fid%3D46363921%2346367412"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366717&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=jvanderbot"&gt;jvanderbot&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366717"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;parent&lt;/a&gt; | &lt;a href="#46367412"&gt;prev&lt;/a&gt; | &lt;a href="#46367472"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
I would love more blogs on mechanical, hardware, and especially industrial engineering, but the demographics in those areas skew stereo-typically older and also likely less blog-oriented, right?&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366717&amp;amp;goto=item%3Fid%3D46363921%2346366717"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366933&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=georgeburdell"&gt;georgeburdell&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366933"&gt;1 hour ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;root&lt;/a&gt; | &lt;a href="#46366717"&gt;parent&lt;/a&gt; | &lt;a href="#46367560"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Blogs are almost 30 years old at this point, but yes, I do associate a nearly compulsive need to show off one's work in meticulously-crafted blog posts with younger people.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366933&amp;amp;goto=item%3Fid%3D46363921%2346366933"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367560&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=wheelinsupial"&gt;wheelinsupial&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367560"&gt;33 minutes ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;root&lt;/a&gt; | &lt;a href="#46366717"&gt;parent&lt;/a&gt; | &lt;a href="#46366933"&gt;prev&lt;/a&gt; | &lt;a href="#46367472"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Depending on what you're looking for in industrial engineering, there are a lot of blogs on lean manufacturing and the Toyota Production System. INFORMS, may be paywalled, also publishes a lot of pretty interesting articles on applications of operations research to industry.&lt;p&gt;In general, though, my very limited experience working in manufacturing was that much of the blog equivalents were covered in things like white papers from hardware manufacturers or articles in trade publications. We always had a bunch of magazines delivered each month and there were usually some interesting articles to review.&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367560&amp;amp;goto=item%3Fid%3D46363921%2346367560"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367472&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=tekno45"&gt;tekno45&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367472"&gt;40 minutes ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;parent&lt;/a&gt; | &lt;a href="#46366717"&gt;prev&lt;/a&gt; | &lt;a href="#46367852"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
people building physical things are probably too busy to blog about it lol&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367472&amp;amp;goto=item%3Fid%3D46363921%2346367472"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367852&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=Swizec"&gt;Swizec&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367852"&gt;9 minutes ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;prev&lt;/a&gt; | &lt;a href="#46367782"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
While not exactly a blog, I've collected ~16 years of [startup] engineering lessons into a book and I think it came out fantastic. People are saying super nice things.&lt;p&gt;&lt;a href="https://scalingfastbook.com"&gt;https://scalingfastbook.com&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367852&amp;amp;goto=item%3Fid%3D46363921%2346367852"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367782&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=bodash"&gt;bodash&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367782"&gt;15 minutes ago&lt;/a&gt;  | &lt;a href="#46367852"&gt;prev&lt;/a&gt; | &lt;a href="#46364321"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
&amp;gt; &lt;a href="https://lessnews.dev"&gt;https://lessnews.dev&lt;/a&gt;&lt;p&gt;A while ago I felt this "information fatigue" due to the overwhelming updates from the typical news sources (reddit, twitter, even hn).&lt;p&gt;So I built a _slow_ webdev newsfeed aggregator that doesn't overwhelm you of constant updates, so you focus on reading the actual blog contents and enjoy other things.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367782&amp;amp;goto=item%3Fid%3D46363921%2346367782"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364321&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=xnorswap"&gt;xnorswap&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364321"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46367782"&gt;prev&lt;/a&gt; | &lt;a href="#46365543"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
You might be more interested in books than a blog.&lt;p&gt;For example: The Architecture of Open Source Applications&lt;p&gt;&lt;a href="https://aosabook.org/en/index.html"&gt;https://aosabook.org/en/index.html&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364321&amp;amp;goto=item%3Fid%3D46363921%2346364321"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364479&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=alhirzel"&gt;alhirzel&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364479"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364321"&gt;parent&lt;/a&gt; | &lt;a href="#46365543"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Such a great resource!&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364479&amp;amp;goto=item%3Fid%3D46363921%2346364479"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365543&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=simonw"&gt;simonw&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365543"&gt;4 hours ago&lt;/a&gt;  | &lt;a href="#46364321"&gt;prev&lt;/a&gt; | &lt;a href="#46365655"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
This post by Jay Kreps that introduced Kafka to the world remains one of my favorite pieces of engineering blog content of all time: &lt;a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying"&gt;https://engineering.linkedin.com/distributed-systems/log-wha...&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365543&amp;amp;goto=item%3Fid%3D46363921%2346365543"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365655&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=sateesh"&gt;sateesh&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365655"&gt;3 hours ago&lt;/a&gt;  | &lt;a href="#46365543"&gt;prev&lt;/a&gt; | &lt;a href="#46367429"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://jvns.ca/"&gt;https://jvns.ca/&lt;/a&gt;
Not a tech. company blog. Explains technical concepts clearly and top notch technical posts. Fits 1,2, 3 criteria of what you ask, though not the 4th one.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365655&amp;amp;goto=item%3Fid%3D46363921%2346365655"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367180&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=skywhopper"&gt;skywhopper&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367180"&gt;1 hour ago&lt;/a&gt;  | &lt;a href="#46365655"&gt;parent&lt;/a&gt; | &lt;a href="#46367429"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Yes! Julia is fantastic at explaining concepts, and creating ways to learn about them. She produces a great series of â€œzinesâ€ summarizing a bunch of technical topics, her blog archives are really fascinating, and sheâ€™s created really useful tools like Mess With DNS (&lt;a href="https://messwithdns.net"&gt;https://messwithdns.net&lt;/a&gt;) which gives you your own DNS subdomain and the means to update records so you can try things out in an easy, harmless way.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367180&amp;amp;goto=item%3Fid%3D46363921%2346367180"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367429&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=jupin"&gt;jupin&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367429"&gt;45 minutes ago&lt;/a&gt;  | &lt;a href="#46365655"&gt;prev&lt;/a&gt; | &lt;a href="#46367631"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
To balance all of the computer engineering blogs, check out this mechanical engineering channel: &lt;a href="https://youtu.be/8yUsDnBXo_g?si=CXzWV9D5OvHcCBm3"&gt;https://youtu.be/8yUsDnBXo_g?si=CXzWV9D5OvHcCBm3&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367429&amp;amp;goto=item%3Fid%3D46363921%2346367429"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367631&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=vishnuharidas"&gt;vishnuharidas&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367631"&gt;28 minutes ago&lt;/a&gt;  | &lt;a href="#46367429"&gt;prev&lt;/a&gt; | &lt;a href="#46366390"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://engineeringblogs.xyz/"&gt;https://engineeringblogs.xyz/&lt;/a&gt; is a good place listing more than 500 (and adding more) engineering blogs.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367631&amp;amp;goto=item%3Fid%3D46363921%2346367631"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366390&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=aranw"&gt;aranw&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366390"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46367631"&gt;prev&lt;/a&gt; | &lt;a href="#46364152"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://samwho.dev"&gt;https://samwho.dev&lt;/a&gt; has some fantastic blog posts with great visualisations&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366390&amp;amp;goto=item%3Fid%3D46363921%2346366390"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366733&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=samwho"&gt;samwho&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366733"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46366390"&gt;parent&lt;/a&gt; | &lt;a href="#46364152"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Thank you &amp;lt;3&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366733&amp;amp;goto=item%3Fid%3D46363921%2346366733"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364152&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=yrand"&gt;yrand&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364152"&gt;8 hours ago&lt;/a&gt;  | &lt;a href="#46366390"&gt;prev&lt;/a&gt; | &lt;a href="#46364405"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Encountered one specific example about a month ago here on HackerNews - All about automotive lidar.
&lt;a href="https://news.ycombinator.com/item?id=46110395"&gt;https://news.ycombinator.com/item?id=46110395&lt;/a&gt;&lt;p&gt;Blog posts where I find quality really shows are usually about something I know next to nothing about how it works. A badly written article usually either goes really shallow or skips some facts when going into depth and requires catchup elsewhere to actually understand it. The lidar article from Main Street Autonomy goes beyond basics and explained everything from the ground up in such a connected way that it was a real pleasure reading it.&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364152&amp;amp;goto=item%3Fid%3D46363921%2346364152"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364405&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=Okkef"&gt;Okkef&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364405"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364152"&gt;prev&lt;/a&gt; | &lt;a href="#46364159"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Armin Ronacher's blog (of flask/jinja fame) &lt;a href="https://lucumr.pocoo.org/"&gt;https://lucumr.pocoo.org/&lt;/a&gt;&lt;p&gt;Antirez' blog (of Redis fame) &lt;a href="https://antirez.com/"&gt;https://antirez.com/&lt;/a&gt;&lt;p&gt;Simon Willison's blog (about AI) &lt;a href="https://simonwillison.net/"&gt;https://simonwillison.net/&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364405&amp;amp;goto=item%3Fid%3D46363921%2346364405"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364159&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=qznc"&gt;qznc&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364159"&gt;8 hours ago&lt;/a&gt;  | &lt;a href="#46364405"&gt;prev&lt;/a&gt; | &lt;a href="#46366497"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Sounds like you look for an intersection of academic papers (1.), tech blogs (2.), text books (3.), and confidential business strategies (4.)? A very high ambition.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364159&amp;amp;goto=item%3Fid%3D46363921%2346364159"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367699&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=cess11"&gt;cess11&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367699"&gt;23 minutes ago&lt;/a&gt;  | &lt;a href="#46364159"&gt;parent&lt;/a&gt; | &lt;a href="#46364194"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Corporations commonly describe some of their internal processes and achievements because it builds reputation and that can be important for both sales and recruitment.&lt;p&gt;Sometimes they do it in the form of free or open source software releases.&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367699&amp;amp;goto=item%3Fid%3D46363921%2346367699"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364194&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=gchamonlive"&gt;gchamonlive&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364194"&gt;8 hours ago&lt;/a&gt;  | &lt;a href="#46364159"&gt;parent&lt;/a&gt; | &lt;a href="#46367699"&gt;prev&lt;/a&gt; | &lt;a href="#46366497"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
A very high ambition?&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364194&amp;amp;goto=item%3Fid%3D46363921%2346364194"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366497&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=nsm"&gt;nsm&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366497"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46364159"&gt;prev&lt;/a&gt; | &lt;a href="#46364190"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://randomascii.wordpress.com/"&gt;https://randomascii.wordpress.com/&lt;/a&gt; - former Chrome engineer about all things performance engineering and particularly focused on Windows.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366497&amp;amp;goto=item%3Fid%3D46363921%2346366497"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364190&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=nchmy"&gt;nchmy&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364190"&gt;8 hours ago&lt;/a&gt;  | &lt;a href="#46366497"&gt;prev&lt;/a&gt; | &lt;a href="#46364537"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
You're probably looking for something that is more focused on specific software decisions/implementations, but &lt;a href="https://infrequently.org"&gt;https://infrequently.org&lt;/a&gt; is the best web development blog out there.&lt;p&gt;It's not "technical" so much as it just educates you on how to be a good web developer/run a team. There's zero fluff and considerable detail (footnotes are practically blog posts themselves).&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364190&amp;amp;goto=item%3Fid%3D46363921%2346364190"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364537&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=bzGoRust"&gt;bzGoRust&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364537"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364190"&gt;prev&lt;/a&gt; | &lt;a href="#46366909"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://discord.com/blog"&gt;https://discord.com/blog&lt;/a&gt;
&lt;a href="https://blog.cloudflare.com/"&gt;https://blog.cloudflare.com/&lt;/a&gt;
&lt;a href="https://netflixtechblog.com/"&gt;https://netflixtechblog.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364537&amp;amp;goto=item%3Fid%3D46363921%2346364537"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366909&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=mitthrowaway2"&gt;mitthrowaway2&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366909"&gt;1 hour ago&lt;/a&gt;  | &lt;a href="#46364537"&gt;prev&lt;/a&gt; | &lt;a href="#46367369"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
I always enjoyed Jason Sachs' blog at embedded related.&lt;p&gt;&lt;a href="https://www.embeddedrelated.com/showarticle/152.php"&gt;https://www.embeddedrelated.com/showarticle/152.php&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366909&amp;amp;goto=item%3Fid%3D46363921%2346366909"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367369&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=mad44"&gt;mad44&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367369"&gt;50 minutes ago&lt;/a&gt;  | &lt;a href="#46366909"&gt;prev&lt;/a&gt; | &lt;a href="#46364206"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
MongoDB Engineering Blog is shaping up well&lt;p&gt;&lt;a href="https://www.mongodb.com/company/blog/channel/engineering-blog"&gt;https://www.mongodb.com/company/blog/channel/engineering-blo...&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367369&amp;amp;goto=item%3Fid%3D46363921%2346367369"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364206&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=tekichan"&gt;tekichan&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364206"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46367369"&gt;prev&lt;/a&gt; | &lt;a href="#46366325"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
&lt;a href="http://highscalability.squarespace.com/all-time-favorites/"&gt;http://highscalability.squarespace.com/all-time-favorites/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364206&amp;amp;goto=item%3Fid%3D46363921%2346364206"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366325&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=NickJLange"&gt;NickJLange&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366325"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46364206"&gt;prev&lt;/a&gt; | &lt;a href="#46364182"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
A lot of great links here to the firehose (or at least for working parents). Unless someone has built it - anything that aggregates and shows beyond the first click of the by-line. (i.e. a first paragraph, or LLM-summary of the content)?&lt;p&gt;Otherwise... coming soon from a vibe-coding session near you...&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366325&amp;amp;goto=item%3Fid%3D46363921%2346366325"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366840&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=soulofmischief"&gt;soulofmischief&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366840"&gt;1 hour ago&lt;/a&gt;  | &lt;a href="#46366325"&gt;parent&lt;/a&gt; | &lt;a href="#46366721"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
A friend and I worked on a startup together that did this back when only the GPT-3 API was available. Sucked up everything we could think of, including HN and traditionally opaque sources such as Telegram&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366840&amp;amp;goto=item%3Fid%3D46363921%2346366840"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366721&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=SleepySteve_sk"&gt;SleepySteve_sk&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366721"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46366325"&gt;parent&lt;/a&gt; | &lt;a href="#46366840"&gt;prev&lt;/a&gt; | &lt;a href="#46364182"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
We're currently building something to solve this problem.&lt;p&gt;&lt;a href="https://joinheader.com/"&gt;https://joinheader.com/&lt;/a&gt;&lt;p&gt;We'll filter an RSS feed based on the topic and description that you provide. Feel free to reach out to me at s.kufuor@&amp;lt;domain&amp;gt; if you have any questions or feedback.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366721&amp;amp;goto=item%3Fid%3D46363921%2346366721"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364182&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=tester756"&gt;tester756&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364182"&gt;8 hours ago&lt;/a&gt;  | &lt;a href="#46366325"&gt;prev&lt;/a&gt; | &lt;a href="#46366537"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Maybe&lt;p&gt;&lt;a href="https://projectzero.google/archive.html"&gt;https://projectzero.google/archive.html&lt;/a&gt;&lt;p&gt;&lt;a href="https://netflixtechblog.medium.com/"&gt;https://netflixtechblog.medium.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://www.uber.com/en-US/blog/engineering/"&gt;https://www.uber.com/en-US/blog/engineering/&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364182&amp;amp;goto=item%3Fid%3D46363921%2346364182"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366537&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=agumonkey"&gt;agumonkey&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366537"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46364182"&gt;prev&lt;/a&gt; | &lt;a href="#46364444"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Often enjoyed article by chris wellons &lt;a href="https://nullprogram.com/"&gt;https://nullprogram.com/&lt;/a&gt;&lt;p&gt;quite diverse, often challenging, sometimes mind bending&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366537&amp;amp;goto=item%3Fid%3D46363921%2346366537"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364444&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=noam_k"&gt;noam_k&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364444"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46366537"&gt;prev&lt;/a&gt; | &lt;a href="#46364885"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://lcamtuf.substack.com/"&gt;https://lcamtuf.substack.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364444&amp;amp;goto=item%3Fid%3D46363921%2346364444"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364885&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=avisk"&gt;avisk&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364885"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46364444"&gt;prev&lt;/a&gt; | &lt;a href="#46366959"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://technology.riotgames.com/"&gt;https://technology.riotgames.com/&lt;/a&gt;
&lt;a href="https://fabiensanglard.net/"&gt;https://fabiensanglard.net/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364885&amp;amp;goto=item%3Fid%3D46363921%2346364885"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366959&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=jonstewart"&gt;jonstewart&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366959"&gt;1 hour ago&lt;/a&gt;  | &lt;a href="#46364885"&gt;prev&lt;/a&gt; | &lt;a href="#46364625"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Not corporate, but two of the best individual developer blogs are Eli Bendersky's and Rachel by the Bay. They've both been blogging prolifically for a decade+, Eli with a focus on, broadly, compilers and Rachel on SRE/debugging.&lt;p&gt;Raymond Chen's The Old New Thing is also required reading for anyone that works with Windows.&lt;p&gt;&lt;a href="https://eli.thegreenplace.net/"&gt;https://eli.thegreenplace.net/&lt;/a&gt;&lt;p&gt;&lt;a href="https://rachelbythebay.com/w/"&gt;https://rachelbythebay.com/w/&lt;/a&gt;&lt;p&gt;&lt;a href="https://devblogs.microsoft.com/oldnewthing/"&gt;https://devblogs.microsoft.com/oldnewthing/&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366959&amp;amp;goto=item%3Fid%3D46363921%2346366959"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364625&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=thundergolfer"&gt;thundergolfer&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364625"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46366959"&gt;prev&lt;/a&gt; | &lt;a href="#46364681"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
- &lt;a href="https://modal.com/blog/vprox"&gt;https://modal.com/blog/vprox&lt;/a&gt;
- &lt;a href="https://modal.com/blog/host-overhead-inference-efficiency"&gt;https://modal.com/blog/host-overhead-inference-efficiency&lt;/a&gt;
- &lt;a href="https://modal.com/blog/resource-solver"&gt;https://modal.com/blog/resource-solver&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364625&amp;amp;goto=item%3Fid%3D46363921%2346364625"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364681&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=mkosmul"&gt;mkosmul&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364681"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364625"&gt;prev&lt;/a&gt; | &lt;a href="#46364335"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Allegro Tech Blog: &lt;a href="https://blog.allegro.tech/"&gt;https://blog.allegro.tech/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364681&amp;amp;goto=item%3Fid%3D46363921%2346364681"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364335&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=pveierland"&gt;pveierland&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364335"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364681"&gt;prev&lt;/a&gt; | &lt;a href="#46364395"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Tweag has many interesting entries with good technical depth:&lt;p&gt;&lt;a href="https://www.tweag.io/blog"&gt;https://www.tweag.io/blog&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364335&amp;amp;goto=item%3Fid%3D46363921%2346364335"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364395&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=ludicity"&gt;ludicity&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364395"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364335"&gt;prev&lt;/a&gt; | &lt;a href="#46364971"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
I'm a huge fan of &lt;a href="https://eblog.fly.dev/index.html"&gt;https://eblog.fly.dev/index.html&lt;/a&gt;. The author, Efron, very graciously advises me on a lot of little things around my engineering practice, and I've learned a huge amount about weird holes in my practice from industry dysfunction in a very short period of time from him.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364395&amp;amp;goto=item%3Fid%3D46363921%2346364395"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364971&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=alzamos"&gt;alzamos&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364971"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46364395"&gt;prev&lt;/a&gt; | &lt;a href="#46364815"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Francesco Mazzoliâ€™s blog on &lt;a href="https://mazzo.li/archive.html"&gt;https://mazzo.li/archive.html&lt;/a&gt;. His blog has topped HN a few times with various low-level/linux topics, some deep dives into algorithms etc.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364971&amp;amp;goto=item%3Fid%3D46363921%2346364971"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364815&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=sevazhidkov"&gt;sevazhidkov&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364815"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364971"&gt;prev&lt;/a&gt; | &lt;a href="#46365128"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Itâ€™s not a traditional blog, but Oxideâ€™s RFDs cover exactly what you asked â€” implementation details and trade-offs: &lt;a href="https://rfd.shared.oxide.computer/"&gt;https://rfd.shared.oxide.computer/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364815&amp;amp;goto=item%3Fid%3D46363921%2346364815"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365128&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=sdairs"&gt;sdairs&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365128"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46364815"&gt;prev&lt;/a&gt; | &lt;a href="#46365215"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://clickhouse.com/blog?category=engineering"&gt;https://clickhouse.com/blog?category=engineering&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365128&amp;amp;goto=item%3Fid%3D46363921%2346365128"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365215&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=robofanatic"&gt;robofanatic&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365215"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46365128"&gt;prev&lt;/a&gt; | &lt;a href="#46364526"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://www.makingsoftware.com/"&gt;https://www.makingsoftware.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365215&amp;amp;goto=item%3Fid%3D46363921%2346365215"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364526&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=louiechristie"&gt;louiechristie&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364526"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46365215"&gt;prev&lt;/a&gt; | &lt;a href="#46364874"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://dora.dev/research/2025/dora-report/"&gt;https://dora.dev/research/2025/dora-report/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364526&amp;amp;goto=item%3Fid%3D46363921%2346364526"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364874&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=nickmonad"&gt;nickmonad&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364874"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46364526"&gt;prev&lt;/a&gt; | &lt;a href="#46365166"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
TigerBeetle: &lt;a href="https://tigerbeetle.com/blog/"&gt;https://tigerbeetle.com/blog/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364874&amp;amp;goto=item%3Fid%3D46363921%2346364874"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365166&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=GeoAtreides"&gt;GeoAtreides&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365166"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46364874"&gt;prev&lt;/a&gt; | &lt;a href="#46366569"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Seems to me you're describing books.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365166&amp;amp;goto=item%3Fid%3D46363921%2346365166"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366569&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=corbet"&gt;corbet&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366569"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46365166"&gt;prev&lt;/a&gt; | &lt;a href="#46364305"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
I feel obligated to mention LWN - &lt;a href="https://lwn.net/"&gt;https://lwn.net/&lt;/a&gt; - since that is exactly what we aspire to.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366569&amp;amp;goto=item%3Fid%3D46363921%2346366569"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364305&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=vogu66"&gt;vogu66&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364305"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46366569"&gt;prev&lt;/a&gt; | &lt;a href="#46364264"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
not software engineering, but &lt;a href="https://practical.engineering/"&gt;https://practical.engineering/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364305&amp;amp;goto=item%3Fid%3D46363921%2346364305"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364264&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=Agingcoder"&gt;Agingcoder&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364264"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364305"&gt;prev&lt;/a&gt; | &lt;a href="#46365415"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Cloudflare, google project zero.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364264&amp;amp;goto=item%3Fid%3D46363921%2346364264"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365415&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=rramadass"&gt;rramadass&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365415"&gt;4 hours ago&lt;/a&gt;  | &lt;a href="#46364264"&gt;prev&lt;/a&gt; | &lt;a href="#46364525"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Not a blog, but books detailing real-world experiences from Indian Engineers/Scientists/Researchers; Quite inspiring to see how people strive unceasingly towards a goal in spite of all the limitations and hurdles (viz. Political/Financial/Material etc.) imposed on them.&lt;p&gt;There is much to learn, in these books.&lt;p&gt;The Mind of an Engineer by Purnendu Ghosh et al. - &lt;a href="https://link.springer.com/book/10.1007/978-981-10-0119-2"&gt;https://link.springer.com/book/10.1007/978-981-10-0119-2&lt;/a&gt;&lt;p&gt;The Mind of an Engineer: Volume 2 by Purnendu Ghosh et al. - &lt;a href="https://link.springer.com/book/10.1007/978-981-15-1330-5"&gt;https://link.springer.com/book/10.1007/978-981-15-1330-5&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365415&amp;amp;goto=item%3Fid%3D46363921%2346365415"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364525&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=louiechristie"&gt;louiechristie&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364525"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46365415"&gt;prev&lt;/a&gt; | &lt;a href="#46364307"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://youtube.com/@modernsoftwareengineeringyt"&gt;https://youtube.com/@modernsoftwareengineeringyt&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364525&amp;amp;goto=item%3Fid%3D46363921%2346364525"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364307&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=mitjam"&gt;mitjam&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364307"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364525"&gt;prev&lt;/a&gt; | &lt;a href="#46364917"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Maybe it's just because I'm LLMing a bit too much, recently, but this question sounds to me like a prompt.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364307&amp;amp;goto=item%3Fid%3D46363921%2346364307"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365433&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=x187463"&gt;x187463&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365433"&gt;4 hours ago&lt;/a&gt;  | &lt;a href="#46364307"&gt;parent&lt;/a&gt; | &lt;a href="#46364572"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Some people act like the use of an LLM immediately invalidates or lowers the value of a piece of content. But the case of a question or simple post, especially by somebody for whom English is second language, using an LLM to rephrase or clean-up some text seems like an innocent and practical use case for LLMs.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365433&amp;amp;goto=item%3Fid%3D46363921%2346365433"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364572&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=runlaszlorun"&gt;runlaszlorun&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364572"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364307"&gt;parent&lt;/a&gt; | &lt;a href="#46365433"&gt;prev&lt;/a&gt; | &lt;a href="#46364509"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
I'm not beating up on OP but I chuckled when I read the question. Literally the only place I see the phrase "no fluff" with any frequency is with Deepseek lol.&lt;p&gt;Nothing wrong with the phrase itself of course, other than the fact that it's like literally in every other reply for me lol.&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364572&amp;amp;goto=item%3Fid%3D46363921%2346364572"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364509&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=atoav"&gt;atoav&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364509"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364307"&gt;parent&lt;/a&gt; | &lt;a href="#46364572"&gt;prev&lt;/a&gt; | &lt;a href="#46364917"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Had the same thought. ChatGPT often tells me things like: "This is the hard truth" or "I am telling it to you as it is (no fluff)" or whatever. Just because my initial prompt contains a line about it not making things up and telling me how things are instead of what would please me to hear. I added a line to specifically tell it to not phrase out these things, but it appears to be surprisingly hard to get rid of those phrases.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364509&amp;amp;goto=item%3Fid%3D46363921%2346364509"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364917&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=snvzz"&gt;snvzz&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364917"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46364307"&gt;prev&lt;/a&gt; | &lt;a href="#46364271"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
For deeper understanding of seL4's developments and the historical context in which it appeared, Gernot Heiser's blog[0].&lt;p&gt;0. &lt;a href="https://microkerneldude.org/"&gt;https://microkerneldude.org/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364917&amp;amp;goto=item%3Fid%3D46363921%2346364917"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364271&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=throw_await"&gt;throw_await&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364271"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364917"&gt;prev&lt;/a&gt; | &lt;a href="#46364394"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
oldnewthing&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364271&amp;amp;goto=item%3Fid%3D46363921%2346364271"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364394&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=vibesareoff"&gt;vibesareoff&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364394"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364271"&gt;prev&lt;/a&gt; | &lt;a href="#46364716"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Ask the LLM you wrote this post with!&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364394&amp;amp;goto=item%3Fid%3D46363921%2346364394"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366786&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=voxleone"&gt;voxleone&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366786"&gt;1 hour ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;parent&lt;/a&gt; | &lt;a href="#46364684"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
No judgement here whatsoever, but i think LLM would be "the" tool for this job. I also wonder if there's any point to "Ask" sections in websites after LLM's.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366786&amp;amp;goto=item%3Fid%3D46363921%2346366786"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364684&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=bell-cot"&gt;bell-cot&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364684"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;parent&lt;/a&gt; | &lt;a href="#46366786"&gt;prev&lt;/a&gt; | &lt;a href="#46364467"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
OP is asking a good question.  There's no dishonor if he is not fluent in English, and used an LLM to translate.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364684&amp;amp;goto=item%3Fid%3D46363921%2346364684"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364808&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=vibesareoff"&gt;vibesareoff&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364808"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364684"&gt;parent&lt;/a&gt; | &lt;a href="#46364467"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
"OP" couldn't even be bothered to reformat the numbered list to run on separate fucking lines.&lt;p&gt;But sure, cheer on the homogenization of online spaces into beige slop staccato bullshit!&lt;p&gt;Ë™ ÍœÊŸË™&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364808&amp;amp;goto=item%3Fid%3D46363921%2346364808"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367743&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=CamperBob2"&gt;CamperBob2&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367743"&gt;18 minutes ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364808"&gt;parent&lt;/a&gt; | &lt;a href="#46366648"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Other sites beckon.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367743&amp;amp;goto=item%3Fid%3D46363921%2346367743"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366648&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=loloquwowndueo"&gt;loloquwowndueo&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366648"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364808"&gt;parent&lt;/a&gt; | &lt;a href="#46367743"&gt;prev&lt;/a&gt; | &lt;a href="#46365355"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
How do you reformat a list so it runs on separate fucking lines?&lt;p&gt;Always happens to me (and I donâ€™t use fucking LLMs) so Iâ€™d really like to know.&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366648&amp;amp;goto=item%3Fid%3D46363921%2346366648"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365355&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=fnordlord"&gt;fnordlord&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365355"&gt;4 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364808"&gt;parent&lt;/a&gt; | &lt;a href="#46366648"&gt;prev&lt;/a&gt; | &lt;a href="#46364934"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
I will always cheer on anyone who shares their curiosity.&lt;p&gt;It was a great question and now I have a ton of new things on my reading list.&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365355&amp;amp;goto=item%3Fid%3D46363921%2346365355"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364934&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=bell-cot"&gt;bell-cot&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364934"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364808"&gt;parent&lt;/a&gt; | &lt;a href="#46365355"&gt;prev&lt;/a&gt; | &lt;a href="#46364467"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
You seem to be picking metrics for their utility in angrily excluding people who you a priori despise.  :(&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364934&amp;amp;goto=item%3Fid%3D46363921%2346364934"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364467&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=sieste"&gt;sieste&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364467"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;parent&lt;/a&gt; | &lt;a href="#46364684"&gt;prev&lt;/a&gt; | &lt;a href="#46365859"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
The LLM instructed him to gather training data.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364467&amp;amp;goto=item%3Fid%3D46363921%2346364467"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364488&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=ozim"&gt;ozim&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364488"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364467"&gt;parent&lt;/a&gt; | &lt;a href="#46365859"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
So prompt injection on humans&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364488&amp;amp;goto=item%3Fid%3D46363921%2346364488"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364516&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=sieste"&gt;sieste&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364516"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364488"&gt;parent&lt;/a&gt; | &lt;a href="#46365859"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Polluting the internet with meat slop.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364516&amp;amp;goto=item%3Fid%3D46363921%2346364516"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364585&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=themafia"&gt;themafia&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364585"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364516"&gt;parent&lt;/a&gt; | &lt;a href="#46365859"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
"What if we used more energy and got worse results?"&lt;p&gt;Sort of makes you miss "move fast and break things."&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364585&amp;amp;goto=item%3Fid%3D46363921%2346364585"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365859&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=asupkay"&gt;asupkay&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365859"&gt;3 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;parent&lt;/a&gt; | &lt;a href="#46364467"&gt;prev&lt;/a&gt; | &lt;a href="#46364716"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
Maybe the LLM is the one asking&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365859&amp;amp;goto=item%3Fid%3D46363921%2346365859"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364716&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=Joel_Mckay"&gt;Joel_Mckay&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364716"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;prev&lt;/a&gt; | &lt;a href="#46364243"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
These should be read at least once in your life if interested in building industrial grade electrical, mechanical, and or software.&lt;p&gt;1. &lt;a href="https://nepp.nasa.gov/whisker/"&gt;https://nepp.nasa.gov/whisker/&lt;/a&gt;&lt;p&gt;2. &lt;a href="https://standards.nasa.gov/standard/NASA/NASA-STD-87394"&gt;https://standards.nasa.gov/standard/NASA/NASA-STD-87394&lt;/a&gt;&lt;p&gt;3. &lt;a href="https://standards.nasa.gov/NASA-Technical-Standards"&gt;https://standards.nasa.gov/NASA-Technical-Standards&lt;/a&gt;&lt;p&gt;4. &lt;a href="https://sma.nasa.gov/sma-disciplines/workmanship"&gt;https://sma.nasa.gov/sma-disciplines/workmanship&lt;/a&gt;&lt;p&gt;5. &lt;a href="https://www.stroustrup.com/JSF-AV-rules.pdf"&gt;https://www.stroustrup.com/JSF-AV-rules.pdf&lt;/a&gt;&lt;p&gt;6. &lt;a href="https://en.wikipedia.org/wiki/The_Power_of_10:_Rules_for_Developing_Safety-Critical_Code"&gt;https://en.wikipedia.org/wiki/The_Power_of_10:_Rules_for_Dev...&lt;/a&gt;&lt;p&gt;7. &lt;a href="https://www.nist.gov/pml/owm/laboratory-metrology/metrology-training"&gt;https://www.nist.gov/pml/owm/laboratory-metrology/metrology-...&lt;/a&gt;&lt;p&gt;8. &lt;a href="https://www.mitutoyo.com/training-education/"&gt;https://www.mitutoyo.com/training-education/&lt;/a&gt;&lt;p&gt;9. "Memoirs of extraordinary popular delusions and the madness of crowds" (Charles Mackay, 1852, &lt;a href="https://www.gutenberg.org/files/24518/24518-h/24518-h.htm"&gt;https://www.gutenberg.org/files/24518/24518-h/24518-h.htm&lt;/a&gt; )&lt;p&gt;The artifacts are usually beautiful from good Workmanship Standards, Design For Manufacturability, and systematic Metrology.  Dragging us all into the future one project at a time.&lt;p&gt;Note that training an ML model with such data would be pointless, as statistical saliency forms a paradox with consumer product design compromises. Note, there are _always_ tradeoffs in every problem domain.&lt;p&gt;'What it actually means to be "AI Generated"' ( &lt;a href="https://www.youtube.com/watch?v=ERiXDhLHxmo"&gt;https://www.youtube.com/watch?v=ERiXDhLHxmo&lt;/a&gt; )&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=iXbzktx1KfU"&gt;https://www.youtube.com/watch?v=iXbzktx1KfU&lt;/a&gt;&lt;p&gt;Have a nice day, and note &amp;gt;52% of the web is LLM slop now. YMMV =3&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364716&amp;amp;goto=item%3Fid%3D46363921%2346364716"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364243&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=gethly"&gt;gethly&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364243"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364716"&gt;prev&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[â€“]&lt;/a&gt;&lt;br/&gt;
There are no such blogs. Usually companies, or individuals, will write these after they implement some feature into their products. Which makes them inherently little pieces of information scattered all over the internet and there is no one blog that is just about this.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364243&amp;amp;goto=item%3Fid%3D46363921%2346364243"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;br/&gt;&lt;br/&gt;
&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;br/&gt;
&lt;a href="https://news.ycombinator.com/newsguidelines.html"&gt;Guidelines&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/newsfaq.html"&gt;FAQ&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/lists"&gt;Lists&lt;/a&gt; | &lt;a href="https://github.com/HackerNews/API"&gt;API&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/security.html"&gt;Security&lt;/a&gt; | &lt;a href="https://www.ycombinator.com/legal/"&gt;Legal&lt;/a&gt; | &lt;a href="https://www.ycombinator.com/apply/"&gt;Apply to YC&lt;/a&gt; | &lt;a href="mailto:hn@ycombinator.com"&gt;Contact&lt;/a&gt;&lt;br/&gt;&lt;br/&gt;
&lt;/body&gt;</content:encoded>
      <guid isPermaLink="false">https://news.ycombinator.com/item?id=46363921</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 09:50:31 +0000</pubDate>
    </item>
    <item>
      <title>Carnap â€“ A formal logic framework for Haskell</title>
      <link>https://carnap.io/</link>
      <description>A formal logic framework for Haskell</description>
      <content:encoded>&lt;body&gt;

&lt;h1&gt;Welcome to Carnap.io&lt;/h1&gt;
&lt;p&gt;A formal logic framework for Haskell&lt;/p&gt;


&lt;img src="https://static.carnap.io/img/russell2.png?etag=OWVOGUWw"/&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt; Carnap is a free and open software framework written in &lt;a href="https://haskell.org"&gt;Haskell&lt;/a&gt;
 for teaching and studying formal logic.  Carnap powers logic courses at &lt;a href="https://carnap.io/about#who"&gt;dozens of colleges and universities&lt;/a&gt;
 around the world.&lt;/p&gt;&lt;p&gt; If you're a student in a course that uses Carnap, please follow the links at the top of the page to log in and to access course materials.&lt;/p&gt;
&lt;p&gt; If you're just curious about Carnap, you can find some general information on our &lt;a href="https://carnap.io/about"&gt;about&lt;/a&gt;
 page.  If you're interested in the project, and would like to use Carnap in a class you're teaching, or get involved in some other way, please feel free to &lt;a href="mailto:gleachkr@ksu.edu"&gt;get in touch!&lt;/a&gt;
&lt;/p&gt;



&lt;/body&gt;</content:encoded>
      <guid isPermaLink="false">https://carnap.io/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 09:17:42 +0000</pubDate>
    </item>
    <item>
      <title>Instant database clones with PostgreSQL 18</title>
      <link>https://boringsql.com/posts/instant-database-clones/</link>
      <description>Have you ever watched long running migration script, wondering if it's about
to wreck your data? Or wish you can "just" spin a fresh copy of database for
each test run? Or wanted to have reproducible snapshots to reset between
runs of your test suite, (and yes, because you are reading boringSQL) needed
to reset the learning environment?</description>
      <content:encoded>&lt;article class="post-single"&gt;




Table of Contents


&lt;ul&gt;
&lt;li&gt;
&lt;a href="#create-database-strategy"&gt;CREATE DATABASE ... STRATEGY&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="#file-copy"&gt;FILE_COPY&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="#the-benchmark"&gt;The benchmark&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="#working-with-cloned-data"&gt;Working with cloned data&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="#xfs-proof"&gt;XFS proof&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="#things-to-be-aware-of"&gt;Things to be aware of&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;




&lt;p&gt;Have you ever watched long running migration script, wondering if it's about
to wreck your data? Or wish you can "just" spin a fresh copy of database for
each test run? Or wanted to have reproducible snapshots to reset between
runs of your test suite, (and yes, because you are reading boringSQL) needed
to reset the learning environment?&lt;/p&gt;
&lt;p&gt;When your database is a few megabytes, &lt;code&gt;pg_dump&lt;/code&gt; and restore works fine. But
what happens when you're dealing with hundreds of megabytes/gigabytes - or more?
Suddenly "just make a copy" becomes a burden.&lt;/p&gt;
&lt;p&gt;You've probably noticed that PostgreSQL connects to &lt;code&gt;template1&lt;/code&gt; by default. What
you might have missed is that there's a whole templating system hiding in plain
sight. Every time you run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE DATABASE dbname;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PostgreSQL quietly clones standard system database &lt;code&gt;template1&lt;/code&gt; behind the
scenes. Making it same as if you would use&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE DATABASE dbname TEMPLATE template1;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The real power comes from the fact that you can replace &lt;code&gt;template1&lt;/code&gt; with any
database. You can find more at &lt;a href="https://www.postgresql.org/docs/current/manage-ag-templatedbs.html"&gt;Template Database
documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this article, we will cover a few tweaks that turn this templating system
into an instant, zero-copy database cloning machine.&lt;/p&gt;
&lt;h2&gt;CREATE DATABASE ... STRATEGY&lt;a href="#create-database-strategy"&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Before PostgreSQL 15, when you created a new database from a template, it
operated strictly on the file level. This was effective, but to make it
reliable, Postgres had to flush all pending operations to disk (using
&lt;code&gt;CHECKPOINT&lt;/code&gt;) before taking a consistent snapshot. This created a massive I/O
spike - a "Checkpoint Storm" - that could stall your production traffic.&lt;/p&gt;
&lt;p&gt;Version 15 of PostgreSQL introduced new parameter &lt;code&gt;CREATE DATABASE ... STRATEGY = [strategy]&lt;/code&gt; and at the same time changed the default behaviour how the new
databases are created from templates. The new default become &lt;code&gt;WAL_LOG&lt;/code&gt; which
copies block-by-block via the Write-Ahead Log (WAL), making I/O sequential (and
much smoother) and support for concurrency without facing latency spike. This
prevented the need to CHECKPOINT but made the database cloning operation
potentially significantly slower. For an empty &lt;code&gt;template1&lt;/code&gt;, you won't notice the
difference. But if you try to clone a 500GB database using WAL_LOG, you are
going to be waiting a long time.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;STRATEGY&lt;/code&gt; parameter allows us to switch back to the original method
&lt;code&gt;FILE_COPY&lt;/code&gt; to keep the behaviour, and speed. And since PostgreSQL 18, this
opens the whole new set of options.&lt;/p&gt;
&lt;h2&gt;FILE_COPY&lt;a href="#file-copy"&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Because the &lt;code&gt;FILE_COPY&lt;/code&gt; strategy is a proxy to operating system file operations,
we can change how the OS handles those files.&lt;/p&gt;
&lt;p&gt;When using standard file system (like &lt;code&gt;ext4&lt;/code&gt;), PostgreSQL reads every byte of
the source file and writes it to a new location. It's a physical copy. However
starting with PostgreSQL 18 - &lt;code&gt;file_copy_method&lt;/code&gt; gives you options to switch
that logic; while default option remains &lt;code&gt;copy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;With modern filesystems (like ZFS, XFS with reflinks, APFS, etc.) you can switch
it to &lt;code&gt;clone&lt;/code&gt; and leverage &lt;code&gt;CLONE&lt;/code&gt; (&lt;code&gt;FICLONE&lt;/code&gt; on Linux) operation for almost
instant operation. And it won't take any additional space.&lt;/p&gt;
&lt;p&gt;All you have to do is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linux with XFS or ZFS support (we will use XFS for the demostration) or similar
operating system. MacOS APFS is also fully supported. FreeBSD with ZFS also
supported (which normally would be my choice, but haven't got time to test so
far)&lt;/li&gt;
&lt;li&gt;PostgreSQL cluster on that file system&lt;/li&gt;
&lt;li&gt;update the configuration &lt;code&gt;file_copy_method = clone&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;and reload the configuration&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The benchmark&lt;a href="#the-benchmark"&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;We need some dummy data to copy. This is the only part of the tutorial where you
have to wait. Let's generate a ~6GB database.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE DATABASE source_db;
\c source_db

CREATE TABLE boring_data (
    id serial PRIMARY KEY,
    payload text
);

-- generate 50m rows
INSERT INTO boring_data (payload)
SELECT md5(random()::text) || md5(random()::text)
FROM generate_series(1, 50000000);

-- force a checkpoint
CHECKPOINT;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can verify the database now has roughly 6GB of data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name              | source_db
Owner             | postgres
Encoding          | UTF8
Locale Provider   | libc
Collate           | en_US.UTF-8
Ctype             | en_US.UTF-8
Locale            |
ICU Rules         |
Access privileges |
Size              | 6289 MB
Tablespace        | pg_default
Description       |
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While enabling &lt;code&gt;\timing&lt;/code&gt; you can test the default (WAL_LOG) strategy. And on my
test volume (relatively slow storage) I get&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE DATABASE slow_copy TEMPLATE source_db;
CREATE DATABASE
Time: 67000.615 ms (01:07.001)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let's verify our configuration is set for speed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;show file_copy_method;
 file_copy_method
------------------
 clone
(1 row)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let's request the semi-instant clone of the same database, without taking
extra disk space at the same time.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE DATABASE fast_clone TEMPLATE source_db STRATEGY=FILE_COPY;
CREATE DATABASE
Time: 212.053 ms
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That's a quite an improvement, isn't it?&lt;/p&gt;
&lt;h2&gt;Working with cloned data&lt;a href="#working-with-cloned-data"&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;That was the simple part. But what is happening behind the scenes?&lt;/p&gt;
&lt;p&gt;When you clone a database with &lt;code&gt;file_copy_method = clone&lt;/code&gt;, PostgreSQL doesn't
duplicate any data. The filesystem creates new metadata entries that point to
the same physical blocks. Both databases share identical storage.&lt;/p&gt;
&lt;p&gt;This can create some initial confusion. If you ask PostgreSQL for the size:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT pg_database_size('source_db') as source,
       pg_database_size('fast_clone') as clone;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PostgreSQL reports both as ~6GB because that's the logical size - how much data
each database "contains" - i.e. logical size.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;-[ RECORD 1 ]------
source | 6594041535
clone  | 6594041535
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The interesting part happens when you start writing. PostgreSQL doesn't update
tuples in place. When you UPDATE a row, it writes a new tuple version somewhere
(often a different page entirely) and marks the old one as dead. The filesystem
doesn't care about PostgreSQL internals - it just sees writes to 8KB pages. Any
write to a shared page triggers a copy of that entire page.&lt;/p&gt;
&lt;p&gt;A single UPDATE will therefore trigger copy-on-write on multiple pages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the page holding the old tuple&lt;/li&gt;
&lt;li&gt;the page receiving the new tuple&lt;/li&gt;
&lt;li&gt;index pages if any indexed columns changed&lt;/li&gt;
&lt;li&gt;FSM and visibility map pages as PostgreSQL tracks free space&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And later, VACUUM touches even more pages while cleaning up dead tuples. In this
case diverging quickly from the linked storage.&lt;/p&gt;
&lt;h2&gt;XFS proof&lt;a href="#xfs-proof"&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Using the database OID and relfilenode we can verify the both databases are now
sharing physical blocks.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16402/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16402/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..    2031:   10471550..  10473581:   2032:             shared
   1:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   2:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   3:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   4:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   5:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   6:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16402/16404: 7 extents found
root@clone-demo:/var/lib/postgresql#
root@clone-demo:/var/lib/postgresql#
root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16418/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16418/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..    2031:   10471550..  10473581:   2032:             shared
   1:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   2:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   3:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   4:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   5:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   6:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16418/16404: 7 extents found
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All it takes is to update some rows using&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;update boring_data set payload = 'new value' || id where id IN (select id from boring_data limit 20);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the situation will start to change.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16402/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16402/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..      39:   10471550..  10471589:     40:
   1:       40..    2031:   10471590..  10473581:   1992:             shared
   2:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   3:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   4:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   5:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   6:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   7:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16402/16404: 7 extents found
root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16418/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16418/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..      39:   10297326..  10297365:     40:
   1:       40..    2031:   10471590..  10473581:   1992:   10297366: shared
   2:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   3:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   4:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   5:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   6:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   7:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16418/16404: 8 extents found
root@clone-demo:/var/lib/postgresql#
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case extent 0 no longer has shared flag, first 40 blocks size (with
default size 4KB) now diverge, making it total of 160KB. Each database now has
its own copy at different physical address. The remaining extents are still
shared.&lt;/p&gt;
&lt;h2&gt;Things to be aware of&lt;a href="#things-to-be-aware-of"&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Cloning is tempting but there's one serious limitation you need to be aware if
you ever attempt to do it in production. The source database can't have any
active connections during cloning. This is a PostgreSQL limitation, not a
filesystem one. For production use, this usually means you create a dedicated
template database rather than cloning your live database directly. Or given the
relatively short time the operation takes you have to schedule the cloning in
times where you can temporary block/terminate all connections.&lt;/p&gt;
&lt;p&gt;Other limitation is that the cloning only works within a single filesystem. If
your databases spans multiple table spaces on different mount points, cloning
will fall back to regular physical copy.&lt;/p&gt;
&lt;p&gt;Finally, in most managed cloud environments (AWS RDS, Google Cloud SQL), you
will not have access to the underlying filesystem to configure this. You are
stuck with their proprietary (and often billed) functionality. But for your own
VMs or bare metal? Go ahead and try it.&lt;/p&gt;


&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://boringsql.com/posts/instant-database-clones/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 07:58:25 +0000</pubDate>
    </item>
    <item>
      <title>10 years bootstrapped: â‚¬6.5M revenue with a team of 13</title>
      <link>https://www.datocms.com/blog/a-look-back-at-2025</link>
      <description>Explore all the capabilities of the friendliest Headless CMS in town</description>
      <content:encoded>&lt;article class="_itemBody_11r63_114"&gt;  &lt;h4&gt;Meet DatoCMS!&lt;/h4&gt;  &lt;p&gt;Explore all the capabilities of the friendliest Headless CMS in town&lt;/p&gt; &lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://www.datocms.com/blog/a-look-back-at-2025</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 07:50:03 +0000</pubDate>
    </item>
    <item>
      <title>Show HN: CineCLI â€“ Browse and torrent movies directly from your terminal</title>
      <link>https://github.com/eyeblech/cinecli</link>
      <description>GitHub - eyeblech/cinecli: CineCLI is a cross-platform command-line movie browser built with Python.</description>
      <content:encoded>&lt;article class="markdown-body entry-content container-lg" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://camo.githubusercontent.com/d7a2a0757d30060c754de6e7b99cb5b4f0ffbc5ca1c30226c193693c606268c7/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f63696e65636c69"&gt;&lt;img alt="PyPI" src="https://camo.githubusercontent.com/d7a2a0757d30060c754de6e7b99cb5b4f0ffbc5ca1c30226c193693c606268c7/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f63696e65636c69"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/6095792cccbdf1e7ff3823e190b373efb89d3be5920ef7d442bffad7e06a46d4/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f63696e65636c69"&gt;&lt;img alt="Python" src="https://camo.githubusercontent.com/6095792cccbdf1e7ff3823e190b373efb89d3be5920ef7d442bffad7e06a46d4/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f63696e65636c69"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/f8df3091bbe1149f398a5369b2c39e896766f9f6efba3477c63e9b4aa940ef14/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e"&gt;&lt;img alt="License" src="https://camo.githubusercontent.com/f8df3091bbe1149f398a5369b2c39e896766f9f6efba3477c63e9b4aa940ef14/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/8016a83d8d76dafb654d0d0996f171069ef251332c073a69a63ea84549a1c9b6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f63696e65636c69"&gt;&lt;img alt="Downloads" src="https://camo.githubusercontent.com/8016a83d8d76dafb654d0d0996f171069ef251332c073a69a63ea84549a1c9b6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f63696e65636c69"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://camo.githubusercontent.com/5714c1d3e40f657a117aa9c961de32bf3aa29c10f83ea590e736f48c08a628a5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f657965626c6563682f63696e65636c693f7374796c653d666c61742d737175617265"&gt;&lt;img alt="Stars" src="https://camo.githubusercontent.com/5714c1d3e40f657a117aa9c961de32bf3aa29c10f83ea590e736f48c08a628a5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f657965626c6563682f63696e65636c693f7374796c653d666c61742d737175617265"/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;ğŸ“¡ YTS API Status&lt;/h2&gt;&lt;a href="#-yts-api-status"&gt;&lt;/a&gt;
&lt;p&gt;
&lt;a href="https://camo.githubusercontent.com/ee1c288267112cfd617cecbe2f9ec3209df0d2dbe299de8b1cd3fb4df4e3bfb5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f657965626c6563682f63696e65636c692f6170692d6865616c74682e796d6c3f6c6162656c3d595453253230415049267374796c653d666f722d7468652d6261646765"&gt;&lt;img alt="YTS API Status" src="https://camo.githubusercontent.com/ee1c288267112cfd617cecbe2f9ec3209df0d2dbe299de8b1cd3fb4df4e3bfb5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f657965626c6563682f63696e65636c692f6170692d6865616c74682e796d6c3f6c6162656c3d595453253230415049267374796c653d666f722d7468652d6261646765"/&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;Status is automatically monitored every 15 minutes.&lt;/strong&gt;&lt;br/&gt;

    ğŸŸ¢ Green = Operational Â  â€¢ Â 
    ğŸ”´ Red = Outage / API Down
  
&lt;/p&gt;

&lt;h1&gt;ğŸ¬ CineCLI&lt;/h1&gt;&lt;a href="#-cinecli"&gt;&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;Browse, inspect, and launch movie torrents directly from your terminal.&lt;br/&gt;
Fast. Cross-platform. Minimal. Beautiful.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/eyeblech/cinecli/blob/master/demo.gif"&gt;&lt;img alt="Demo" src="https://github.com/eyeblech/cinecli/raw/master/demo.gif"/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://camo.githubusercontent.com/6bce8c074ba1847154d5448fa4b909b24bdc5e2dead78e01601b2388cba04190/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6c696e75782532302537432532306d61634f5325323025374325323077696e646f77732d626c7565"&gt;&lt;img alt="Platform" src="https://camo.githubusercontent.com/6bce8c074ba1847154d5448fa4b909b24bdc5e2dead78e01601b2388cba04190/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6c696e75782532302537432532306d61634f5325323025374325323077696e646f77732d626c7565"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/4d05de71d175fd180e6a54008bb655173476617f053e30cce46a2ae0c33139dc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e746572666163652d434c492d6f72616e6765"&gt;&lt;img alt="CLI" src="https://camo.githubusercontent.com/4d05de71d175fd180e6a54008bb655173476617f053e30cce46a2ae0c33139dc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e746572666163652d434c492d6f72616e6765"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/a875f41a2bd54510be00d5743cb13308c385a54e0b945c0c2d49622d786f1ddf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f776f726b73253230696e2d7465726d696e616c2d626c61636b"&gt;&lt;img alt="Terminal" src="https://camo.githubusercontent.com/a875f41a2bd54510be00d5743cb13308c385a54e0b945c0c2d49622d786f1ddf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f776f726b73253230696e2d7465726d696e616c2d626c61636b"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;âœ¨ Features&lt;/h2&gt;&lt;a href="#-features"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ” Search movies from &lt;strong&gt;YTS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;ğŸ¥ View detailed movie information&lt;/li&gt;
&lt;li&gt;ğŸ§² Launch magnet links directly into your torrent client&lt;/li&gt;
&lt;li&gt;ğŸ“¦ Download &lt;code&gt;.torrent&lt;/code&gt; files if preferred&lt;/li&gt;
&lt;li&gt;âš¡ Auto-select best torrent (highest quality + healthy seeds)&lt;/li&gt;
&lt;li&gt;ğŸ–¥ Cross-platform (Linux, macOS, Windows)&lt;/li&gt;
&lt;li&gt;ğŸ¨ Rich, clean terminal UI (powered by &lt;code&gt;rich&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;ğŸ§  Smart defaults with full user control&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href="https://camo.githubusercontent.com/24291b36ca6bfe1e10b218fe428f13ad6a14a9ad12c9d0f814872d851314e944/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6275696c74253230776974682d54797065722d666636396234"&gt;&lt;img alt="Built with Typer" src="https://camo.githubusercontent.com/24291b36ca6bfe1e10b218fe428f13ad6a14a9ad12c9d0f814872d851314e944/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6275696c74253230776974682d54797065722d666636396234"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/7a65b723320e5684e19ed5a6a8383a2f2722db455ae63eabb40f81d666aa1fdd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6275696c74253230776974682d526963682d626c756576696f6c6574"&gt;&lt;img alt="Built with Rich" src="https://camo.githubusercontent.com/7a65b723320e5684e19ed5a6a8383a2f2722db455ae63eabb40f81d666aa1fdd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6275696c74253230776974682d526963682d626c756576696f6c6574"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;ğŸ“¦ Installation&lt;/h2&gt;&lt;a href="#-installation"&gt;&lt;/a&gt;
&lt;pre&gt;pip install cinecli
&lt;/pre&gt;
&lt;p&gt;Requires &lt;strong&gt;Python 3.9+&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;ğŸš€ Usage&lt;/h2&gt;&lt;a href="#-usage"&gt;&lt;/a&gt;
&lt;h3&gt;ğŸ” Search for movies&lt;/h3&gt;&lt;a href="#-search-for-movies"&gt;&lt;/a&gt;
&lt;pre&gt;cinecli search matrix
&lt;/pre&gt;
&lt;p&gt;Displays matching movies with IDs:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ID     Title                 Year   Rating
3525   The Matrix            1999   8.7
3526   The Matrix Reloaded   2003   7.2

&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;ğŸ¬ Watch a movie&lt;/h3&gt;&lt;a href="#-watch-a-movie"&gt;&lt;/a&gt;
&lt;pre&gt;cinecli watch 3525
&lt;/pre&gt;
&lt;p&gt;What happens:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Shows movie details&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lists available torrents&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Auto-selects the best option (you can override)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Launches magnet or downloads &lt;code&gt;.torrent&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;ğŸ§­ Interactive mode (recommended for exploration)&lt;/h3&gt;&lt;a href="#-interactive-mode-recommended-for-exploration"&gt;&lt;/a&gt;
&lt;pre&gt;cinecli interactive
&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Search â†’ select movie â†’ choose torrent&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Manual selection by design (safe &amp;amp; explicit)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;âš™ï¸ How magnet launching works&lt;/h2&gt;&lt;a href="#ï¸-how-magnet-launching-works"&gt;&lt;/a&gt;
&lt;p&gt;CineCLI delegates magnet handling to your OS.&lt;/p&gt;
&lt;p&gt;That means:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Whatever torrent client is registered (&lt;code&gt;qBittorrent&lt;/code&gt;, &lt;code&gt;Transmission&lt;/code&gt;, etc.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CineCLI will launch it directly&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example (Linux):&lt;/p&gt;
&lt;pre&gt;xdg-mime query default x-scheme-handler/magnet
&lt;/pre&gt;

&lt;h2&gt;ğŸ Demo Video&lt;/h2&gt;&lt;a href="#-demo-video"&gt;&lt;/a&gt;
&lt;p&gt;Full terminal walkthrough:&lt;/p&gt;



demo.mov






&lt;h2&gt;ğŸ›  Tech Stack&lt;/h2&gt;&lt;a href="#-tech-stack"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Python&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Typer&lt;/strong&gt; â€” CLI framework&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rich&lt;/strong&gt; â€” terminal UI&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Requests&lt;/strong&gt; â€” API communication&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;YTS API&lt;/strong&gt; â€” movie data source&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;ğŸ“„ License&lt;/h2&gt;&lt;a href="#-license"&gt;&lt;/a&gt;
&lt;p&gt;MITâ€”see &lt;a href="https://github.com/eyeblech/cinecli/blob/master/LICENSE"&gt;LICENSE&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Use it. Fork it. Improve it.&lt;/p&gt;

&lt;h2&gt;ğŸ™Œ Author&lt;/h2&gt;&lt;a href="#-author"&gt;&lt;/a&gt;
&lt;p&gt;Built by &lt;strong&gt;eyeblech&lt;/strong&gt;&lt;br/&gt;
ğŸ“§ &lt;a href="mailto:0x1123@proton.me"&gt;0x1123@proton.me&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;STAR the repo if you like it! â­&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://camo.githubusercontent.com/55adbef48125ea91f8864924c309ff1710b23c84e01b0670b45e467e795ae13b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6f70656e2d2d736f757263652d7965732d627269676874677265656e"&gt;&lt;img alt="Open Source" src="https://camo.githubusercontent.com/55adbef48125ea91f8864924c309ff1710b23c84e01b0670b45e467e795ae13b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6f70656e2d2d736f757263652d7965732d627269676874677265656e"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/da21b16c25c3d396a17fe7ba748e0497a765b934030128e412161493c4ea5bec/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d61696e7461696e65642d7965732d73756363657373"&gt;&lt;img alt="Maintained" src="https://camo.githubusercontent.com/da21b16c25c3d396a17fe7ba748e0497a765b934030128e412161493c4ea5bec/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d61696e7461696e65642d7965732d73756363657373"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/03fe1c696836c0a3e46f187fad3b80abadafb5f331737069810e10468871395f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5052732d77656c636f6d652d707572706c65"&gt;&lt;img alt="PRs Welcome" src="https://camo.githubusercontent.com/03fe1c696836c0a3e46f187fad3b80abadafb5f331737069810e10468871395f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5052732d77656c636f6d652d707572706c65"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;â­ Star History&lt;/h2&gt;&lt;a href="#-star-history"&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href="https://star-history.com/#eyeblech/cinecli&amp;amp;Date"&gt;&lt;img alt="Star History Chart" src="https://camo.githubusercontent.com/b96f697184e70f5a4a70e67126c617988ef3cbcab75c7d140ab123f6695090ab/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d657965626c6563682f63696e65636c6926747970653d44617465"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://github.com/eyeblech/cinecli</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 05:17:50 +0000</pubDate>
    </item>
    <item>
      <title>Snitch â€“ A friendlier ss/netstat</title>
      <link>https://github.com/karol-broda/snitch</link>
      <description>a friendlier ss / netstat for humans. inspect network connections with a clean tui or styled tables.</description>
      <content:encoded>&lt;article class="markdown-body entry-content container-lg" itemprop="text"&gt;&lt;h1&gt;snitch&lt;/h1&gt;&lt;a href="#snitch"&gt;&lt;/a&gt;
&lt;p&gt;a friendlier &lt;code&gt;ss&lt;/code&gt; / &lt;code&gt;netstat&lt;/code&gt; for humans. inspect network connections with a clean tui or styled tables.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/karol-broda/snitch/blob/master/demo/demo.gif"&gt;&lt;img alt="snitch demo" src="https://github.com/karol-broda/snitch/raw/master/demo/demo.gif"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;install&lt;/h2&gt;&lt;a href="#install"&gt;&lt;/a&gt;
&lt;h3&gt;go&lt;/h3&gt;&lt;a href="#go"&gt;&lt;/a&gt;
&lt;pre&gt;go install github.com/karol-broda/snitch@latest&lt;/pre&gt;
&lt;h3&gt;nixos / nix&lt;/h3&gt;&lt;a href="#nixos--nix"&gt;&lt;/a&gt;
&lt;pre&gt;# try it
nix run github:karol-broda/snitch

# install to profile
nix profile install github:karol-broda/snitch

# or add to flake inputs
{
  inputs.snitch.url = "github:karol-broda/snitch";
}
# then use: inputs.snitch.packages.${system}.default&lt;/pre&gt;
&lt;h3&gt;arch linux (aur)&lt;/h3&gt;&lt;a href="#arch-linux-aur"&gt;&lt;/a&gt;
&lt;pre&gt;# with yay
yay -S snitch-bin

# with paru
paru -S snitch-bin&lt;/pre&gt;
&lt;h3&gt;shell script&lt;/h3&gt;&lt;a href="#shell-script"&gt;&lt;/a&gt;
&lt;pre&gt;curl -sSL https://raw.githubusercontent.com/karol-broda/snitch/master/install.sh | sh&lt;/pre&gt;
&lt;p&gt;installs to &lt;code&gt;~/.local/bin&lt;/code&gt; if available, otherwise &lt;code&gt;/usr/local/bin&lt;/code&gt;. override with:&lt;/p&gt;
&lt;pre&gt;curl -sSL https://raw.githubusercontent.com/karol-broda/snitch/master/install.sh | INSTALL_DIR=~/bin sh&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;macos:&lt;/strong&gt; the install script automatically removes the quarantine attribute (&lt;code&gt;com.apple.quarantine&lt;/code&gt;) from the binary to allow it to run without gatekeeper warnings. to disable this, set &lt;code&gt;KEEP_QUARANTINE=1&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;binary&lt;/h3&gt;&lt;a href="#binary"&gt;&lt;/a&gt;
&lt;p&gt;download from &lt;a href="https://github.com/karol-broda/snitch/releases"&gt;releases&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;linux:&lt;/strong&gt; &lt;code&gt;snitch_&amp;lt;version&amp;gt;_linux_&amp;lt;arch&amp;gt;.tar.gz&lt;/code&gt; or &lt;code&gt;.deb&lt;/code&gt;/&lt;code&gt;.rpm&lt;/code&gt;/&lt;code&gt;.apk&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;macos:&lt;/strong&gt; &lt;code&gt;snitch_&amp;lt;version&amp;gt;_darwin_&amp;lt;arch&amp;gt;.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;tar xzf snitch_*.tar.gz
sudo mv snitch /usr/local/bin/&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;macos:&lt;/strong&gt; if blocked with "cannot be opened because the developer cannot be verified", run:&lt;/p&gt;
&lt;pre&gt;xattr -d com.apple.quarantine /usr/local/bin/snitch&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h2&gt;quick start&lt;/h2&gt;&lt;a href="#quick-start"&gt;&lt;/a&gt;
&lt;pre&gt;snitch              # launch interactive tui
snitch -l           # tui showing only listening sockets
snitch ls           # print styled table and exit
snitch ls -l        # listening sockets only
snitch ls -t -e     # tcp established connections
snitch ls -p        # plain output (parsable)&lt;/pre&gt;
&lt;h2&gt;commands&lt;/h2&gt;&lt;a href="#commands"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;code&gt;snitch&lt;/code&gt; / &lt;code&gt;snitch top&lt;/code&gt;&lt;/h3&gt;&lt;a href="#snitch--snitch-top"&gt;&lt;/a&gt;
&lt;p&gt;interactive tui with live-updating connection list.&lt;/p&gt;
&lt;pre&gt;snitch                  # all connections
snitch -l               # listening only
snitch -t               # tcp only
snitch -e               # established only
snitch -i 2s            # 2 second refresh interval&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;keybindings:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;j/k, â†‘/â†“      navigate
g/G           top/bottom
t/u           toggle tcp/udp
l/e/o         toggle listen/established/other
s/S           cycle sort / reverse
w             watch/monitor process (highlight)
W             clear all watched
K             kill process (with confirmation)
/             search
enter         connection details
?             help
q             quit
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;code&gt;snitch ls&lt;/code&gt;&lt;/h3&gt;&lt;a href="#snitch-ls"&gt;&lt;/a&gt;
&lt;p&gt;one-shot table output. uses a pager automatically if output exceeds terminal height.&lt;/p&gt;
&lt;pre&gt;snitch ls               # styled table (default)
snitch ls -l            # listening only
snitch ls -t -l         # tcp listeners
snitch ls -e            # established only
snitch ls -p            # plain/parsable output
snitch ls -o json       # json output
snitch ls -o csv        # csv output
snitch ls -n            # numeric (no dns resolution)
snitch ls --no-headers  # omit headers&lt;/pre&gt;
&lt;h3&gt;&lt;code&gt;snitch json&lt;/code&gt;&lt;/h3&gt;&lt;a href="#snitch-json"&gt;&lt;/a&gt;
&lt;p&gt;json output for scripting.&lt;/p&gt;
&lt;pre&gt;snitch json
snitch json -l&lt;/pre&gt;
&lt;h3&gt;&lt;code&gt;snitch watch&lt;/code&gt;&lt;/h3&gt;&lt;a href="#snitch-watch"&gt;&lt;/a&gt;
&lt;p&gt;stream json frames at an interval.&lt;/p&gt;
&lt;pre&gt;snitch watch -i 1s | jq '.count'
snitch watch -l -i 500ms&lt;/pre&gt;
&lt;h3&gt;&lt;code&gt;snitch upgrade&lt;/code&gt;&lt;/h3&gt;&lt;a href="#snitch-upgrade"&gt;&lt;/a&gt;
&lt;p&gt;check for updates and upgrade in-place.&lt;/p&gt;
&lt;pre&gt;snitch upgrade              # check for updates
snitch upgrade --yes        # upgrade automatically
snitch upgrade -v 0.1.7     # install specific version&lt;/pre&gt;
&lt;h2&gt;filters&lt;/h2&gt;&lt;a href="#filters"&gt;&lt;/a&gt;
&lt;p&gt;shortcut flags work on all commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;-t, --tcp           tcp only
-u, --udp           udp only
-l, --listen        listening sockets
-e, --established   established connections
-4, --ipv4          ipv4 only
-6, --ipv6          ipv6 only
-n, --numeric       no dns resolution
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;for more specific filtering, use &lt;code&gt;key=value&lt;/code&gt; syntax with &lt;code&gt;ls&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;snitch ls proto=tcp state=listen
snitch ls pid=1234
snitch ls proc=nginx
snitch ls lport=443
snitch ls contains=google&lt;/pre&gt;
&lt;h2&gt;output&lt;/h2&gt;&lt;a href="#output"&gt;&lt;/a&gt;
&lt;p&gt;styled table (default):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â•®
  â”‚ PROCESS         â”‚ PID   â”‚ PROTO â”‚ STATE       â”‚ LADDR           â”‚ LPORT  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ nginx           â”‚ 1234  â”‚ tcp   â”‚ LISTEN      â”‚ *               â”‚ 80     â”‚
  â”‚ postgres        â”‚ 5678  â”‚ tcp   â”‚ LISTEN      â”‚ 127.0.0.1       â”‚ 5432   â”‚
  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â•¯
  2 connections
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;plain output (&lt;code&gt;-p&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PROCESS    PID    PROTO   STATE    LADDR       LPORT
nginx      1234   tcp     LISTEN   *           80
postgres   5678   tcp     LISTEN   127.0.0.1   5432
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;configuration&lt;/h2&gt;&lt;a href="#configuration"&gt;&lt;/a&gt;
&lt;p&gt;optional config file at &lt;code&gt;~/.config/snitch/snitch.toml&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;[defaults]
numeric = false
theme = "auto"&lt;/pre&gt;
&lt;h2&gt;requirements&lt;/h2&gt;&lt;a href="#requirements"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;linux or macos&lt;/li&gt;
&lt;li&gt;linux: reads from &lt;code&gt;/proc/net/*&lt;/code&gt;, root or &lt;code&gt;CAP_NET_ADMIN&lt;/code&gt; for full process info&lt;/li&gt;
&lt;li&gt;macos: uses system APIs, may require sudo for full process info&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://github.com/karol-broda/snitch</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 01:03:57 +0000</pubDate>
    </item>
    <item>
      <title>Inside CECOT â€“ 60 Minutes [video]</title>
      <link>https://archive.org/details/insidececot</link>
      <description>0 Views</description>
      <content:encoded>&lt;main id="maincontent"&gt;

&lt;!--HTML--&gt;
&lt;!--//.container-ia--&gt;






&lt;h1&gt;
    Inside CECOT | 60 Minutes  &lt;/h1&gt;
&lt;h2&gt;
    Video Item Preview
  &lt;/h2&gt;




&lt;a href="#"&gt;&lt;/a&gt;
&lt;a href="#"&gt;

&lt;/a&gt;
&lt;a href="#"&gt;&lt;/a&gt;
&lt;a href="#"&gt;


&lt;/a&gt;
&lt;!--/#theatre-controls--&gt;


&lt;!--/.xs-col-12--&gt;
&lt;!--/.row--&gt;






remove-circle 
&lt;h1&gt;Share or Embed This Item&lt;/h1&gt;
&lt;!--/.modal-header--&gt;




&lt;a href="https://twitter.com/intent/tweet?url=https://archive.org/details/insidececot&amp;amp;via=internetarchive&amp;amp;text=Inside+CECOT+%7C+60+Minutes+%3A+CBS+News+%3A+Free+Download%2C+Borrow%2C+and+Streaming+%3A+Internet+Archive"&gt;

Share to Twitter
&lt;/a&gt;
&lt;a href="https://www.facebook.com/sharer/sharer.php?u=https://archive.org/details/insidececot"&gt;

Share to Facebook
&lt;/a&gt;
&lt;a href="http://www.reddit.com/submit?url=https://archive.org/details/insidececot&amp;amp;title=Inside+CECOT+%7C+60+Minutes+%3A+CBS+News+%3A+Free+Download%2C+Borrow%2C+and+Streaming+%3A+Internet+Archive"&gt;

Share to Reddit
&lt;/a&gt;
&lt;a href="https://www.tumblr.com/widgets/share/tool?posttype=link&amp;amp;title=Inside+CECOT+%7C+60+Minutes+%3A+CBS+News+%3A+Free+Download%2C+Borrow%2C+and+Streaming+%3A+Internet+Archive&amp;amp;caption=Inside+CECOT+%7C+60+Minutes+%3A+CBS+News+%3A+Free+Download%2C+Borrow%2C+and+Streaming+%3A+Internet+Archive&amp;amp;content=https://archive.org/details/insidececot&amp;amp;canonicalUrl=https://archive.org/details/insidececot"&gt;

Share to Tumblr
&lt;/a&gt;
&lt;a href="http://www.pinterest.com/pin/create/button/?url=https://archive.org/details/insidececot&amp;amp;description=Inside+CECOT+%7C+60+Minutes+%3A+CBS+News+%3A+Free+Download%2C+Borrow%2C+and+Streaming+%3A+Internet+Archive"&gt;

Share to Pinterest
&lt;/a&gt;
&lt;a href="mailto:?body=https://archive.org/details/insidececot&amp;amp;subject=Inside CECOT | 60 Minutes : CBS News : Free Download, Borrow, and Streaming : Internet Archive"&gt;

Share via email
&lt;/a&gt;
&lt;a href="#"&gt;

&lt;img src="https://archive.org/images/link.svg"&gt;
&lt;/img&gt;
Copy Link
&lt;/a&gt;

&lt;br&gt;
&lt;/br&gt;



Begin playing at:
&lt;br&gt;
&lt;/br&gt;







                Want more?
                &lt;a href="https://archive.org/help/video.php?identifier=insidececot"&gt;Advanced embedding details, examples, and help&lt;/a&gt;!
              
&lt;!--/#cher-body--&gt;
&lt;!--/.modal-content--&gt;

&lt;!--/.modal-dialog--&gt;
&lt;!--/#cher-modal--&gt;
&lt;!--//#theatre-ia--&gt;


&lt;!--//.container-ia--&gt;

&lt;!--/.container-ia--&gt;








Favorite 




Share 



Flag

&lt;h1&gt;Flag this item for&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href="https://archive.org/login?referer=https%3A%2F%2Farchive.org%2Fdetails%2Finsidececot"&gt;
                  Graphic Violence                &lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://archive.org/login?referer=https%3A%2F%2Farchive.org%2Fdetails%2Finsidececot"&gt;
                  Explicit Sexual Content                &lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://archive.org/login?referer=https%3A%2F%2Farchive.org%2Fdetails%2Finsidececot"&gt;
                  Hate Speech                &lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://archive.org/login?referer=https%3A%2F%2Farchive.org%2Fdetails%2Finsidececot"&gt;
                  Misinformation/Disinformation                &lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://archive.org/login?referer=https%3A%2F%2Farchive.org%2Fdetails%2Finsidececot"&gt;
                  Marketing/Phishing/Advertising                &lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://archive.org/login?referer=https%3A%2F%2Farchive.org%2Fdetails%2Finsidececot"&gt;
                  Misleading/Inaccurate/Missing Metadata                &lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
 &lt;!-- /#flag-popover --&gt;
 &lt;!--/.dropdown --&gt;
 
&lt;!--/.thats-right--&gt;

movies
&lt;h1&gt;
Inside CECOT | 60 Minutes
&lt;/h1&gt;

by

&lt;a href="https://archive.org/search.php?query=creator%3A%22CBS+News%22"&gt;CBS News&lt;/a&gt; 

&lt;br&gt;
&lt;/br&gt;&lt;!--/.thats-left--&gt;











Publication date

&lt;a href="https://archive.org/search.php?query=date:2025-12-22"&gt;
2025-12-22
&lt;/a&gt;

Topics
&lt;a href="https://archive.org/search.php?query=subject%3A%2260+Minutes%22"&gt;60 Minutes&lt;/a&gt;
&lt;!-- contributor (also does usage rights, if specified for the contributor) --&gt;
&lt;!-- display Item Size in this position if theatre_type is not TV --&gt;
Item Size

            1.4G                

 &lt;!-- class="row metadata-list" --&gt;

Sharyn Alfonsi's "Inside CECOT" for 60 Minutes, which was censored by Bari Weiss, as it appeared on Canada's Global TV app.
&lt;!--/#descript--&gt;


Addeddate

2025-12-23 00:05:32 
Identifier

insidececot 
Scanner

            Internet Archive HTML5 Uploader 1.7.0                





&lt;h2&gt;


plus-circle Add Review

&lt;br&gt;
&lt;/br&gt;
comment
        Reviews (1)
&lt;/h2&gt;



&lt;!--/.thats-left--&gt;


&lt;p&gt;

0

        Views      &lt;/p&gt;
&lt;p&gt;
102
Favorites
&lt;/p&gt;
&lt;p&gt;
&lt;a href="#reviews"&gt;
1
            Review          &lt;/a&gt;
&lt;/p&gt;


&lt;h1&gt;
        DOWNLOAD OPTIONS
      &lt;/h1&gt;


&lt;a href="https://archive.org/download/insidececot/60minutesCECOTsegment.ia.mp4"&gt;

download                    1 file                  
&lt;/a&gt;

&lt;a href="https://archive.org/download/insidececot/60minutesCECOTsegment.ia.mp4"&gt;
                  H.264 IA                  download &lt;/a&gt;



&lt;a href="https://archive.org/download/insidececot/__ia_thumb.jpg"&gt;

download                    1 file                  
&lt;/a&gt;

&lt;a href="https://archive.org/download/insidececot/__ia_thumb.jpg"&gt;
                  ITEM TILE                  download &lt;/a&gt;



&lt;a href="https://archive.org/download/insidececot/60minutesCECOTsegment.mp4"&gt;

download                    1 file                  
&lt;/a&gt;

&lt;a href="https://archive.org/download/insidececot/60minutesCECOTsegment.mp4"&gt;
                  MPEG4                  download &lt;/a&gt;



&lt;a href="https://archive.org/download/insidececot/insidececot_archive.torrent"&gt;

download                    1 file                  
&lt;/a&gt;

&lt;a href="https://archive.org/download/insidececot/insidececot_archive.torrent"&gt;
                  TORRENT                  download &lt;/a&gt;



&lt;a href="https://archive.org/compress/insidececot"&gt;
download 24 Files
            &lt;/a&gt;&lt;br/&gt;
&lt;a href="https://archive.org/compress/insidececot/formats=MPEG4,ITEM TILE,ARCHIVE BITTORRENT,METADATA"&gt;download                7 Original&lt;/a&gt;&lt;br/&gt;

&lt;a href="https://archive.org/download/insidececot"&gt;SHOW ALL&lt;/a&gt;
&lt;br/&gt;




&lt;h1&gt;IN COLLECTIONS&lt;/h1&gt;
&lt;a href="https://archive.org/details/opensource_movies"&gt;
Community Video
&lt;img src="https://archive.org/services/img/opensource_movies"/&gt;
&lt;/a&gt;



&lt;p&gt;
        Uploaded by
                  &lt;a href="https://archive.org/details/@colin_haskins"&gt;
            Colin Haskins          &lt;/a&gt;
        
                  on December 23, 2025
&lt;/p&gt;

&lt;!--/.col-md-2--&gt;
&lt;!--/.thats-right--&gt;
&lt;!--/.row--&gt;
&lt;!--//.container-ia--&gt;



&lt;h1&gt;SIMILAR ITEMS (based on metadata)&lt;/h1&gt;



&lt;!--//.container-ia--&gt;

&lt;!--/.container--&gt;

&lt;a href="https://archive.org/about/terms"&gt;Terms of Service (last updated 12/31/2014)&lt;/a&gt;

&lt;/main&gt;</content:encoded>
      <guid isPermaLink="false">https://archive.org/details/insidececot</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 00:36:27 +0000</pubDate>
    </item>
    <item>
      <title>Local AI is driving the biggest change in laptops in decades</title>
      <link>https://spectrum.ieee.org/ai-models-locally</link>
      <description>Local AI is driving the biggest change in laptops in decades</description>
      <content:encoded>&lt;article class="clearfix image-article sm-mb-1 quality-HD post-2674288969" data-category="Computing" data-frozen-sections="[]" elid="2674288969"&gt;&lt;a href="https://spectrum.ieee.org/topic/computing/"&gt;Computing&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/topic/artificial-intelligence/"&gt;AI&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/magazine/"&gt;Magazine&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/type/feature/"&gt;Feature&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/magazine/2025/december/"&gt;December 2025&lt;/a&gt;&lt;h1&gt;
        Your Laptop Isnâ€™t Ready for LLMs. Thatâ€™s About to Change
    &lt;/h1&gt;&lt;h2&gt;&lt;p&gt;Local AI is driving the biggest change in laptops in decades&lt;/p&gt;&lt;/h2&gt;&lt;a href="https://spectrum.ieee.org/u/matthew-s-smith"&gt;Matthew S. Smith&lt;/a&gt;17 Nov 202510 min read&lt;!-- EMAIL --&gt;&lt;a href="mailto:?subject=Your%20Laptop%20Isn%E2%80%99t%20Ready%20for%20LLMs.%20That%E2%80%99s%20About%20to%20Change&amp;amp;body=https://spectrum.ieee.org/ai-models-locally"&gt;&lt;/a&gt;&lt;!-- COPY LINK --&gt;&lt;!-- TWITTER --&gt;&lt;!-- FACEBOOK --&gt;&lt;!-- LINKEDIN --&gt;Vertical&lt;img alt="Hands typing on laptop with a building scaffold shaped like a face on the screen." src="https://spectrum.ieee.org/media-library/hands-typing-on-laptop-with-a-building-scaffold-shaped-like-a-face-on-the-screen.jpg?id=62173854&amp;amp;width=1200&amp;amp;height=1587"/&gt;
        Dan Page
    Blue&lt;p&gt;&lt;strong&gt;Odds are the PC in &lt;/strong&gt;your office today isnâ€™t ready to run AI &lt;a href="https://spectrum.ieee.org/large-language-model-performance"&gt;large language models&lt;/a&gt; (LLMs).&lt;/p&gt;&lt;p&gt;Today, most users interact with LLMs via an online, browser-based interface. The more technically inclined might use an application &lt;a href="https://spectrum.ieee.org/tag/programming"&gt;programming&lt;/a&gt; interface or command line interface. In either case, the queries are sent to a &lt;a href="https://spectrum.ieee.org/dcflex-data-center-flexibility"&gt;data center&lt;/a&gt;, where the model is hosted and run. It works well, until it doesnâ€™t; a data-center outage can take a model offline for hours. Plus, some users might be unwilling to send &lt;a href="https://spectrum.ieee.org/tag/personal-data"&gt;personal data&lt;/a&gt; to an anonymous entity.&lt;/p&gt;&lt;p&gt;Running a model locally on your computer could offer significant benefits: lower latency, better understanding of your personal needs, and the privacy that comes with keeping your data on your own machine.&lt;/p&gt;&lt;p&gt;However, for the average laptop thatâ€™s over a year old, the number of useful &lt;a href="https://spectrum.ieee.org/tag/ai-models"&gt;AI models&lt;/a&gt; you can run locally on your PC is close to zero. This laptop might have a four- to eight-core processor (&lt;a href="https://en.wikipedia.org/wiki/Central_processing_unit"&gt;CPU&lt;/a&gt;), no dedicated &lt;a href="https://spectrum.ieee.org/tag/graphics"&gt;graphics&lt;/a&gt; chip (&lt;a href="https://en.wikipedia.org/wiki/Graphics_processing_unit"&gt;GPU&lt;/a&gt;) or neural-processing unit (&lt;a href="https://en.wikipedia.org/wiki/Neural_processing_unit"&gt;NPU&lt;/a&gt;), and 16 gigabytes of &lt;a href="https://en.wikipedia.org/wiki/Random-access_memory"&gt;RAM&lt;/a&gt;, leaving it underpowered for LLMs.&lt;/p&gt;&lt;p&gt;Even new, high-end PC &lt;a href="https://spectrum.ieee.org/tag/laptops"&gt;laptops&lt;/a&gt;, which often include an NPU and a GPU, can struggle. The largest AI models have over a trillion parameters, which requires memory in &lt;a href="https://snowkylin.github.io/blogs/a-note-on-deepseek-r1.html#:~:text=Note-,Models,Studio%20(%245.6k)!"&gt;the hundreds of gigabytes&lt;/a&gt;. Smaller versions of these models are available, even prolific, but they often lack the intelligence of larger models, which only dedicated AI &lt;a href="https://spectrum.ieee.org/tag/data-centers"&gt;data centers&lt;/a&gt; can handle.&lt;/p&gt;&lt;p&gt;The situation is even worse when other AI features aimed at making the model more capable are considered. &lt;a href="https://huggingface.co/blog/jjokah/small-language-model"&gt;Small language models (SLMs)&lt;/a&gt; that run on local hardware either scale back these features or omit them entirely. Image and video generation are difficult to run locally on laptops, too, and until recently they were reserved for high-end tower desktop PCs.&lt;/p&gt;&lt;p&gt;Thatâ€™s a problem for AI adoption.&lt;/p&gt;&lt;p&gt;To make running AI models locally possible, the hardware found inside laptops and the software that runs on it will need an upgrade. This is the beginning of a shift in laptop design that will give engineers the opportunity to abandon the last vestiges of the past and reinvent the PC from the ground up.&lt;/p&gt;&lt;h2&gt;NPUs enter the chat&lt;/h2&gt;&lt;p&gt;The most obvious way to boost a PCâ€™s AI performance is to place a powerful NPU alongside the CPU.&lt;/p&gt;&lt;p&gt;An NPU is a specialized chip &lt;a href="https://penntoday.upenn.edu/what-is-an-NPU-in-computing"&gt;designed for the matrix multiplication calculations&lt;/a&gt; that most AI models rely on. These matrix operations are highly parallelized, which is why &lt;a href="https://spectrum.ieee.org/tag/gpus"&gt;GPUs&lt;/a&gt; (which were already better at highly parallelized tasks than CPUs) became the go-to option for AI data centers.&lt;/p&gt;&lt;p&gt;However, because NPUs are designed specifically to handle these matrix operationsâ€”and not other tasks, like 3D graphicsâ€”&lt;a href="https://www.ibm.com/think/topics/npu-vs-gpu"&gt;theyâ€™re more power efficient than GPUs&lt;/a&gt;. Thatâ€™s important for accelerating AI on portable consumer technology. NPUs also tend to provide better support for low-precision arithmetic than laptop GPUs. AI models often use low-precision arithmetic to reduce computational and memory needs on portable hardware, such as laptops.&lt;/p&gt;&lt;p&gt;&lt;h3&gt;
            
                Laptops Are Being Rebuilt to Run LLMs
            
            
        &lt;/h3&gt;&lt;img alt="Open laptop showing numbered internal components and hands at edges, with a screwdriver nearby." src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%202120%201414'%3E%3C/svg%3E"/&gt;&lt;p&gt;Your laptop today is probably not equipped to run large language models. But future laptops might. Chasing the dream of locally run LLMs, laptop architects are rethinking many aspects of current designs, leading to changes that are only now starting to take hold. &lt;/p&gt;&lt;p&gt;iStockphoto&lt;/p&gt;&lt;p&gt;1. &lt;strong&gt;Addition of NPUs.&lt;/strong&gt; Neural processing units (NPUs)â€”specialized accelerator chips that can run large language models (LLMs) and other AI models faster than CPUs and GPUs canâ€”are being incorporated into laptops.&lt;/p&gt;&lt;p&gt;2. &lt;strong&gt;Addition of moreâ€”and fasterâ€”memory.&lt;/strong&gt; The largest language models take up hundreds of gigabytes of memory. To host these models, and serve them quickly to the number-crunching processing units, laptops are increasing their memory capacity and speed.&lt;/p&gt;&lt;p&gt;3. &lt;strong&gt;Consolidation of memory.&lt;/strong&gt; Most laptops today have a divided memory architecture, with a separate pool of memory to serve the GPUs. This made sense when the design first came out: GPUs needed faster memory access than could be supplied by the common bus. Now, to feed AIâ€™s data appetite, laptop architects are rethinking this decision, and now theyâ€™re pooling memory together with faster &lt;a href="https://spectrum.ieee.org/tag/interconnects"&gt;interconnects&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;4. &lt;strong&gt;Combination of chips on the same silicon.&lt;/strong&gt; To help shorten the path to pooled memory, all the processing unitsâ€”CPUs, GPUs, and NPUsâ€”are now being integrated into the same silicon chip. This helps them connect to one another and to memory, but it will make maintenance more challenging.&lt;/p&gt;&lt;p&gt;5. &lt;strong&gt;&lt;a href="https://spectrum.ieee.org/tag/power-management"&gt;Power management&lt;/a&gt;.&lt;/strong&gt; AI models can see heavy use when they power always-on features like Microsoftâ€™s Windows Recall, or the AI-powered Windows Search. Power-sipping NPUs help laptops run these models without excessive battery drain.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;â€œWith the NPU, the entire structure is really designed around the data type of tensors [a multidimensional array of numbers],â€ said &lt;a href="https://www.microsoft.com/applied-sciences/people/steven-bathiche"&gt;Steven Bathiche&lt;/a&gt;, technical fellow at &lt;a href="https://spectrum.ieee.org/tag/microsoft"&gt;Microsoft&lt;/a&gt;. â€œNPUs are much more specialized for that workload. And so we go from a CPU that can handle three [trillion] operations per second (TOPS), to an NPUâ€ in &lt;a href="https://www.qualcomm.com"&gt;Qualcommâ€™s&lt;/a&gt; &lt;a href="https://spectrum.ieee.org/tag/snapdragon"&gt;Snapdragon&lt;/a&gt; X chip, which can power &lt;a href="https://www.microsoft.com/en-us/"&gt;Microsoftâ€™s&lt;/a&gt; &lt;a href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/"&gt;Copilot+&lt;/a&gt; features. This includes &lt;a href="https://support.microsoft.com/en-us/windows/retrace-your-steps-with-recall-aa03f8a0-a78b-4b3e-b0a1-2eb8ac48701c"&gt;Windows Recall&lt;/a&gt;, which uses AI to create a searchable timeline of a userâ€™s usage history by analyzing screenshots, and &lt;a href="https://blogs.windows.com/windows-insider/2024/02/22/windows-photos-gets-generative-erase-and-recent-ai-editing-features-now-available-on-arm64-devices-and-windows-10/"&gt;Windows Photosâ€™ Generative erase&lt;/a&gt;, which can remove the background or specific objects from an image.&lt;/p&gt;&lt;p&gt;While &lt;a href="https://qualcomm.com"&gt;Qualcomm&lt;/a&gt; was arguably the first to provide an NPU for Windows laptops, it kickstarted an NPU TOPS arms race that also includes &lt;a href="https://www.amd.com/en.html"&gt;AMD&lt;/a&gt; and &lt;a href="https://www.intel.com/content/www/us/en/homepage.html"&gt;Intel&lt;/a&gt;, and the competition is already pushing NPU performance upward.&lt;/p&gt;&lt;p&gt;In 2023, prior to Qualcommâ€™s Snapdragon X, &lt;a href="https://spectrum.ieee.org/tag/amd"&gt;AMD&lt;/a&gt; chips with NPUs were uncommon, and those that existed delivered about 10 TOPS. Today, AMD and &lt;a href="https://spectrum.ieee.org/tag/intel"&gt;Intel&lt;/a&gt; have NPUs that are competitive with Snapdragon, &lt;a href="https://www.pcworld.com/article/2806864/intel-vs-amd-vs-qualcomm-which-copilot-pc-cpu-is-best-for-you.html"&gt;providing 40 to 50 TOPS&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.lifewire.com/dell-pro-max-plus-ai-laptop-11739880"&gt;Dellâ€™s upcoming Pro Max Plus AI PC&lt;/a&gt; will up the ante with a &lt;a href="https://spectrum.ieee.org/tag/qualcomm"&gt;Qualcomm&lt;/a&gt; AI 100 NPU that promises up to 350 TOPS, improving performance by a staggering 35 times compared with that of the best available NPUs just a few years ago. Drawing that line up and to the right implies that NPUs capable of thousands of TOPS are just a couple of years away.&lt;/p&gt;&lt;p&gt;How many TOPS do you need to run state-of-the-art models with hundreds of millions of parameters? No one knows exactly. Itâ€™s not possible to run these models on todayâ€™s consumer hardware, so real-world tests just canâ€™t be done. But it stands to reason that weâ€™re within throwing distance of those capabilities. Itâ€™s also worth noting that LLMs are not the only use case for NPUs. &lt;a href="https://www.qualcomm.com/news/onq/2024/05/from-olympic-table-tennis-to-ai-product-manager-meet-dr-vinesh-sukumar"&gt;Vinesh Sukumar&lt;/a&gt;, Qualcommâ€™s head of AI and &lt;a href="https://spectrum.ieee.org/tag/machine-learning"&gt;machine learning&lt;/a&gt; product management, says &lt;a href="https://spectrum.ieee.org/tag/ai-image-generation"&gt;AI image generation&lt;/a&gt; and manipulation is an example of a task thatâ€™s difficult without an NPU or high-end GPU.&lt;/p&gt;&lt;h2&gt;Building balanced chips for better AI&lt;/h2&gt;&lt;p&gt;Faster NPUs will handle more tokens per second, which in turn will deliver a faster, more fluid experience when using AI models. Yet thereâ€™s more to running AI on local hardware than throwing a bigger, better NPU at the problem.&lt;/p&gt;&lt;p&gt;&lt;a href="https://ieeexplore.ieee.org/author/37089001134"&gt;Mike Clark&lt;/a&gt;, corporate fellow design engineer at AMD, says that companies that design chips to accelerate AI on the PC canâ€™t put all their bets on the NPU. Thatâ€™s in part because AI isnâ€™t a replacement for, but rather an addition to, the tasks a PC is expected to handle.&lt;/p&gt;&lt;p&gt;â€œWe must be good at low latency, at handling smaller data types, at branching codeâ€”traditional workloads. We canâ€™t give that up, but we still want to be good at AI,â€ says Clark. He also noted that â€œthe CPU is used to prepare dataâ€ for AI workloads, which means an inadequate CPU could become a bottleneck.&lt;/p&gt;&lt;p&gt;NPUs must also compete or cooperate with GPUs. On the PC, that often means a high-end AMD or &lt;a href="https://spectrum.ieee.org/tag/nvidia"&gt;Nvidia&lt;/a&gt; GPU with large amounts of built-in memory. The &lt;a href="https://signal65.com/research/ai/nvidia-geforce-rtx-5090-founders-edition-performance-analysis/"&gt;Nvidia GeForce RTX 5090&lt;/a&gt;â€™s specifications quote an AI performance up to 3,352 TOPS, which leaves even the Qualcomm AI 100 in the dust.&lt;/p&gt;&lt;p&gt;That comes with a big caveat, however: power. Though extremely capable, the RTX 5090 is designed to draw up to &lt;a href="https://signal65.com/research/ai/nvidia-geforce-rtx-5090-founders-edition-performance-analysis/"&gt;575 watts&lt;/a&gt; on its own. Mobile versions for laptops are more miserly but still draw up to &lt;a href="https://www.notebookcheck.net/Nvidia-GeForce-RTX-5090-Laptop-Benchmarks-and-Specs.934947.0.html"&gt;175 W&lt;/a&gt;, which can quickly drain a laptop battery.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.linkedin.com/in/simonng39/?originalSubdomain=ca"&gt;Simon Ng&lt;/a&gt;, client AI product manager at Intel, says the company is â€œseeing that the NPU will just do things much more efficiently at lower power.â€&lt;a href="https://www.linkedin.com/in/rakesh-s-anigundi/"&gt; Rakesh Anigundi&lt;/a&gt;, AMDâ€™s director of product management for Ryzen AI, agrees. He adds that &lt;a href="https://spectrum.ieee.org/tag/low-power"&gt;low-power&lt;/a&gt; operation is particularly important because AI workloads tend to take longer to run than other demanding tasks, like encoding a video or rendering graphics. â€œYouâ€™ll want to be running this for a longer period of time, such as an AI personal assistant, which could be always active and listening for your command,â€ he says.&lt;/p&gt;&lt;p&gt;These competing priorities mean chip architects and system designers will need to make tough calls about how to allocate silicon and power in AI PCs, especially those that often rely on battery power, such as laptops.&lt;/p&gt;&lt;p&gt;â€œWe have to be very deliberate in how we design our &lt;a href="https://spectrum.ieee.org/tag/system-on-a-chip"&gt;system-on-a-chip&lt;/a&gt; to ensure that a larger &lt;a href="https://spectrum.ieee.org/tag/soc"&gt;SoC&lt;/a&gt; can perform to our requirements in a thin and light form factor,â€ said&lt;a href="https://www.linkedin.com/in/mahesh-subramony-a0ba60/"&gt; Mahesh Subramony&lt;/a&gt;, senior fellow design engineer at AMD.&lt;/p&gt;&lt;h2&gt;When it comes to AI, memory matters&lt;/h2&gt;&lt;p&gt;Squeezing an NPU alongside a CPU and GPU will improve the average PCâ€™s performance in AI tasks, but itâ€™s not the only revolutionary change AI will force on PC architecture. Thereâ€™s another thatâ€™s perhaps even more fundamental: memory.&lt;/p&gt;&lt;p&gt;Most modern PCs have a divided memory architecture &lt;a href="https://www.electronicdesign.com/technologies/embedded/article/55300900/jon-peddie-research-what-came-before-pcie-the-evolution-of-pc-graphics-buses"&gt;rooted in decisions made over 25 years ago&lt;/a&gt;. Limitations in bus bandwidth led GPUs (and other add-in cards that might require high-bandwidth memory) to move away from accessing a PCâ€™s system memory and instead rely on the GPUâ€™s own dedicated memory. As a result, powerful PCs typically have two pools of memory, system memory and graphics memory, which operate independently.&lt;/p&gt;&lt;p&gt;Thatâ€™s a problem for AI. Models require large amounts of memory, and the entire model must load into memory at once. The legacy PC architecture, which splits memory between the system and the GPU, is at odds with that requirement.&lt;/p&gt;&lt;p&gt;â€œWhen I have a discrete GPU, I have a separate memory subsystem hanging off it,â€ explained &lt;a href="https://www.linkedin.com/in/joseph-macri-9a288a55/"&gt;Joe Macri, &lt;/a&gt; vice president and chief technology officer at AMD. â€œWhen I want to share data between our [CPU] and GPU, Iâ€™ve got to take the data out of my memory, slide it across the PCI Express bus, put it in the GPU memory, do my processing, then move it all back.â€ Macri said this increases power draw and leads to a sluggish &lt;a href="https://spectrum.ieee.org/tag/user-experience"&gt;user experience&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The solution is a unified memory architecture that provides all system resources access to the same pool of memory over a fast, interconnected memory bus. Appleâ€™s in-house silicon is perhaps the most well-known recent example of a chip with a unified memory architecture. However, unified memory is otherwise rare in modern PCs.&lt;/p&gt;&lt;p&gt;AMD is following suit in the laptop space. The company announced a new line of APUs targeted at high-end laptops, &lt;a href="https://www.amd.com/en/products/processors/laptop/ryzen/ai-300-series/amd-ryzen-ai-max-plus-395.html"&gt;Ryzen AI Max&lt;/a&gt;, at &lt;a href="https://spectrum.ieee.org/tag/ces"&gt;CES&lt;/a&gt; (&lt;a href="https://spectrum.ieee.org/topic/consumer-electronics/"&gt;Consumer Electronics&lt;/a&gt; Show) 2025.&lt;/p&gt;&lt;p&gt;Ryzen AI Max places the companyâ€™s Ryzen CPU cores on the same silicon as Radeon-branded GPU cores, plus an NPU rated at 50 TOPS, on a single piece of silicon with a unified memory architecture. Because of this, the CPU, GPU, and NPU can all access up to a maximum of &lt;a href="https://www.amd.com/en/developer/resources/technical-articles/2025/amd-ryzen-ai-max-395--a-leap-forward-in-generative-ai-performanc.html"&gt;128 GB of system memory&lt;/a&gt;, which is shared among all three. AMD believes this strategy is ideal for memory and performance management in consumer PCs. â€œBy bringing it all under a single thermal head, the entire power envelope becomes something that we can manage,â€ said Subramony.&lt;/p&gt;&lt;p&gt;The Ryzen AI Max is already available in several laptops, including&lt;a href="https://www.pcworld.com/article/2650073/hands-on-the-hp-zbook-ultra-g1a-smashes-the-work-laptop-paradigm.html"&gt; the HP Zbook Ultra G1a&lt;/a&gt; and the &lt;a href="https://rog.asus.com/laptops/rog-flow/rog-flow-z13-2025/"&gt;Asus ROG Flow Z13&lt;/a&gt;. It also powers the&lt;a href="https://frame.work/marketplace/desktops"&gt; Framework Desktop&lt;/a&gt; and several mini desktops from less well-known brands, such as the &lt;a href="https://www.gmktec.com/products/amd-ryzen%E2%84%A2-ai-max-395-evo-x2-ai-mini-pc?srsltid=AfmBOooME4uCrsnIh5mOf98eGHteIzsi-DAPl6E5xhNrTzG94qr3Tjf6"&gt;GMKtec EVO-X2 AI mini PC&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Intel and Nvidia will also join this party, though in an unexpected way. In September, the former rivals announced an alliance to sell chips that pair Intel CPU cores with Nvidia GPU cores. While the details are still under wraps, the chip architecture will likely include unified memory and an Intel NPU.&lt;/p&gt;&lt;p&gt;Chips like these stand to drastically change PC architecture if they catch on. Theyâ€™ll offer access to much larger pools of memory than before and integrate the CPU, GPU, and NPU into one piece of silicon that can be closely monitored and controlled. These factors should make it easier to shuffle an AI workload to the hardware best suited to execute it at a given moment.&lt;/p&gt;&lt;p&gt;Unfortunately, theyâ€™ll also make PC upgrades and repairs more difficult, as chips with a unified memory architecture typically bundle the CPU, GPU, NPU, and memory into a single, physically inseparable package on a PC mainboard. Thatâ€™s in contrast with traditional PCs, where the CPU, GPU, and memory can be replaced individually.&lt;/p&gt;&lt;h2&gt;Microsoftâ€™s bullish take on AI is rewriting Windows&lt;/h2&gt;&lt;p&gt;MacOS is well regarded for its attractive, intuitive &lt;a href="https://spectrum.ieee.org/tag/user-interface"&gt;user interface&lt;/a&gt;, and Apple Silicon chips have a unified memory architecture that can prove useful for AI. However, Appleâ€™s GPUs arenâ€™t as capable as the best ones used in PCs, and its AI tools for developers are less widely adopted.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.linkedin.com/in/chrissiecremers/?originalSubdomain=nl"&gt;Chrissie Cremers&lt;/a&gt;, cofounder of the AI-focused marketing firm Aigency Amsterdam, told me earlier this year that although she prefers macOS, her agency doesnâ€™t use Mac computers for AI work. â€œThe GPU in my Mac desktop can hardly manage [our AI workflow], and itâ€™s not an old computer,â€ she said. â€œIâ€™d love for them to catch up here, because they used to be the creative tool.â€&lt;/p&gt;&lt;p&gt; &lt;img alt="Laptop beneath glass dome shaped like human head on striped orange and blue background." src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201200%201200'%3E%3C/svg%3E"/&gt;  Dan Page&lt;/p&gt;&lt;p&gt;That leaves an opening for competitors to become the go-to choice for AI on the PCâ€”and Microsoft knows it.&lt;/p&gt;&lt;p&gt;Microsoft launched &lt;a href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/"&gt;Copilot+ PCs&lt;/a&gt; at the companyâ€™s 2024 Build developer conference. The launch had problems, most notably the &lt;a href="https://www.theverge.com/2024/6/13/24178144/microsoft-windows-ai-recall-feature-delay"&gt;botched&lt;/a&gt; release of its key feature,&lt;a href="https://spectrum.ieee.org/microsoft-copilot"&gt; Windows Recall&lt;/a&gt;, which uses AI to help users search through anything theyâ€™ve seen or heard on their PC. Still, the launch was successful in pushing the PC industry toward NPUs, as AMD and Intel both introduced new laptop chips with upgraded NPUs in late 2024.&lt;/p&gt;&lt;p&gt;At Build 2025, Microsoft also revealed &lt;a href="https://devblogs.microsoft.com/foundry/foundry-local-a-new-era-of-edge-ai/"&gt;Windowsâ€™ AI Foundry Local&lt;/a&gt;, a â€œruntime stackâ€ that includes a catalog of popular open-source &lt;a href="https://spectrum.ieee.org/tag/large-language-models"&gt;large language models&lt;/a&gt;. While Microsoftâ€™s own models are available,&lt;a href="https://azure.microsoft.com/en-us/products/ai-model-catalog#tabs-pill-bar-oc92d8_tab0"&gt; the catalog includes thousands of open-source models&lt;/a&gt; from &lt;a href="https://spectrum.ieee.org/tag/alibaba"&gt;Alibaba&lt;/a&gt;, DeepSeek, &lt;a href="https://spectrum.ieee.org/tag/meta"&gt;Meta&lt;/a&gt;, Mistral AI, Nvidia, &lt;a href="https://spectrum.ieee.org/tag/openai"&gt;OpenAI&lt;/a&gt;, Stability AI, xAI, and more.&lt;/p&gt;&lt;p&gt;Once a model is selected and implemented into an app, Windows executes AI tasks on local hardware through the Windows ML runtime, which automatically directs AI tasks to the CPU, GPU, or NPU hardware best suited for the job.&lt;/p&gt;&lt;p&gt;AI &lt;a href="https://spectrum.ieee.org/tag/foundry"&gt;Foundry&lt;/a&gt; also provides APIs for local knowledge retrieval and low-rank adaptation (LoRA), advanced features that let developers customize the data an AI model can reference and how it responds. Microsoft also announced support for on-device semantic search and retrieval-augmented generation, features that help developers build AI tools that reference specific on-device information.&lt;/p&gt;&lt;p&gt;â€œ[AI Foundry] is about being smart. Itâ€™s about using all the &lt;a href="https://spectrum.ieee.org/tag/processors"&gt;processors&lt;/a&gt; at hand, being efficient, and prioritizing workloads across the CPU, the NPU, and so on. Thereâ€™s a lot of opportunity and runway to improve,â€ said Bathiche.&lt;/p&gt;&lt;h3&gt;Toward &lt;a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence"&gt;AGI&lt;/a&gt; on PCs&lt;/h3&gt;&lt;p&gt;The rapid evolution of AI-capable PC hardware represents more than just an incremental upgrade. It signals a coming shift in the PC industry thatâ€™s likely to wipe away the last vestiges of the PC architectures designed in the â€™80s, â€™90s, and early 2000s.&lt;/p&gt;&lt;p&gt;The combination of increasingly powerful NPUs, unified memory architectures, and sophisticated software-optimization techniques is closing the performance gap between local and cloud-based AI at a pace that has surprised even industry insiders, such as Bathiche.&lt;/p&gt;&lt;p&gt;It will also nudge chip designers toward ever-more-integrated chips that have a unified memory subsystem and to bring the CPU, GPU, and NPU onto a single piece of siliconâ€”even in high-end laptops and desktops. AMDâ€™s Subramony said the goal is to have users â€œcarrying a mini workstation in your hand, whether itâ€™s for AI workloads, or for high compute. You wonâ€™t have to go to the cloud.â€&lt;/p&gt;&lt;p&gt;A change that massive wonâ€™t happen overnight. Still, itâ€™s clear that many in the PC industry are committed to reinventing the computers we use every day in a way that optimizes for AI. Qualcommâ€™s Vinesh Sukumar even believes affordable consumer laptops, much like data centers, should aim for &lt;a href="https://spectrum.ieee.org/tag/agi"&gt;AGI&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;â€œI want a complete &lt;a href="https://spectrum.ieee.org/tag/artificial-general-intelligence"&gt;artificial general intelligence&lt;/a&gt; running on Qualcomm devices,â€ he said. â€œThatâ€™s what weâ€™re trying to push for.â€ &lt;/p&gt;&lt;p&gt;&lt;em&gt;This article appears in the December 2025 print issue.&lt;/em&gt;&lt;/p&gt;From Your Site Articles&lt;ul&gt;&lt;li&gt;&lt;a href="https://spectrum.ieee.org/personal-ai-assistant"&gt;When AI Unplugs, All Bets Are Off â€º&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://spectrum.ieee.org/agentic-ai-opera-mini"&gt;Opera Includes AI Agents in Latest Web Browser â€º&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;Related Articles Around the Web&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/16y95hk/a_starter_guide_for_playing_with_your_own_local_ai/"&gt;A Starter Guide for Playing with Your Own Local AI! : r/LocalLLaMA â€º&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://medium.com/@springrod/why-you-should-use-local-models-a3fce1124c94"&gt;Why You Should Use Local Models. When building Gen AI ... â€º&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;a href="https://spectrum.ieee.org/tag/large-language-models"&gt;large language models&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/laptops"&gt;laptops&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/amd"&gt;amd&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/apple"&gt;apple&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/microsoft"&gt;microsoft&lt;/a&gt;&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://spectrum.ieee.org/ai-models-locally</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 00:12:16 +0000</pubDate>
    </item>
    <item>
      <title>It's Always TCP_NODELAY</title>
      <link>https://brooker.co.za/blog/2024/05/09/nagle.html</link>
      <description>It's not the 1980s anymore, thankfully.</description>
      <content:encoded>&lt;body&gt;


&lt;h1&gt;&lt;a href="https://brooker.co.za/blog/"&gt;Marc's Blog&lt;/a&gt;&lt;/h1&gt;


&lt;h1&gt;About Me&lt;/h1&gt;
    My name is Marc Brooker. I've been writing code, reading code, and living vicariously through computers for as long as I can remember. I like to build things that work. I also dabble in machining, welding, cooking and skiing.&lt;br/&gt;&lt;br/&gt;

    I'm currently an engineer at Amazon Web Services (AWS) in Seattle, where I work on databases, serverless, and serverless databases. Before that, I worked on EC2 and EBS.&lt;br/&gt;

All opinions are my own.
    &lt;h1&gt;Links&lt;/h1&gt;
&lt;a href="https://brooker.co.za/blog/publications.html"&gt;My Publications and Videos&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://fediscience.org/@marcbrooker"&gt;@marcbrooker on Mastodon&lt;/a&gt;
&lt;a href="https://twitter.com/MarcJBrooker"&gt;@MarcJBrooker on Twitter&lt;/a&gt;


&lt;h1&gt;ItÃ¢Â€Â™s always TCP_NODELAY. Every damn time.&lt;/h1&gt;
&lt;p&gt;It's not the 1980s anymore, thankfully.&lt;/p&gt;
&lt;p&gt;The first thing I check when debugging latency issues in distributed systems is whether &lt;a href="https://linux.die.net/man/7/tcp"&gt;TCP_NODELAY&lt;/a&gt; is enabled. And itÃ¢Â€Â™s not just me. Every distributed system builder I know has lost hours to latency issues quickly fixed by enabling this simple socket option, suggesting that the default behavior is wrong, and perhaps that the whole concept is outmoded.&lt;/p&gt;
&lt;p&gt;First, letÃ¢Â€Â™s be clear about what weÃ¢Â€Â™re talking about. ThereÃ¢Â€Â™s no better source than John NagleÃ¢Â€Â™s &lt;a href="https://datatracker.ietf.org/doc/html/rfc896"&gt;RFC896&lt;/a&gt; from 1984&lt;a href="#foot1"&gt;1&lt;/a&gt;. First, the problem statement:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There is a special problem associated with small  packets.   When TCP  is  used  for  the transmission of single-character messages originating at a keyboard, the typical result  is  that  41  byte packets (one  byte  of data, 40 bytes of header) are transmitted for each byte of useful data.  This 4000%  overhead  is  annoying but tolerable on lightly loaded networks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In short, Nagle was interested in better amortizing the cost of TCP headers, to get better throughput out of the network. Up to 40x better throughput! These tiny packets had two main causes: human-interactive applications like shells, where folks were typing a byte at a time, and poorly implemented programs that dribbled messages out to the kernel through many &lt;code&gt;write&lt;/code&gt; calls. NagleÃ¢Â€Â™s proposal for fixing this was simple and smart:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A  simple and elegant solution has been discovered.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The solution is to inhibit the sending of new TCP  segments  when new  outgoing  data  arrives  from  the  user  if  any previously transmitted data on the connection remains unacknowledged.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When many people talk about NagleÃ¢Â€Â™s algorithm, they talk about timers, but RFC896 doesnÃ¢Â€Â™t use any kind of timer other than the round-trip time on the network.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NagleÃ¢Â€Â™s Algorithm and Delayed Acks&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;NagleÃ¢Â€Â™s nice, clean, proposal interacted poorly with another TCP feature: delayed &lt;code&gt;ACK&lt;/code&gt;. The idea behind delayed &lt;code&gt;ACK&lt;/code&gt; is to delay sending the acknowledgement of a packet at least until thereÃ¢Â€Â™s some data to send back (e.g. a &lt;code&gt;telnet&lt;/code&gt; session echoing back the userÃ¢Â€Â™s typing), or until a timer expires. &lt;a href="https://datatracker.ietf.org/doc/html/rfc813"&gt;RFC813&lt;/a&gt; from 1982 is that first that seems to propose delaying &lt;code&gt;ACKs&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The receiver of data will   refrain   from   sending   an   acknowledgement   under   certain circumstances, in which case it must set a timer which  will  cause  the acknowledgement  to be sent later.  However, the receiver should do this only where it is a reasonable guess that some other event will intervene and prevent the necessity of the timer  interrupt.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;which is then formalized further in &lt;a href="https://datatracker.ietf.org/doc/html/rfc1122"&gt;RFC1122&lt;/a&gt; from 1989. The interaction between these two features causes a problem: NagleÃ¢Â€Â™s algorithm is blocking sending more data until an &lt;code&gt;ACK&lt;/code&gt; is received, but delayed ack is delaying that &lt;code&gt;ack&lt;/code&gt; until a response is ready. Great for keeping packets full, not so great for latency-sensitive pipelined applications.&lt;/p&gt;
&lt;p&gt;This is a point Nagle has made himself several times. For example in this &lt;a href="https://news.ycombinator.com/item?id=10608356"&gt;Hacker News comment&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;That still irks me. The real problem is not tinygram prevention. ItÃ¢Â€Â™s ACK delays, and that stupid fixed timer. They both went into TCP around the same time, but independently. I did tinygram prevention (the Nagle algorithm) and Berkeley did delayed ACKs, both in the early 1980s. The combination of the two is awful.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As systems builders this is should be a familiar situation: two reasonable features of the system that interact to create an undesirable behavior. This kind of interaction is one of the things that makes protocol design so hard.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Is Nagle blameless?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, itÃ¢Â€Â™s not just delayed ACK&lt;a href="#foot2"&gt;2&lt;/a&gt;. Even without delayed ack and that &lt;em&gt;stupid fixed timer&lt;/em&gt;, the behavior of NagleÃ¢Â€Â™s algorithm probably isnÃ¢Â€Â™t what we want in distributed systems. A single in-datacenter RTT is typically around 500ÃÂ¼s, then a couple of milliseconds between datacenters in the same region, and up to hundreds of milliseconds going around the globe. Given the vast amount of work a modern server can do in even a few hundred microseconds, delaying sending data for even one RTT isnÃ¢Â€Â™t clearly a win.&lt;/p&gt;
&lt;p&gt;To make a clearer case, letÃ¢Â€Â™s turn back to the justification behind NagleÃ¢Â€Â™s algorithm: amortizing the cost of headers and avoiding that 40x overhead on single-byte packets. But does anybody send single byte packets anymore? Most distributed databases and systems donÃ¢Â€Â™t. Partially thatÃ¢Â€Â™s because they simply have more to say, partially its because of additional overhead of protocols like TLS, and partially its because of encoding and serialization overhead. But mostly, they have more to say.&lt;/p&gt;
&lt;p&gt;The core concern of not sending tiny messages is still a very real one, but weÃ¢Â€Â™ve very effectively pushed that into the application layer. Sending a byte at a time wrapped in JSON isnÃ¢Â€Â™t going to be very efficient, no matter what NagleÃ¢Â€Â™s algorithm does.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Is Nagle needed?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;First, the uncontroversial take: if youÃ¢Â€Â™re building a latency-sensitive distributed system running on modern datacenter-class hardware, enable &lt;code&gt;TCP_NODELAY&lt;/code&gt; (disable NagleÃ¢Â€Â™s algorithm) without worries. You donÃ¢Â€Â™t need to feel bad. ItÃ¢Â€Â™s not a sin. ItÃ¢Â€Â™s OK. Just go ahead.&lt;/p&gt;
&lt;p&gt;More controversially, I suspect that NagleÃ¢Â€Â™s algorithm just isnÃ¢Â€Â™t needed on modern systems, given the traffic and application mix, and the capabilities of the hardware we have today. In other words, &lt;code&gt;TCP_NODELAY&lt;/code&gt; should be the default. ThatÃ¢Â€Â™s going to make some Ã¢Â€Âœ&lt;code&gt;write&lt;/code&gt; every byteÃ¢Â€Â code slower than it would otherwise be, but those applications should be fixed anyway if we care about efficiency.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Footnotes&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a&gt;&lt;/a&gt; I wonÃ¢Â€Â™t got into it here, but RFC896 is also one of the earliest statements I can find of metastable behavior in computer networks. In it, Nagle says: Ã¢Â€ÂœThis condition is stable. Once the  saturation point has been reached, if the algorithm for selecting packets to be dropped is fair, the network will continue to operate in a degraded condition.Ã¢Â€Â&lt;/li&gt;
&lt;li&gt;&lt;a&gt;&lt;/a&gt; As this has gone around the internet, a number of folks have asked about &lt;code&gt;TCP_QUICKACK&lt;/code&gt;. I donÃ¢Â€Â™t tend to reach for it for a few reasons, including lack of portability, and weird semantics (seriously, read &lt;a href="https://linux.die.net/man/7/tcp"&gt;the man page&lt;/a&gt;). The bigger problem is that &lt;code&gt;TCP_QUICKACK&lt;/code&gt; doesnÃ¢Â€Â™t fix the fundamental problem of the kernel hanging on to data longer than my program wants it to. When I say &lt;code&gt;write()&lt;/code&gt;, I mean &lt;code&gt;write()&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;


  Â« &lt;a href="https://brooker.co.za/blog"&gt;Back to the blog index&lt;/a&gt;&lt;br/&gt;
&lt;br/&gt;
&lt;!-- Similar Posts --&gt;
&lt;h4&gt;Similar Posts&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;21 Oct 2022 Â» &lt;a href="https://brooker.co.za/blog/2022/10/21/nudge.html"&gt;Give Your Tail a Nudge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;10 May 2014 Â» &lt;a href="https://brooker.co.za/blog/2014/05/10/lynch-pub.html"&gt;The Essential Nancy Lynch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;20 May 2025 Â» &lt;a href="https://brooker.co.za/blog/2025/05/20/icpe.html"&gt;Good Performance for Bad Days&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Dissimilar Posts --&gt;
&lt;h4&gt;Something Completely Different&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;11 Apr 2015 Â» &lt;a href="https://brooker.co.za/blog/2015/04/11/zero-one.html"&gt;The Zero, One, Infinity Disease&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;
        Marc Brooker&lt;br&gt;
        The opinions on this site are my own. They do not necessarily represent those of my employer.&lt;br&gt;
        marcbrooker@gmail.com
      &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href="https://brooker.co.za/blog/rss.xml"&gt;&lt;img src="https://brooker.co.za/blog/images/feed-icon-14x14.png"/&gt; RSS&lt;/a&gt;
&lt;a href="https://brooker.co.za/blog/atom.xml"&gt;&lt;img src="https://brooker.co.za/blog/images/feed-icon-14x14.png"/&gt; Atom&lt;/a&gt;
&lt;/p&gt;


&lt;!-- &lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;&lt;img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /&gt;&lt;/a&gt;&lt;br /&gt; --&gt;
      This work is licensed under a &lt;a href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.
    


&lt;/body&gt;</content:encoded>
      <guid isPermaLink="false">https://brooker.co.za/blog/2024/05/09/nagle.html</guid>
      <category>Hacker News</category>
      <pubDate>Mon, 22 Dec 2025 21:09:59 +0000</pubDate>
    </item>
    <item>
      <title>Ultrasound Cancer Treatment: Sound Waves Fight Tumors</title>
      <link>https://spectrum.ieee.org/ultrasound-cancer-treatment</link>
      <description>HistoSonics turns its tumor-liquifying tech against pancreatic cancer</description>
      <content:encoded>&lt;article class="clearfix image-article sm-mb-1 quality-HD post-2674366292" data-category="Biomedical" data-frozen-sections="[]" elid="2674366292"&gt;&lt;a href="https://spectrum.ieee.org/topic/biomedical/"&gt;Biomedical&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/magazine/"&gt;Magazine&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/type/feature/"&gt;Feature&lt;/a&gt;&lt;h1&gt;
        Ultrasound Treatment Takes on Cancerâ€™s Toughest Tumors
    &lt;/h1&gt;&lt;h2&gt;&lt;p&gt;HistoSonics turns its tumor-liquifying tech against pancreatic cancer&lt;/p&gt;&lt;/h2&gt;&lt;a href="https://spectrum.ieee.org/u/greg-uyeno"&gt;Greg Uyeno&lt;/a&gt;22 Dec 20253 min read&lt;!-- EMAIL --&gt;&lt;a href="mailto:?subject=Ultrasound%20Treatment%20Takes%20on%20Cancer%E2%80%99s%20Toughest%20Tumors&amp;amp;body=https://spectrum.ieee.org/ultrasound-cancer-treatment"&gt;&lt;/a&gt;&lt;!-- COPY LINK --&gt;&lt;!-- TWITTER --&gt;&lt;!-- FACEBOOK --&gt;&lt;!-- LINKEDIN --&gt;&lt;img alt="Illustration of the HistoSonics device over a patientâ€™s abdomen, sending ultrasound through a water-filled membrane into the body. Three insets show a tumor as bubbles form, expand, and break the tumor apart into liquefied tissue." src="https://spectrum.ieee.org/media-library/illustration-of-the-histosonics-device-over-a-patients-abdomen-sending-ultrasound-through-a-water-filled-membrane-into-the-bod.png?id=62599195&amp;amp;width=1199&amp;amp;height=1280"/&gt;&lt;p&gt;HistoSonicsâ€™ Edison system uses a water-filled membrane to transmit focused ultrasound into the body. The resulting bubbles expand and collapse within the tumor, producing mechanical stress that destroys cancer cells and liquefies the tumor.&lt;/p&gt;Gyginfographics.com&lt;p&gt;&lt;strong&gt;For many years,&lt;/strong&gt; doctors and technicians who performed medical &lt;a href="https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes"&gt;ultrasound&lt;/a&gt; procedures viewed bubbles with wary concern. The phenomenon of cavitationâ€”the formation and collapse of tiny gas bubbles due to changes in pressureâ€”was considered an undesirable and largely uncontrollable side effect. But in 2001, researchers at the University of Michigan began exploring ways to harness the phenomenon for the destruction of cancerous tumors and other problematic tissue.&lt;/p&gt;&lt;p&gt;The trouble was, creating and controlling cavitation generated heat, which harmed healthy tissue beyond the target area. &lt;a href="https://bme.umich.edu/people/xu-zhen/"&gt;Zhen Xu&lt;/a&gt;, who was working on a Ph.D. in &lt;a href="https://spectrum.ieee.org/tag/biomedical-engineering"&gt;biomedical engineering&lt;/a&gt; at the time, was bombarding pig heart tissue in a tank of water with &lt;a href="https://spectrum.ieee.org/tag/ultrasound"&gt;ultrasound&lt;/a&gt; when she made a breakthrough.&lt;/p&gt;&lt;p&gt;The key was using extremely powerful ultrasound to produce negative pressure of more than 20 megapascals, delivered in short bursts measured in microsecondsâ€”but separated by relatively long gaps, between a millisecond and a full second long. These parameters created bubbles that quickly formed and collapsed, tearing apart nearby cells and turning the tissue into a kind of slurry, while avoiding heat buildup. The result was a form of incisionless &lt;a href="https://spectrum.ieee.org/tag/surgery"&gt;surgery&lt;/a&gt;, a way to wipe out tumors without scalpels, &lt;a href="https://spectrum.ieee.org/tag/radiation"&gt;radiation&lt;/a&gt;, or heat.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;â€œThe experiments worked,â€ says Xu, now a professor at Michigan, â€œbut I also destroyed the ultrasound equipment that I used,â€ which was the most powerful available at the time. In 2009, she cofounded a company, &lt;a href="https://histosonics.com/"&gt;HistoSonics&lt;/a&gt;, to commercialize more powerful ultrasound machines, test treatment of a variety of diseases, and make the procedure, called histotripsy, widely available.&lt;/p&gt;&lt;p&gt;So far, the killer app is fighting &lt;a href="https://spectrum.ieee.org/tag/cancer"&gt;cancer&lt;/a&gt;. In 2023, HistoSonicsâ€™ Edison system received &lt;a href="https://spectrum.ieee.org/tag/fda"&gt;FDA&lt;/a&gt; approval for &lt;a href="https://histosonics.com/news/fda-awards-histosonics-clearance-of-its-first-of-a-kind-edison-histotripsy-system-2/"&gt;treatment of liver tumors&lt;/a&gt;. In 2026, clinicians will conclude a &lt;a href="https://histosonics.com/news/worlds-first-kidney-tumor-treated-using-the-histosonics-edison-histotripsy-system/"&gt;pivotal kidney cancer study&lt;/a&gt; and apply for regulatory approval. Theyâ€™ll also launch a large-scale pivotal trial for pancreatic &lt;a href="https://spectrum.ieee.org/tag/cancer"&gt;cancer&lt;/a&gt;, considered one of the deadliest forms of the disease with a five-year survival rate of just &lt;a href="https://pancan.org/press-releases/pancreatic-cancer-diagnoses-and-mortality-rates-climb-five-year-survival-rate-for-pancreatic-cancer-stalls-at-13/"&gt;13 percent&lt;/a&gt;. An effective treatment for pancreatic cancer would represent a major advance against one of the most lethal malignancies.&lt;/p&gt;&lt;h2&gt;Histotripsyâ€™s Benefits for Cancer Treatment&lt;/h2&gt;&lt;p&gt;HistoSonics is not the only developer of histotripsy devices or techniques, but it is first to market with a purpose-built device. â€œWhat HistoSonics has developed is a symphony of technologies, which combines physics, biology, and biomedical engineering,â€ says &lt;a href="https://www.cc.nih.gov/meet-our-doctors/bwood"&gt;Bradford Wood&lt;/a&gt;, an interventional radiologist at the National Institutes of Health, who is not affiliated with the company. Its engineering effort has spanned multiple disciplines to produce robotic, computer-guided systems that turn physical forces into therapeutic effects.&lt;/p&gt;&lt;p&gt;Over the past decade, research has confirmed or found other benefits of histotripsy. With precise calibration, fibrous tissueâ€”such as blood vesselsâ€”can be spared from damage even in the target zone. And while other noninvasive techniques may leave scar tissue, the liquefied debris created by histotripsy is cleared away by the bodyâ€™s natural processes.&lt;/p&gt;&lt;p&gt;In HistoSonicsâ€™ early trials for pancreatic cancer, doctors used &lt;a href="https://spectrum.ieee.org/tag/focused-ultrasound"&gt;focused ultrasound&lt;/a&gt; pulses to ablate, or destroy, tumors deep within the pancreas. â€œItâ€™s a great achievement for the entire field to show that it is possible to ablate pancreatic tumors and that itâ€™s well tolerated,â€ says &lt;a href="https://gastro.uw.edu/people/faculty/khokhlova-t"&gt;Tatiana Khokhlova&lt;/a&gt;, a medical ultrasound researcher at the University of Washington, in Seattle, who has worked on alternative histotripsy techniques.&lt;/p&gt;&lt;p&gt;Khokhlova says the key to harnessing histotripsyâ€™s benefits â€œwill be combining ablation of the primary tumor in the pancreas with some other therapy.â€ Combination treatment could fight recurrent cancer and tiny tumors that ultrasound might miss, while also tapping into a surprising benefit.&lt;/p&gt;&lt;p&gt;Histotripsy generally seems to &lt;a href="https://pubmed.ncbi.nlm.nih.gov/31940590/"&gt;stimulate an immune response&lt;/a&gt;, helping the body attack cancer cells that werenâ€™t targeted directly by ultrasound. The mechanical destruction of tumors likely leaves behind recognizable traces of cancer proteins that help the immune system learn to identify and destroy similar cells elsewhere in the body, explains Wood. Researchers are now exploring ways to pair histotripsy with immunotherapy to amplify that effect.&lt;/p&gt;&lt;p&gt;The companyâ€™s capacity to explore the treatmentâ€˜s potential for different conditions will only improve with time, says HistoSonics CEO &lt;a href="https://www.linkedin.com/in/mike-blue-860b9522/"&gt;Mike Blue&lt;/a&gt;. The company has fresh resources to accelerate R&amp;amp;D: A new ownership group, which includes billionaire &lt;a href="https://spectrum.ieee.org/tag/jeff-bezos"&gt;Jeff Bezos&lt;/a&gt;, &lt;a href="https://www.businesswire.com/news/home/20250807749442/en/HistoSonics-Announces-%242.25B-Acquisition-by-Consortium-of-Top-Tier-Investors"&gt;acquired&lt;/a&gt; HistoSonics in August 2025 at a valuation of US $2.25 billion. &lt;/p&gt;&lt;p&gt;Engineers are already testing a new guidance system that uses a form of &lt;a href="https://spectrum.ieee.org/tag/x-rays"&gt;X-rays&lt;/a&gt; rather than &lt;a href="https://spectrum.ieee.org/tag/ultrasound-imaging"&gt;ultrasound imaging&lt;/a&gt;, which should expand use cases. The R&amp;amp;D team is also developing a feedback system that analyzes echoes from the therapeutic ultrasound to detect tissue destruction and integrates that information into the live display, says Blue.&lt;/p&gt;&lt;p&gt;If those advances pan out, histotripsy could move well beyond the liver, kidney, and pancreas in the fight against cancer. What started as a curiosity about bubbles might soon become a new pillar of noninvasive medicineâ€”a future in which surgeons wield not scalpels, but sound waves.&lt;/p&gt;From Your Site Articles&lt;ul&gt;&lt;li&gt;&lt;a href="https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes"&gt;Doctors Could Hack the Nervous System With Ultrasound â€º&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://spectrum.ieee.org/cancer-ultrasound-blood-brain-barrier"&gt;Ultrasound Opens New Front Against Lethal Brain Cancer in Kids â€º&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;Related Articles Around the Web&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.uchicagomedicine.org/cancer/types-treatments/histotripsy"&gt;Histotripsy - Liver Cancer Ultrasound - UChicago Medicine â€º&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://news.umich.edu/tumor-destroying-sound-waves-receive-fda-approval-for-liver-treatment-in-humans/"&gt;Tumor-destroying sound waves receive FDA approval for liver ... â€º&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;a href="https://spectrum.ieee.org/tag/cancer"&gt;cancer&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/cancer-therapy"&gt;cancer therapy&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/focused-ultrasound"&gt;focused ultrasound&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/immune-system"&gt;immune system&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/pancreas"&gt;pancreas&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/ultrasound"&gt;ultrasound&lt;/a&gt;&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://spectrum.ieee.org/ultrasound-cancer-treatment</guid>
      <category>Hacker News</category>
      <pubDate>Mon, 22 Dec 2025 19:37:34 +0000</pubDate>
    </item>
    <item>
      <title>The Illustrated Transformer</title>
      <link>https://jalammar.github.io/illustrated-transformer/</link>
      <description>Discussions: Hacker News (65 points, 4 comments) , Reddit r/MachineLearning (29 points, 3 comments) Translations: Arabic , Chinese (Simplified) 1 , Chinese (Simplified) 2 , French 1 , French 2 , Italian , Japanese , Korean , Persian , Russian , Spanish 1 , Spanish 2 , Vietnamese Watch: MITâ€™s Deep Learning State of the Art lecture referencing this post Featured in courses at Stanford , Harvard , MIT , Princeton , CMU and others</description>
      <content:encoded>&lt;article class="post"&gt;
&lt;h1&gt;The Illustrated Transformer&lt;/h1&gt;

&lt;p&gt;Discussions:
&lt;a href="https://news.ycombinator.com/item?id=18351674"&gt;Hacker News (65 points, 4 comments)&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/MachineLearning/comments/8uh2yz/p_the_illustrated_transformer_a_visual_look_at/"&gt;Reddit r/MachineLearning (29 points, 3 comments)&lt;/a&gt;

&lt;br/&gt;
Translations: &lt;a href="https://www.mundhor.site/post/post14"&gt;Arabic&lt;/a&gt;, &lt;a href="https://blog.csdn.net/yujianmin1990/article/details/85221271"&gt;Chinese (Simplified) 1&lt;/a&gt;, &lt;a href="https://blog.csdn.net/qq_36667170/article/details/124359818"&gt;Chinese (Simplified) 2&lt;/a&gt;, &lt;a href="https://a-coles.github.io/2020/11/15/transformer-illustre.html"&gt;French 1&lt;/a&gt;, &lt;a href="https://lbourdois.github.io/blog/nlp/Transformer/"&gt;French 2&lt;/a&gt;, &lt;a href="https://medium.com/@val.mannucci/il-transformer-illustrato-it-37a78e3e2348"&gt;Italian&lt;/a&gt;, &lt;a href="https://tips-memo.com/translation-jayalmmar-transformer"&gt;Japanese&lt;/a&gt;, &lt;a href="https://nlpinkorean.github.io/illustrated-transformer/"&gt;Korean&lt;/a&gt;, &lt;a href="http://dml.qom.ac.ir/2022/05/17/illustrated-transformer/"&gt;Persian&lt;/a&gt;, &lt;a href="https://habr.com/ru/post/486358/"&gt;Russian&lt;/a&gt;, &lt;a href="https://www.ibidemgroup.com/edu/transformer-ilustrado-jay-alammar/"&gt;Spanish 1&lt;/a&gt;, &lt;a href="https://hackernoon.com/el-transformador-ilustrado-una-traduccion-al-espanol-0y73wwp"&gt;Spanish 2&lt;/a&gt;, &lt;a href="https://trituenhantao.io/tin-tuc/minh-hoa-transformer/"&gt;Vietnamese&lt;/a&gt;
&lt;br/&gt;
Watch: MITâ€™s &lt;a href="https://youtu.be/53YvP6gdD7U?t=432"&gt;Deep Learning State of the Art&lt;/a&gt; lecture referencing this post
&lt;br/&gt;
Featured in courses at &lt;a href="https://web.stanford.edu/class/cs224n/"&gt;Stanford&lt;/a&gt;, &lt;a href="https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/transformers"&gt;Harvard&lt;/a&gt;, &lt;a href="https://ocw.mit.edu/courses/6-s897-machine-learning-for-healthcare-spring-2019/d39a6eed387ee90b1f72a01949c35c7b_MIT6_S897S19_lec8.pdf"&gt;MIT&lt;/a&gt;, &lt;a href="https://www.cs.princeton.edu/courses/archive/fall22/cos597G/"&gt;Princeton&lt;/a&gt;, &lt;a href="https://www.cs.cmu.edu/~leili/course/mldl22w/14-Transformer.pdf"&gt;CMU&lt;/a&gt; and others&lt;/p&gt;



&lt;a href="https://www.llm-book.com"&gt;&lt;img src="https://github.com/user-attachments/assets/a471dfff-00cc-4cb4-8df5-123e195bcc71"/&gt;&lt;/a&gt;

Update: This post has now become a book! Check out &lt;a href="https://llm-book.com"&gt;LLM-book.com&lt;/a&gt; which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).
  


&lt;p&gt;In the &lt;a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"&gt;previous post, we looked at Attention&lt;/a&gt; â€“ a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at &lt;strong&gt;The Transformer&lt;/strong&gt; â€“ a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloudâ€™s recommendation to use The Transformer as a reference model to use their &lt;a href="https://cloud.google.com/tpu/"&gt;Cloud TPU&lt;/a&gt; offering. So letâ€™s try to break the model apart and look at how it functions.&lt;/p&gt;
&lt;p&gt;The Transformer was proposed in the paper &lt;a href="https://arxiv.org/abs/1706.03762"&gt;Attention is All You Need&lt;/a&gt;. A TensorFlow implementation of it is available as a part of the &lt;a href="https://github.com/tensorflow/tensor2tensor"&gt;Tensor2Tensor&lt;/a&gt; package. Harvardâ€™s NLP group created a &lt;a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html"&gt;guide annotating the paper with PyTorch implementation&lt;/a&gt;. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2025 Update&lt;/strong&gt;: Weâ€™ve built a &lt;a href="https://bit.ly/4aRnn7Z"&gt;free short course&lt;/a&gt; that brings the contents of this post up-to-date with animations:&lt;/p&gt;



&lt;h2&gt;A High-Level Look&lt;/h2&gt;
&lt;p&gt;Letâ€™s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/the_transformer_3.png"/&gt;

&lt;!--more--&gt;
&lt;p&gt;Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png"/&gt;

&lt;p&gt;The encoding component is a stack of encoders (the paper stacks six of them on top of each other â€“ thereâ€™s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"/&gt;

&lt;p&gt;The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/Transformer_encoder.png"/&gt;

&lt;p&gt;The encoderâ€™s inputs first flow through a self-attention layer â€“ a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. Weâ€™ll look closer at self-attention later in the post.&lt;/p&gt;
&lt;p&gt;The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.&lt;/p&gt;
&lt;p&gt;The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in &lt;a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"&gt;seq2seq models&lt;/a&gt;).&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/Transformer_decoder.png"/&gt;

&lt;h2&gt;Bringing The Tensors Into The Picture&lt;/h2&gt;
&lt;p&gt;Now that weâ€™ve seen the major components of the model, letâ€™s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.&lt;/p&gt;
&lt;p&gt;As is the case in NLP applications in general, we begin by turning each input word into a vector using an &lt;a href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca"&gt;embedding algorithm&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/embeddings.png"/&gt;
&lt;br/&gt;
  Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes.

&lt;p&gt;The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 â€“ In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder thatâ€™s directly below. The size of this list is hyperparameter we can set â€“ basically it would be the length of the longest sentence in our training dataset.&lt;/p&gt;
&lt;p&gt;After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/encoder_with_tensors.png"/&gt;
&lt;br/&gt;

&lt;p&gt;Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.&lt;/p&gt;
&lt;p&gt;Next, weâ€™ll switch up the example to a shorter sentence and weâ€™ll look at what happens in each sub-layer of the encoder.&lt;/p&gt;
&lt;h2&gt;Now Weâ€™re Encoding!&lt;/h2&gt;
&lt;p&gt;As weâ€™ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a â€˜self-attentionâ€™ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/encoder_with_tensors_2.png"/&gt;
&lt;br/&gt;
  The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.

&lt;h2&gt;Self-Attention at a High Level&lt;/h2&gt;
&lt;p&gt;Donâ€™t be fooled by me throwing around the word â€œself-attentionâ€ like itâ€™s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.&lt;/p&gt;
&lt;p&gt;Say the following sentence is an input sentence we want to translate:&lt;/p&gt;
&lt;p&gt;â€&lt;code&gt;The animal didn't cross the street because it was too tired&lt;/code&gt;â€&lt;/p&gt;
&lt;p&gt;What does â€œitâ€ in this sentence refer to? Is it referring to the street or to the animal? Itâ€™s a simple question to a human, but not as simple to an algorithm.&lt;/p&gt;
&lt;p&gt;When the model is processing the word â€œitâ€, self-attention allows it to associate â€œitâ€ with â€œanimalâ€.&lt;/p&gt;
&lt;p&gt;As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.&lt;/p&gt;
&lt;p&gt;If youâ€™re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one itâ€™s processing. Self-attention is the method the Transformer uses to bake the â€œunderstandingâ€ of other relevant words into the one weâ€™re currently processing.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization.png"/&gt;
&lt;br/&gt;
  As we are encoding the word "it" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on "The Animal", and baked a part of its representation into the encoding of "it".

&lt;p&gt;Be sure to check out the &lt;a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb"&gt;Tensor2Tensor notebook&lt;/a&gt; where you can load a Transformer model, and examine it using this interactive visualization.&lt;/p&gt;
&lt;h2&gt;Self-Attention in Detail&lt;/h2&gt;
&lt;p&gt;Letâ€™s first look at how to calculate self-attention using vectors, then proceed to look at how itâ€™s actually implemented â€“ using matrices.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;first step&lt;/strong&gt; in calculating self-attention is to create three vectors from each of the encoderâ€™s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.&lt;/p&gt;
&lt;p&gt;Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They donâ€™t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_self_attention_vectors.png"/&gt;
&lt;br/&gt;
  Multiplying x1 by the WQ weight matrix produces q1, the "query" vector associated with that word. We end up creating a "query", a "key", and a "value" projection of each word in the input sentence.

&lt;p&gt;&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;What are the â€œqueryâ€, â€œkeyâ€, and â€œvalueâ€ vectors?
&lt;br/&gt;
&lt;br/&gt;
Theyâ€™re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, youâ€™ll know pretty much all you need to know about the role each of these vectors plays.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;second step&lt;/strong&gt; in calculating self-attention is to calculate a score. Say weâ€™re calculating the self-attention for the first word in this example, â€œThinkingâ€. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.&lt;/p&gt;
&lt;p&gt;The score is calculated by taking the dot product of the query vector with the key vector of the respective word weâ€™re scoring. So if weâ€™re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_self_attention_score.png"/&gt;
&lt;br/&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;third and fourth steps&lt;/strong&gt; are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper â€“ 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so theyâ€™re all positive and add up to 1.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/self-attention_softmax.png"/&gt;
&lt;br/&gt;

&lt;p&gt;This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes itâ€™s useful to attend to another word that is relevant to the current word.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;fifth step&lt;/strong&gt; is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;sixth step&lt;/strong&gt; is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/self-attention-output.png"/&gt;
&lt;br/&gt;

&lt;p&gt;That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So letâ€™s look at that now that weâ€™ve seen the intuition of the calculation on the word level.&lt;/p&gt;
&lt;h2&gt;Matrix Calculation of Self-Attention&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The first step&lt;/strong&gt; is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices weâ€™ve trained (WQ, WK, WV).&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png"/&gt;
&lt;br/&gt;
  Every row in the X matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finally&lt;/strong&gt;, since weâ€™re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png"/&gt;
&lt;br/&gt;
  The self-attention calculation in matrix form

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2&gt;The Beast With Many Heads&lt;/h2&gt;
&lt;p&gt;The paper further refined the self-attention layer by adding a mechanism called â€œmulti-headedâ€ attention. This improves the performance of the attention layer in two ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;It expands the modelâ€™s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If weâ€™re translating a sentence like â€œThe animal didnâ€™t cross the street because it was too tiredâ€, it would be useful to know which word â€œitâ€ refers to.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It gives the attention layer multiple â€œrepresentation subspacesâ€. As weâ€™ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png"/&gt;
&lt;br/&gt;
   With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.
 
&lt;p&gt;&lt;br/&gt;
If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_attention_heads_z.png"/&gt;
&lt;br/&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices â€“ itâ€™s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.&lt;/p&gt;
&lt;p&gt;How do we do that? We concat the matrices then multiply them by an additional weights matrix WO.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png"/&gt;
&lt;br/&gt;

&lt;p&gt;Thatâ€™s pretty much all there is to multi-headed self-attention. Itâ€™s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png"/&gt;
&lt;br/&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Now that we have touched upon attention heads, letâ€™s revisit our example from before to see where the different attention heads are focusing as we encode the word â€œitâ€ in our example sentence:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png"/&gt;
&lt;br/&gt;
  As we encode the word "it", one attention head is focusing most on "the animal", while another is focusing on "tired" -- in a sense, the model's representation of the word "it" bakes in some of the representation of both "animal" and "tired".

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;If we add all the attention heads to the picture, however, things can be harder to interpret:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png"/&gt;
&lt;br/&gt;

&lt;h2&gt;Representing The Order of The Sequence Using Positional Encoding&lt;/h2&gt;
&lt;p&gt;One thing thatâ€™s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.&lt;/p&gt;
&lt;p&gt;To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once theyâ€™re projected into Q/K/V vectors and during dot-product attention.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png"/&gt;
&lt;br/&gt;
  To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_positional_encoding_example.png"/&gt;
&lt;br/&gt;
  A real example of positional encoding with a toy embedding size of 4

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;What might this pattern look like?&lt;/p&gt;
&lt;p&gt;In the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector weâ€™d add to the embedding of the first word in an input sequence. Each row contains 512 values â€“ each with a value between 1 and -1. Weâ€™ve color-coded them so the pattern is visible.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png"/&gt;
&lt;br/&gt;
  A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.

&lt;p&gt;The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in &lt;a href="https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py"&gt;&lt;code&gt;get_timing_signal_1d()&lt;/code&gt;&lt;/a&gt;. This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;July 2020 Update:&lt;/strong&gt; 
The positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesnâ€™t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. &lt;a href="https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb"&gt;Hereâ€™s the code to generate it&lt;/a&gt;:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png"/&gt;
&lt;br/&gt;

&lt;h2&gt;The Residuals&lt;/h2&gt;
&lt;p&gt;One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a &lt;a href="https://arxiv.org/abs/1607.06450"&gt;layer-normalization&lt;/a&gt; step.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png"/&gt;
&lt;br/&gt;

&lt;p&gt;If weâ€™re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png"/&gt;
&lt;br/&gt;

&lt;p&gt;This goes for the sub-layers of the decoder as well. If weâ€™re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png"/&gt;
&lt;br/&gt;

&lt;h2&gt;The Decoder Side&lt;/h2&gt;
&lt;p&gt;Now that weâ€™ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But letâ€™s take a look at how they work together.&lt;/p&gt;
&lt;p&gt;The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its â€œencoder-decoder attentionâ€ layer which helps the decoder focus on appropriate places in the input sequence:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_decoding_1.gif"/&gt;
&lt;br/&gt;
  After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).

&lt;p&gt;The following steps repeat the process until a special  symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif"/&gt;
&lt;br/&gt;

&lt;p&gt;The self attention layers in the decoder operate in a slightly different way than the one in the encoder:&lt;/p&gt;
&lt;p&gt;In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to &lt;code&gt;-inf&lt;/code&gt;) before the softmax step in the self-attention calculation.&lt;/p&gt;
&lt;p&gt;The â€œEncoder-Decoder Attentionâ€ layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.&lt;/p&gt;
&lt;h2&gt;The Final Linear and Softmax Layer&lt;/h2&gt;
&lt;p&gt;The decoder stack outputs a vector of floats. How do we turn that into a word? Thatâ€™s the job of the final Linear layer which is followed by a Softmax Layer.&lt;/p&gt;
&lt;p&gt;The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.&lt;/p&gt;
&lt;p&gt;Letâ€™s assume that our model knows 10,000 unique English words (our modelâ€™s â€œoutput vocabularyâ€) that itâ€™s learned from its training dataset. This would make the logits vector 10,000 cells wide â€“ each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.&lt;/p&gt;
&lt;p&gt;The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png"/&gt;
&lt;br/&gt;
  This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2&gt;Recap Of Training&lt;/h2&gt;
&lt;p&gt;Now that weâ€™ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.&lt;/p&gt;
&lt;p&gt;During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.&lt;/p&gt;
&lt;p&gt;To visualize this, letâ€™s assume our output vocabulary only contains six words(â€œaâ€, â€œamâ€, â€œiâ€, â€œthanksâ€, â€œstudentâ€, and â€œ&amp;lt;eos&amp;gt;â€ (short for â€˜end of sentenceâ€™)).&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/vocabulary.png"/&gt;
&lt;br/&gt;
   The output vocabulary of our model is created in the preprocessing phase before we even begin training.
 
&lt;p&gt;Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word â€œamâ€ using the following vector:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/one-hot-vocabulary-example.png"/&gt;
&lt;br/&gt;
  Example: one-hot encoding of our output vocabulary

&lt;p&gt;Following this recap, letâ€™s discuss the modelâ€™s loss function â€“ the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.&lt;/p&gt;
&lt;h2&gt;The Loss Function&lt;/h2&gt;
&lt;p&gt;Say we are training our model. Say itâ€™s our first step in the training phase, and weâ€™re training it on a simple example â€“ translating â€œmerciâ€ into â€œthanksâ€.&lt;/p&gt;
&lt;p&gt;What this means, is that we want the output to be a probability distribution indicating the word â€œthanksâ€. But since this model is not yet trained, thatâ€™s unlikely to happen just yet.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_logits_output_and_label.png"/&gt;
&lt;br/&gt;
  Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output.

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;How do you compare two probability distributions? We simply subtract one from the other. For more details, look at  &lt;a href="https://colah.github.io/posts/2015-09-Visual-Information/"&gt;cross-entropy&lt;/a&gt; and &lt;a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained"&gt;Kullbackâ€“Leibler divergence&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But note that this is an oversimplified example. More realistically, weâ€™ll use a sentence longer than one word. For example â€“ input: â€œje suis Ã©tudiantâ€ and expected output: â€œi am a studentâ€. What this really means, is that we want our model to successively output probability distributions where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)&lt;/li&gt;
&lt;li&gt;The first probability distribution has the highest probability at the cell associated with the word â€œiâ€&lt;/li&gt;
&lt;li&gt;The second probability distribution has the highest probability at the cell associated with the word â€œamâ€&lt;/li&gt;
&lt;li&gt;And so on, until the fifth output distribution indicates â€˜&lt;code&gt;&amp;lt;end of sentence&amp;gt;&lt;/code&gt;â€™ symbol, which also has a cell associated with it from the 10,000 element vocabulary.&lt;/li&gt;
&lt;/ul&gt;

&lt;img src="https://jalammar.github.io/images/t/output_target_probability_distributions.png"/&gt;
&lt;br/&gt;
   The targeted probability distributions we'll train our model against in the training example for one sample sentence.
 
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png"/&gt;
&lt;br/&gt;
    Hopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: &lt;a href="https://www.youtube.com/watch?v=TIgfjmp-4BA"&gt;cross validation&lt;/a&gt;). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process.

&lt;p&gt;Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. Thatâ€™s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, â€˜Iâ€™ and â€˜aâ€™ for example), then in the next step, run the model twice: once assuming the first output position was the word â€˜Iâ€™, and another time assuming the first output position was the word â€˜aâ€™, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3â€¦etc. This method is called â€œbeam searchâ€, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning weâ€™ll return two translations). These are both hyperparameters that you can experiment with.&lt;/p&gt;
&lt;h2&gt;Go Forth And Transform&lt;/h2&gt;
&lt;p&gt;I hope youâ€™ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, Iâ€™d suggest these next steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read the &lt;a href="https://arxiv.org/abs/1706.03762"&gt;Attention Is All You Need&lt;/a&gt; paper, the Transformer blog post (&lt;a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"&gt;Transformer: A Novel Neural Network Architecture for Language Understanding&lt;/a&gt;), and the &lt;a href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html"&gt;Tensor2Tensor announcement&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Watch &lt;a href="https://www.youtube.com/watch?v=rBCqOTEfxvg"&gt;Åukasz Kaiserâ€™s talk&lt;/a&gt; walking through the model and its details&lt;/li&gt;
&lt;li&gt;Play with the &lt;a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb"&gt;Jupyter Notebook provided as part of the Tensor2Tensor repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Explore the &lt;a href="https://github.com/tensorflow/tensor2tensor"&gt;Tensor2Tensor repo&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Follow-up works:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1706.03059"&gt;Depthwise Separable Convolutions for Neural Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1706.05137"&gt;One Model To Learn Them All&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1801.09797"&gt;Discrete Autoencoders for Sequence Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1801.10198"&gt;Generating Wikipedia by Summarizing Long Sequences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1802.05751"&gt;Image Transformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1804.00247"&gt;Training Tips for the Transformer Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1803.02155"&gt;Self-Attention with Relative Position Representations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1803.03382"&gt;Fast Decoding in Sequence Models using Discrete Latent Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1804.04235"&gt;Adafactor: Adaptive Learning Rates with Sublinear Memory Cost&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Thanks to &lt;a href="https://twitter.com/ilblackdragon"&gt;Illia Polosukhin&lt;/a&gt;, &lt;a href="http://jakob.uszkoreit.net/"&gt;Jakob Uszkoreit&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/llion-jones-9ab3064b"&gt;Llion Jones &lt;/a&gt;, &lt;a href="https://ai.google/research/people/LukaszKaiser"&gt;Lukasz Kaiser&lt;/a&gt;, &lt;a href="https://twitter.com/nikiparmar09"&gt;Niki Parmar&lt;/a&gt;, and &lt;a href="https://dblp.org/pers/hd/s/Shazeer:Noam"&gt;Noam Shazeer&lt;/a&gt; for providing feedback on earlier versions of this post.&lt;/p&gt;
&lt;p&gt;Please hit me up on &lt;a href="https://twitter.com/JayAlammar"&gt;Twitter&lt;/a&gt; for any corrections or feedback.&lt;/p&gt;


    Written on June 27, 2018
  
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://jalammar.github.io/illustrated-transformer/</guid>
      <category>Hacker News</category>
      <pubDate>Mon, 22 Dec 2025 19:15:56 +0000</pubDate>
    </item>
    <item>
      <title>GLM-4.7: Advancing the Coding Capability</title>
      <link>https://z.ai/blog/glm-4.7</link>
      <description>GLM-4.7: Advancing the Coding Capability</description>
      <content:encoded>&lt;div class="example-content"&gt;Live StyleCyber DomainArtistic PortfolioPromptbuild a html website, High-contrast dark mode + bold condensed headings + animated ticker + chunky category chips + magnetic CTA.&lt;!-- --&gt; &lt;a href="https://chat.z.ai/s/23c15468-405d-4993-9110-cffa99f79acb"&gt;View full trajectory at Z.ai&lt;img src="https://z-cdn.chatglm.cn/z-blog/z-icon.svg"/&gt;&lt;/a&gt;GLM-4.7GLM-4.6Scroll down to see more&lt;/div&gt;</content:encoded>
      <guid isPermaLink="false">https://z.ai/blog/glm-4.7</guid>
      <category>Hacker News</category>
      <pubDate>Mon, 22 Dec 2025 18:46:32 +0000</pubDate>
    </item>
    <item>
      <title>NIST was 5 Î¼s off UTC after last week's power cut</title>
      <link>https://www.jeffgeerling.com/blog/2025/nist-was-5-Î¼s-utc-after-last-weeks-power-cut</link>
      <description>If you were 5 microseconds late today, blame it on NIST.</description>
      <content:encoded>&lt;article class="node node--type-blog-post node--promoted node--view-mode-full"&gt;



&lt;p&gt;If you were 5 microseconds late today, blame it on NIST.&lt;/p&gt;
&lt;p&gt;Their facility in Boulder Colorado just had its power cut for multiple days. After a backup generator failed, their main ensemble clock lost track of UTC, or Universal Time Coordinated.&lt;/p&gt;
&lt;p&gt;But even if you used the &lt;a href="https://tf.nist.gov/tf-cgi/servers.cgi"&gt;NTP timing servers they run&lt;/a&gt;, they were never off by more than 5 microseconds.&lt;/p&gt;
&lt;p&gt;5 Î¼s might seem insignificant. But it is significant for scientists and universities who rely on NIST's more &lt;a href="https://wsts.atis.org/wp-content/uploads/2023/03/04-Judah-Levine.The-NIST-Fiber-Based-Time-Service.pdf"&gt;specialized timing signals&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But no, you don't need to panic. And yes, they have it under control now.&lt;/p&gt;
&lt;p&gt;But I thought I'd go over what happened, what it means, and what we can learn from NIST's near-outage.&lt;/p&gt;
Video
&lt;p&gt;This blog post is a lightly-edited transcript of my most recent YouTube video:&lt;/p&gt;



What happened
&lt;p&gt;The NIST campus, which distributes Internet time on six of the most popular NTP servers, &lt;a href="https://groups.google.com/a/list.nist.gov/g/internet-time-service/c/o0dDDcr1a8I"&gt;lost power last Wednesday&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The power company was forced to cut power because of &lt;a href="https://www.youtube.com/watch?v=E4dMRgPIY_s"&gt;wind gusts over 100 mph&lt;/a&gt; (160 km/h). Power lines were coming down and they didn't want to risk starting a wildfire.&lt;/p&gt;
&lt;p&gt;The whole campus was locked down for safety, so nobody could enter or exit.&lt;/p&gt;
&lt;p&gt;They have backup generators. And those were working... but apparently one of the generators failed after a couple days. Specifically, the generator that powered the main &lt;a href="https://navi.ion.org/content/67/2/333"&gt;ensemble clock&lt;/a&gt; that's used by the NTP servers.&lt;/p&gt;
&lt;p&gt;Things were dicey last Friday, and they couldn't get any more staff in to fix it.&lt;/p&gt;
&lt;p&gt;It got to the point Jeff Sherman, the Group Leader for NIST's Time Realization and Distribution Group, considered shutting down the backup generator that powered the time servers. That would've prevented them from sending out inaccurate time, which would be worse than no time at all for a lot of applications.&lt;/p&gt;
&lt;p&gt;NTP's designed so you have &lt;a href="https://access.redhat.com/solutions/58025"&gt;multiple servers&lt;/a&gt; you look at, and if one fails, it won't cause you to lose time.&lt;/p&gt;
&lt;p&gt;And luckily for NIST, they have another building in their Boulder campus with more clocks, and that building could transfer time back to the one that had the power failure, if they needed to.&lt;/p&gt;
&lt;p&gt;But yesterday Jeff posted &lt;a href="https://groups.google.com/a/list.nist.gov/g/internet-time-service/c/OHOO_1OYjLY"&gt;another update&lt;/a&gt;: power was restored, and apparently there were still some staff on-site who saved the clocks.&lt;/p&gt;
&lt;p&gt;They were able to re-route emergency power after the main backup generator went down.&lt;/p&gt;
&lt;p&gt;Battery backups, I'm assuming some big UPSes, were able to bridge the gap, until they got the backup backup power going.&lt;/p&gt;
&lt;p&gt;When all was said and done, their monitoring showed deviation from UTC was less than 5 Î¼s.&lt;/p&gt;
&lt;p&gt;Seeing all that, Jeff and the team at NIST decided to keep their time servers online.&lt;/p&gt;
&lt;p&gt;But why would they do that, if they were off? Well, time scales are important here. If you're on a Mac like I am, go in the Terminal and run sntp time-a-b.nist.gov.&lt;/p&gt;
&lt;p&gt;This or a command like ntpdate on Linux gives back an error bound, that shows latency between your computer and the &lt;a href="https://tf.nist.gov/tf-cgi/servers.cgi"&gt;NTP time servers&lt;/a&gt;.&lt;/p&gt;
$ sntp time-a-b.nist.gov
+0.005771 +/- 0.035081 time-a-b.nist.gov 132.163.96.1

&lt;p&gt;In my case, it's showing 0.035 seconds. That's 35 milliseconds, or 35 thousand microseconds. 5 microseconds isn't even a blip there.&lt;/p&gt;
&lt;p&gt;So instead of taking down the servers, which &lt;a href="https://community.ntppool.org/t/status-page-not-working/3907/5"&gt;could cause more problems&lt;/a&gt;, NIST kept them online.&lt;/p&gt;
&lt;p&gt;But Jeff said NIST's time is usually about 5,000x more accurate. And if you're one of the universities or aerospace companies that relies on NIST for timing, a 5 Î¼s difference probably does matter.&lt;/p&gt;
&lt;p&gt;So they'll be working with those groups directly. But for most people, they'll never even notice.&lt;/p&gt;
&lt;p&gt;Jeff finished off the email mentioning the US GPS system failed over successfully to the WWV-Ft. Collins campus. So again, for almost everyone, there was zero issue, and the redundancy designed into the system worked like it's supposed to.&lt;/p&gt;
Time is fragile
&lt;p&gt;&lt;img alt="Jeff Geerling studio timing rack" src="https://www.jeffgeerling.com/sites/default/files/images/rack-room-jeff-geerling-timing.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;I was following this closely over the weekend. I have two Raspberry Pi GPS clocks in the studio. One runs my main Stratum 0 NTP server, and the other one I have running as a backup for testing. (Yes I know I should have 4+ going for &lt;a href="https://en.wikipedia.org/wiki/Segal%27s_law"&gt;good quorum&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;They both run off my &lt;a href="https://www.jeffgeerling.com/blog/2025/installing-outdoor-gps-antenna-more-accurate-time"&gt;outdoor GPS antenna&lt;/a&gt;, which is distributed in my rack room and in my studio for time research.&lt;/p&gt;
&lt;p&gt;Like my studio, most places that need precise time rely on GPS. And that could be a problem!&lt;/p&gt;
&lt;p&gt;I'm glad redundancies kept GPS from driftingâ€”I don't know what would happen if GPS time goes away, but it wouldn't be good! But the main takeaway I think is this: timing infrastructure is fragile.&lt;/p&gt;
&lt;p&gt;CISA identified &lt;a href="https://www.gps.gov/sites/default/files/2025-06/AdvisoryMeetings_Roskind_May2023.pdf"&gt;a lot of risk in the US's over-dependence on GPS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Because of that, the US announced it's trying to &lt;a href="https://docs.fcc.gov/public/attachments/DOC-410031A1.pdf"&gt;find good alternatives&lt;/a&gt; for PNT (Position, Navigation, and Timing) earlier this year.&lt;/p&gt;
&lt;p&gt;I was actually at a meeting at the NAB where Jeff Sherman, the scientist who wrote the two NIST updates, was talking about BPS. The &lt;a href="https://www.jeffgeerling.com/blog/2025/bps-gps-alternative-nobodys-heard"&gt;Broadcast Positioning System&lt;/a&gt; would give us redundancy even if GPS was down.&lt;/p&gt;
&lt;p&gt;But even with multiple time sources, some places need more. I have two Rubidium atomic clocks in my studio, including the one inside a fancy GPS Disciplined Oscillator (GPSDO). That's good for holdover. Even if someone were jamming my signal, or my GPS antenna broke, I could keep my time accurate to nanoseconds for a while, and milliseconds for months. That'd be good enough for me.&lt;/p&gt;
&lt;p&gt;(If I'm being truthful, it's actually overkill, but I'm in the &lt;a href="http://www.leapsecond.com/time-nuts.htm"&gt;time-nut rabbit hole&lt;/a&gt; nowâ€”if you know, you know.)&lt;/p&gt;
&lt;p&gt;But some places do need nanoseconds, for science experiments, RF, media, or finance. And they might run their own even more precise clocks. But they still trace things back to NIST, at least most do here in the US.&lt;/p&gt;
&lt;p&gt;So when NIST's disaster response is tested, everyone's watching.&lt;/p&gt;
&lt;p&gt;Last week, when we were microseconds from disaster, the team at NIST fixed it so almost nobody noticed.&lt;/p&gt;




&lt;a href="https://www.jeffgeerling.com/tags/nist"&gt;nist&lt;/a&gt;
&lt;a href="https://www.jeffgeerling.com/tags/time"&gt;time&lt;/a&gt;
&lt;a href="https://www.jeffgeerling.com/tags/timing"&gt;timing&lt;/a&gt;
&lt;a href="https://www.jeffgeerling.com/tags/gps"&gt;gps&lt;/a&gt;
&lt;a href="https://www.jeffgeerling.com/tags/bps"&gt;bps&lt;/a&gt;
&lt;a href="https://www.jeffgeerling.com/tags/clock"&gt;clock&lt;/a&gt;
&lt;a href="https://www.jeffgeerling.com/tags/ntp"&gt;ntp&lt;/a&gt;
&lt;a href="https://www.jeffgeerling.com/tags/internet"&gt;internet&lt;/a&gt;


 


&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://www.jeffgeerling.com/blog/2025/nist-was-5-Î¼s-utc-after-last-weeks-power-cut</guid>
      <category>Hacker News</category>
      <pubDate>Mon, 22 Dec 2025 17:01:28 +0000</pubDate>
    </item>
    <item>
      <title>Show HN: Yapi â€“ FOSS terminal API client for power users</title>
      <link>https://yapi.run/blog/what-is-yapi</link>
      <description>Yapi is Postman, Insomnia or Bruno for the power user.</description>
      <content:encoded>&lt;article class="max-w-3xl w-full"&gt;&lt;a href="https://yapi.run/blog"&gt;Back to Blog&lt;/a&gt;&lt;blockquote&gt;
&lt;p&gt;Yapi is Postman, Insomnia or Bruno for the power user.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yapi is an OSS command line tool that makes it easy to test APIs from your
terminal. Yapi speaks HTTP, gRPC, TCP, GraphQL (and more coming soon).&lt;/p&gt;
&lt;p&gt;&lt;img alt="yapi in action" src="https://github.com/jamierpond/madea.blog/blob/main/yapi/yapi-example.gif?raw=true"/&gt;&lt;/p&gt;
&lt;h3&gt;Heads up! &lt;em&gt;Yapi is early, early alpha software&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;I use yapi daily in my development workflow. However, yapi is a &lt;em&gt;SUPER&lt;/em&gt;
young project and will have bugs, missing features and rough edges.&lt;/p&gt;
&lt;p&gt;If you &lt;a href="https://github.com/jamierpond/yapi"&gt;download yapi&lt;/a&gt;, I would &lt;em&gt;LOVE&lt;/em&gt; your feedback on how to make it better. Please &lt;a href="https://github.com/jamierpond/yapi/issues"&gt;open an issue&lt;/a&gt;
if you have any suggestions or find any bugs!&lt;/p&gt;
&lt;h3&gt;Show me some examples!&lt;/h3&gt;
&lt;h4&gt;POST&lt;/h4&gt;
&lt;p&gt;This request:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#Â create-issue.yapi.yml
yapi: v1 # specify yapi version
method: POST #Â or GET, PUT, DELETE, PATCH, etc.
url: https://api.github.com/repos/jamierpond/yapi/issues

headers:
  Accept: application/vnd.github+json
  Authorization: Bearer ${GITHUB_PAT} # supports environment variables

body: # defaults to JSON body, converted automatically
  title: Help! yapi made me too productive.
  body: Now I can't stop YAPPIN' about yapi!

expect: #Â supports expected response tests
  status: 201
  assert: # assert using jq syntax
    - .body == "Now I can't stop YAPPIN' about yapi!"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Gives you this response:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yapi run create-issue.yapi.yml
{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "OWNER",
  "body": "Now I can't stop YAPPIN' about yapi!\n",
  // ...blah blah blah
}

URL: https://api.github.com/repos/jamierpond/yapi/issues
Time: 579.408625ms
Size: 2.3 kB (1 lines, 2288 chars)

[PASS] status check
[PASS] .body == "Now I can't stop YAPPIN' about yapi!"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;(Only the JSON goes to stdout, the rest goes to stderr, so is pipeable!)&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Yapi supports chaining requests between protocols&lt;/h3&gt;
&lt;h4&gt;Multi-protocol chaining&lt;/h4&gt;
&lt;p&gt;Yapi makes it easy to chain requests and share data between them, even if they are different protocols.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#Â multi-protocol-chain.yapi.yml
yapi: v1
chain:
  - name: get_todo # HTTP request
    url: https://jsonplaceholder.typicode.com/todos/1
    method: GET

  - name: grpc_hello # gRPC request
    url: grpc://grpcb.in:9000
    service: hello.HelloService
    rpc: SayHello # Supports reflection if server has it enabled
    plaintext: true

    body:
      greeting: $get_todo.title #Â use data from previous request

    expect:
      assert:
        - .reply == "hello delectus aut autem" # assert on gRPC response
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://yapi.run/c/Df.8OgAzqUG-ZpArHJoiQovBOB1XLQM8BNSbz655jxnoC_d1kKWwWp3qeqyLUO_M73lKIwrUWJ6prOLMl5R2.qS8r1QzMvktH~6BGCJjH3E6CO6.iI_fuN4z.kAT3W70t~of01maCruwdBfYgS_nxmtesmpiqKbjalxSWIKMGIRbdL_u8WwRc89gKXPB0kW5wy_CGDmIF0Kef.vuRpV7Hm3BjZie2~yI6DHtvb-RgOqEzldl0W~Y.m4RTNw_Oy.OlksIrAkhwiIy6gdyVqPk0tgTzOePDzuo.trOigUj1AtfBNDpowrEdIU3MmT53pjaobcZV73FYwwpjWOfdNCEJqKTTUVrCc8ZgVB5riViWnJxqo5yz759FWyiBFbi~_2nLHnu67V7RLQPUYnFknAovF2SegQYEmmI~vrbJjjmdNShJNaVW"&gt;Run this example in the yapi playground&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Integration Testing with yapi&lt;/h3&gt;
&lt;p&gt;Yapi has built-in support for writing integration tests with expectations and assertions.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yapi: v1
method: GET
url: https://api.github.com/repos/jamierpond/yapi/issues/5
headers:
    Accept: application/vnd.github+json
expect:
    status: 200
    assert:
      - .state == "closed"
      - .closed_by.login == "jamierpond"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://yapi.run/c/TccVrmpVtt_daI~NZovhqJO6LNVxr-qTEAL9aQp.nAvs9GoUK8PdGgJ6o34wC-THPf64WS.-iAWCfXohQ8~eo~7iOhp6Wop-a_lwPT.Y624K-1S97CVJgXwMRxtwUzZz6on7xftwVegIkX~6hO81OIvTsA6ASUgWdnf8va0QlaGHLI5w1d3LZ5WQ4xzYDg8~5lqPB5s3FE~~EL8aBeCKk-FtOPJ1AIPDMNH4X55A8QLYUF3NWY"&gt;Run this example in the yapi playground&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Yapi has an LSP Server for IDE Integration&lt;/h3&gt;
&lt;p&gt;Yapi comes with a Language Server Protocol (LSP) server that provides
syntax highlighting, autocompletion and validation for yapi request files in
editors that support LSP (Neovim, etc), you can also use the &lt;a href="https://github.com/jamierpond/yapi/blob/main/lua/yapi_nvim/init.lua"&gt;yapi Neovim plugin&lt;/a&gt; (still early days).&lt;/p&gt;
&lt;p&gt;At some point I'll write the VSCode extension too, please make an issue if you think this is important!&lt;/p&gt;
&lt;h3&gt;GitHub Actions Support&lt;/h3&gt;
&lt;p&gt;I use yapi's GitHub Action to run &lt;a href="https://github.com/jamierpond/yapi/actions/runs/20426239184/job/58687025263"&gt;integraion tests on the CI for this blog&lt;/a&gt;!&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;name: Integration Tests
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm ci

      - name: Build Application
        run: npm build

      - name: Run Yapi Integration Tests
        uses: jamierpond/yapi/action@0.X.X
        with:
          start: npm run start
          wait-on: http://localhost:3000/health
          command: yapi test ./tests -a
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, here is the output of yapi running integration tests in GitHub Actions:
&lt;img alt="yapi in GitHub Actions" src="https://github.com/jamierpond/madea.blog/blob/main/yapi/ci.png?raw=true"/&gt;&lt;/p&gt;
&lt;h3&gt;Supports for Multiple Environments&lt;/h3&gt;
&lt;p&gt;Yapi make it easy to manage multiple environments (dev, staging, prod, etc).
Define your environments in a &lt;code&gt;yapi.config.yml&lt;/code&gt; file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yapi: v1

default_environment: local

environments:
  local:
    url: http://localhost:3000
    env_file: .env.local
    vars:
      some_param: default_value

  prod:
    url: https://yapi.run
    env_file: .env.prod
    vars:
      some_param: some_value
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then run yapi with the desired environment:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yapi run my-request.yapi.yml --env prod
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This also cleans up your request files a little, now you can use &lt;code&gt;path&lt;/code&gt;s instead of full URLs:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yapi: v1
method: GET
path: /api/v1/status # base URL comes from the selected environment
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Getting Started with Yapi&lt;/h3&gt;
&lt;p&gt;To get started with yapi, simply install it using the instructions on the
&lt;a href="https://github.com/jamierpond/yapi"&gt;yapi GitHub repository&lt;/a&gt; and start creating your
first request files!&lt;/p&gt;
&lt;h3&gt;Contributing to Yapi&lt;/h3&gt;
&lt;p&gt;Yapi is an open-source project maintained by just me, &lt;a href="https://pond.audio"&gt;Jamie&lt;/a&gt;.
If you find bugs or have feature requests, please open an issue on the
&lt;a href="https://github.com/jamierpond/yapi"&gt;yapi GitHub repository&lt;/a&gt;. Pull requests are very welcome too!&lt;/p&gt;&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://yapi.run/blog/what-is-yapi</guid>
      <category>Hacker News</category>
      <pubDate>Mon, 22 Dec 2025 08:49:47 +0000</pubDate>
    </item>
    <item>
      <title>Dancing around the rhythm space with Euclid</title>
      <link>https://pv.wtf/posts/euclidean-rhythms</link>
      <description>2025-12-22</description>
      <content:encoded>&lt;article&gt;
&lt;h1&gt;Dancing around the rhythm space with Euclid&lt;/h1&gt;
&lt;p&gt;2025-12-22&lt;/p&gt;
&lt;p&gt;I've been playing with sequencers and getting out of Euclidean rhythms (kind of) and into Non-Euclidean ones, and at the end of the whole story managed to build a little one of my own. If those words mean nothing to you yet, enjoy the ride.&lt;/p&gt;
&lt;p&gt;There's a little music corner at home, and in it I've got the classic Synth hipster combo of an Elektron Digitone and a Digitakt. Both are really fun to play with, especially the Digitone. I really like the immediacy, and often it's with me on vacation. A powerbank going in and a pair of headphones coming out. The Elektron sequencer workflow makes it easy to get started, and it pairs it with a great FM-synthesis sound engine, which normally has a reputation for being hard to understand.&lt;/p&gt;
&lt;p&gt;And yes, even with the playful Digitone this reputation is still well earned, which has led me to experiment more. Synthesizers are normally fixed in their architecture, oscillator goes into filter and if you are lucky, there's a mysterious mod-matrix to re-jigger parts via cryptic button combinations. I love my hardware, butâ€¦ Sometimes it's just not the best for learning.&lt;/p&gt;
&lt;p&gt;Modular synthesizers exist as well, but my synth corner needs to stay just a corner. This brings us to VCV Rack, it's a software modular synthesis environment with a lot of overlap with the Eurorack hardware format. You can have a lot of fun following &lt;a href="https://www.youtube.com/channel/UCuWKHSHTHMV_nVSeNH4gYAg"&gt;Omri Cohen&lt;/a&gt;'s videos, cables and modulation going everywhere to end up with something &lt;em&gt;Very Nice&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Of course, when you get a cool sound going, it needs to keep playing while you somehow make it worse and worse. This is where a Sequencer usually helps, and there is a wealth of different ones to play with in VCV Rack. Connect them with semi-random modulation sources and all the little bleeps, thonks and thwangs can keep evolving with what the neighbors will add as wall-based percussion track for hours.&lt;/p&gt;
&lt;p&gt;When I play on the Digitone/takt, it's a bit different. What I do is create a set of patterns that evolve in some small way as you move from one to the next. On top of this I tweak and twiddle parameters, filters and effects to help create transitions over time. I keep to my little corner at home, but I'm romanticizing about the idea of a thrilling live performance, especially with live improvisation.&lt;/p&gt;
&lt;p&gt;The tricky thing is that live improvisation requires tools and a thought out workflow, and the way I've been using my devices requires a lot of preparation up-front. On one end maybe more randomization can help, but pure random usually doesn't sound good either, so I need some way to increase my lucky accidents.&lt;/p&gt;
&lt;p&gt;Now, of course, the real solution is really quite simple, you might already be thinking it: I should practice more with the gear that I already have. Yes, so of course I'm doing something entirely different.&lt;/p&gt;
&lt;p&gt;Among the many sequencers in VCV Rack I found some based on Euclidean rhythms, and it turns out that they might be just what I needed. What are they? Hit play and take it away!&lt;/p&gt;

&lt;p&gt;The Euclidean algorithm tries to position a number of hits as evenly as possible across a pattern. There's a lot of material online that explains it from many angles, &lt;a href="https://www.youtube.com/results?search_query=euclidean+rhythms"&gt;YouTube Videos&lt;/a&gt; and &lt;a href="https://cgm.cs.mcgill.ca/~godfried/publications/banff-extended.pdf"&gt;the original paper&lt;/a&gt; written by Godfried Toussaint is also short and clear.&lt;/p&gt;
&lt;p&gt;It turns out that by varying the hits and length of a pattern, you get a fair number of world music rhythms that you will most likely recognize. We'll use &lt;code&gt;E(N,L)&lt;/code&gt; notation with &lt;code&gt;N=Number of hits&lt;/code&gt; and &lt;code&gt;L=Length of pattern&lt;/code&gt; to describe them.&lt;/p&gt;
&lt;p&gt;Take the &lt;code&gt;E(3,8)&lt;/code&gt; rhythm, it's all over the place. In Elvis's Hound Dog, and 50s rockabilly. In Cuba it's the &lt;em&gt;Tresillo&lt;/em&gt; and you can find it in West African drum rhythms. We've also got &lt;code&gt;E(5,8)&lt;/code&gt; which again is present all over the world. The really interesting part is that certain rhythms have made their home more in certain parts of the world than others. The paper goes into more detail so just read it, and maybe use the widget above for playback.&lt;/p&gt;
&lt;p&gt;Back to where we were. The Euclidean algorithm gives us a fair amount of different rhythms. As you probably paused the widget above at some point, you noticed that it can get quite repetitive. Prime number divisions tend to be interesting but some rhythms like &lt;code&gt;E(4,8)&lt;/code&gt; is just a simple four-on-the floor groove. Many Euclidian sequencers allow you to rotate the pattern to create more variation, but you'd need to keep changing it to prevent it from becoming monotonous over time. On the other hand, this might even be desired for something percussive, a funky bassline or the ticking of hi-hats.&lt;/p&gt;
&lt;p&gt;Then modulate the tension by increasing or decreasing the number of hits in the pattern. Get wild, wire a 4-step sequencer that moves every bar, to the density of hits in a Euclidian pattern so that every bar the feeling changes and creates a much more interesting evolution over time. To me, that bridges the gap between a tool for short percussive rhythms, into something that might even be a melody when given a nice big heap of pitch changes.&lt;/p&gt;
&lt;p&gt;There is a more fundamental drawback to Euclidean Rhythms though, which is also their definition: Evenness.&lt;/p&gt;
&lt;p&gt;Or not really a drawback, but a lot of interesting rhythms have uneven rhythm placement. We can use them to explore the rhythm space, but have to keep on the roads. Another approach is to store a lot of interesting patterns and interpolate between them, Mutable Instruments Grids is an example of such a Eurorack module. But I feel like that gives less control and ability to insert intent into the shape of the rhythms.&lt;/p&gt;
&lt;p&gt;There are people experimenting in another direction. Shakmat Knight's gallop is a Eurorack module with a normal Euclidean mode, but also an Anti-Euclidean one. As far as I can tell from the manual, it makes an attempt to position hits as uneven as possible. Both the Knight's Gallop and another one called the lx-euclid with modes that cluster hits in the beginning or end. This gives us some more room in the rhythm space do dance around in.&lt;/p&gt;

&lt;p&gt;Change the interpolation control above to one of the extremes and what you get was one attempt to create something like an Anti-Euclidian rhythm by creating uneven spacing, but I wasn't really happy with the original outcome. There's some interesting grooves in there at lower densities and lengths, but at higher ones it just became too clumped together.&lt;/p&gt;
&lt;p&gt;In his writing, Toussaint discusses how different metrics can be used to describe the distance between two rhythms. That led me to try interpolating between the Euclidean and the Anti-Euclidean rhythm we just played with. It took a bit of tweaking, the key was to constrain the interpolation so that it only moved one hit at a time. This actually felt like exploring the rhythm space in a much more interesting way. The Anti-Euclidean algorithm tended to cluster hits in the beginning, so it made sense to add a mirror version of it as well.&lt;/p&gt;
&lt;p&gt;Nice. That's just one possible take on "optimize for uneven" and there might be more interesting parts of the rhythm space now that we're off-roading it. Another interesting experiment was clumping the hits, and then attempting to do something similar to Wavefolding where as peaks start to form we push them back in on itself, creating holes in the clusters that turn into new peaks.&lt;/p&gt;

&lt;p&gt;I don't know about you, but to me this version also has a couple of good grooves in its repertoire. A drawback is that as you increase the intensity you also get repeats of the same rhythms you've had before, lessening the feeling that you are exploring the rhythm space.&lt;/p&gt;
&lt;p&gt;Though again, setting one of these as the interpolation target and exploring what you find on the way is quite fun still, so I really liked that.&lt;/p&gt;
&lt;p&gt;I continued to play around with more ideas, thinking of different places in the rhythm space we could visit. Trying things like alternating clusters, but they didn't really result in more interesting rhythms. We could also do something like Euclidean subdivisions, treat the pattern as one of a subdivided length and use that to place the hits in clusters instead.&lt;/p&gt;
&lt;p&gt;The widget below collects a few of those experiments, it's less of a curated tour and more of a notebook of different attempts at this point, select an algorithm in the dropdown and make sure to play with interpolation slider.&lt;/p&gt;

&lt;p&gt;My main takeaway from the experimentation was that Euclidean rhythms are good (no surprise there), and to add some spice you don't need to stray too far away from them to get interesting syncopation. A rhythm that looks beautiful on a circle might still not sound so interesting, as I tried to take higher-order polynomial curves and map them to the hits. In the end, interpolating between a clustered version and the Euclidean was one of the best ways to explore.&lt;/p&gt;
&lt;p&gt;With that in mind, I wanted to try moving these experiments a bit further towards a sequencer that could support longer patterns, and combine it with some of the ideas I mentioned earlier, like changing density over time. The interpolation feature is key, and modulating hit density over time. The closest standalone sequencers I know would be either the Torso T1 or the Oxi one, but I don't think either really support do that outside of manually changing controls.&lt;/p&gt;
&lt;p&gt;I started expanding the widgets above into this, and it grew and grew until I split it out into a separate page. Firstly because then I don't need to scroll to the ends of the earth every time I refreshed the page. It also makes it easier for myself to bookmark for the future.&lt;/p&gt;
&lt;p&gt;Obligatory side-note: I'm writing this in the very tail end of 2025, a time where everything circles around AI in some way. If I had been writing a year or two back, all of the interactive widgets would probably have been images instead. They are not that hard to build, but involve a lot of micro-decisions that take time and energy. I do my faffing about in the mornings before work, so time and energy are not that abundant.&lt;/p&gt;
&lt;p&gt;This was an opportunity to experimenting with the latest batch of AI models while building as well. Claude Opus 4.5 seems like a clear step up from Sonnet 4.5 (but more annoying usage limits), and the new Gemini 3 Flash is also a strong contender. They struggled with some parts though. Neither had an easy time understanding how to make the Shift Register work well in the sequencer context, and I had to rewrite the core rhythm generation a couple of times to make it work the way I needed it to before handing it back.&lt;/p&gt;
&lt;p&gt;With that out of the way, follow the link below.&lt;/p&gt;

&lt;a href="https://pv.wtf/playground/euclidean-sequencer"&gt;Open Really Cool Sequencer&lt;/a&gt;

&lt;p&gt;Back? It might need a little bit of explanation.&lt;/p&gt;
&lt;p&gt;There are four different parts to a pattern, with either a global length or individual part lengths. And for each part we can control the density as an offset from the main one, so it's easy to perform by only tweaking the main one. For each we can also control the distance from the euclidean pattern.&lt;/p&gt;
&lt;p&gt;Now this I feel like has the seed of something that can be used beyond simple-ish percussion loops. Lets you get going quickly but also has enough control to shape what's happening, add intent.&lt;/p&gt;
&lt;p&gt;I added controls for rotation, inverting the rhythm, and manual overrides for forcing a hit or a rest. Having the same pitch for each hit started to get a bit tedious, so I added a Shift Register (also known as Turing Machine) inspired generator for pitch information as well.&lt;/p&gt;
&lt;p&gt;The thing that's really cool (alternatively: really confusing) is the Boolean mode. Normally each part plays in sequence, one after another. But in boolean mode they act as different layers that combine into one output. You can choose which one to listen too, and see what the effect of playing Part 1 OR Part 2 is. And since we already have a Shift Register for pitch, why not reuse it for beats as well?&lt;/p&gt;
&lt;p&gt;There are a couple of more features as well, they are hopefully either easy to understand or fun to click around on anyway so I'm pretty satisfied with this version. What comes next?&lt;/p&gt;
&lt;p&gt;I haven't written a VCV Rack plugin, but it might be fun to try as a next step. Or maybe add WebMIDI to the sequencer, either way would let me see how it plays with the Digitone. End result being that I spend some more time in my synth corner with tiny blinking lights rather than big laptop screens.&lt;/p&gt;
&lt;p&gt;That's it for this dance, see you next time!&lt;/p&gt;


&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://pv.wtf/posts/euclidean-rhythms</guid>
      <category>Hacker News</category>
      <pubDate>Mon, 22 Dec 2025 08:08:46 +0000</pubDate>
    </item>
    <item>
      <title>The post-GeForce era: What if Nvidia abandons PC gaming?</title>
      <link>https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html</link>
      <description>Imagine itâ€™s the year 2030 and Nvidia has just announced its newest RTX 7000-series graphics cards. But the cheapest of the cards is priced over $2,000 and the top model is nearly double that. The series offer minimal uplift on rendering performance, but theyâ€™re incredibly good at accelerated upscaling and frame generation. Plus, memory bandwidth is almost double over the last-gen models.</description>
      <content:encoded>&lt;article class="post-3013044 post type-post status-publish format-standard has-post-thumbnail category-gaming category-components-graphics category-technology-business languages-en publication-pcworld publication-us-default story_types-feature origin-wp" id="post-3013044"&gt;
&lt;!-- .entry-header --&gt;
&lt;!-- &lt;hr class="wp-block-separator" /&gt; --&gt;




&lt;img alt="rtx 5070 ti" src="https://www.pcworld.com/wp-content/uploads/2025/12/rtx-5070-ti-1.jpg?quality=50&amp;amp;strip=all&amp;amp;w=1024"/&gt; 
Image: Adam Patrick Murray / Foundry 
&lt;!-- .post-thumbnail --&gt;


&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"&gt;


Summary created by Smart Answers AI&lt;h3&gt;In summary:&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;PCWorld explores Nvidiaâ€™s potential shift away from PC gaming as AI data center revenue reached $51.2 billion versus just $4.3 billion from gaming in Q3 2025.&lt;/li&gt;&lt;li&gt;Rising component costs and memory shortages driven by AI demand may force Nvidia to cut gaming GPU supply, with RTX 7000-series cards expected to start over $2,000.&lt;/li&gt;&lt;li&gt;This transition could push gaming toward cloud services like GeForce Now, fundamentally changing how consumers access high-end graphics performance through subscription models instead of hardware ownership.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Imagine itâ€™s the year 2030 and Nvidia has just announced its newest RTX 7000-series graphics cards. But the cheapest of the cards is priced over $2,000 and the top model is nearly double that. The series offer minimal uplift on rendering performance, but theyâ€™re incredibly good at accelerated upscaling and frame generation. Plus, memory bandwidth is almost double over the last-gen models.&lt;/p&gt;
&lt;p&gt;Letâ€™s continue the hypothetical: Nvidiaâ€™s new xx60-series cards arenâ€™t expected for months while Nvidia stockpiles enough defective GPUs. But donâ€™t worry if you canâ€™t afford these new cards or donâ€™t want to wait. Why? Because GeForce Now offers the full upgrade &lt;em&gt;right now&lt;/em&gt; for an â€œaffordableâ€ monthly fee, especially with an annual sub locked in.&lt;/p&gt;


&lt;p&gt;I wrote the above as a nightmare scenario, but itâ€™s odd how close it sounds to the launch of the RTX 50-series. Itâ€™s a history that seems likely to repeat and accelerate as Nvidiaâ€™s gaming division becomes an ever-more-minor side hustle to its AI initiatives.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nvidia could effectively give up on gaming in the near future,&lt;/strong&gt; and that might be the most financially sensible thing to do if the AI bubble doesnâ€™t burst. But what would happen if they did?&lt;/p&gt;


&lt;!--js block injected --&gt;




&lt;h2&gt;Just follow the money&lt;/h2&gt;
&lt;p&gt;The numbers behind my pessimistic prognosis paint a stark picture. &lt;a href="https://go.skimresources.com?id=111346X1569483&amp;amp;xs=1&amp;amp;url=https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-third-quarter-fiscal-2026&amp;amp;xcust=2-1-3013044-1-0-0-0-0&amp;amp;sref=https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html"&gt;Nvidiaâ€™s Q3 2025 revenue&lt;/a&gt; topped $57 billion. Guess how much of that money came from data centers? A whopping $51.2 billion. Thatâ€™s just shy of 90% of its total revenue and represents a 25% increase over the previous quarter and a 66% increase year on year.&lt;/p&gt;
&lt;p&gt;How much revenue do you think Nvidia pulled in from gaming? A measly $4.3 billion by comparison. Thatâ€™s down 1% on the previous quarter, and thatâ€™s despite having the most powerful graphics cards available &lt;em&gt;and&lt;/em&gt; with stock and prices being far more favorable than they were earlier in the year. &lt;a href="https://go.skimresources.com?id=111346X1569483&amp;amp;xs=1&amp;amp;url=https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-third-quarter-fiscal-2025&amp;amp;xcust=2-1-3013044-1-0-0-0-0&amp;amp;sref=https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html"&gt;Itâ€™s still up 30% on last year&lt;/a&gt;, but the difference in potential between data centers and gaming is &lt;em&gt;staggering&lt;/em&gt;.&lt;/p&gt;
&lt;img alt="Nvidia sign outside luxurious building" src="https://b2c-contenthub.com/wp-content/uploads/2025/12/Nvidia-sign-outside-luxurious-building.jpg?quality=50&amp;amp;strip=all&amp;amp;w=1200"/&gt;

&lt;p&gt;Nvidia&lt;/p&gt;
&lt;p&gt;Indeed, gaming makes up less than 8% of Nvidiaâ€™s total revenue as of now, and although the overall income from gaming continues to increase, itâ€™s miniscule in comparison to its data center take. &lt;a href="https://go.skimresources.com?id=111346X1569483&amp;amp;xs=1&amp;amp;url=https://bullfincher.io/companies/nvidia-corporation/revenue-by-segment&amp;amp;xcust=2-1-3013044-1-0-0-0-0&amp;amp;sref=https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html"&gt;Bullfincher highlights how quickly thatâ€™s changed&lt;/a&gt;, too: just a few years ago, gaming represented over 33% of Nvidiaâ€™s total revenue.&lt;/p&gt;




&lt;p&gt;Where do you think itâ€™s going to be in another five years? Assuming the AI bubble doesnâ€™t pop as catastrophically as it could, gaming is going to become a tiny footnote on Nvidiaâ€™s balance sheet. Will Jensen Huang even bother doing gaming hardware keynotes at that point?&lt;/p&gt;
&lt;img alt="Nvidia Jensen Huang" src="https://b2c-contenthub.com/wp-content/uploads/2025/01/20250107_103508.jpg?quality=50&amp;amp;strip=all&amp;amp;w=1200"/&gt;

&lt;p&gt;Mark Hachman / IDG&lt;/p&gt;
&lt;p&gt;Nvidia might be the biggest megacorp in this space, but its contemporaries show similar gaming red flags on their balance sheets. &lt;a href="https://go.skimresources.com?id=111346X1569483&amp;amp;xs=1&amp;amp;url=https://ir.amd.com/news-events/press-releases/detail/1265/amd-reports-third-quarter-2025-financial-results&amp;amp;xcust=2-1-3013044-1-0-0-0-0&amp;amp;sref=https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html"&gt;AMD made just over $9 billion this past quarter&lt;/a&gt;, but $4.3 billion was from data center sales while only $1.3 billion came from gaming. Thatâ€™s &lt;a href="https://go.skimresources.com?id=111346X1569483&amp;amp;xs=1&amp;amp;url=https://ir.amd.com/news-events/press-releases/detail/1224/amd-reports-third-quarter-2024-financial-results&amp;amp;xcust=2-1-3013044-1-0-0-0-0&amp;amp;sref=https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html"&gt;much better than last year&lt;/a&gt;â€”when data centers brought in $3.5 billion and gaming just $462 millionâ€”but data centers are still a far bigger portion of AMDâ€™s revenue than gaming.&lt;/p&gt;
&lt;p&gt;These numbers make a compelling case for giving up some interest and investment in gaming hardware development. It doesnâ€™t mean theyâ€™re going to stop make gaming GPUs entirely. (Or does it?) But if youâ€™re Jensen Huang facing off against shareholders who are demanding the revenue numbers go up as much as possible as fast as possible, what are you going to sell them on: a new gaming GPU that has historically low margins, or a new generation of data center hardware to feed into the accelerating AI bubble with untold potential?&lt;/p&gt;
&lt;p&gt;You could even argue that Nvidiaâ€™s increasing focus over the past few years on DLSS and ray tracing over pure rasterization performance is an early sign of it putting its eggs in the data center basket.&lt;/p&gt;




&lt;h2&gt;A canary in the RAM mines&lt;/h2&gt;
&lt;p&gt;The biggest side effect of all these new data center builds hasnâ€™t been GPU scarcity, surprisingly. (At least, not to the degree we saw during the cryptocurrency craze.) Rather, itâ€™s &lt;a href="https://www.pcworld.com/article/3010391/ram-prices-are-painfully-out-of-control-4-ways-to-avoid-the-gouging.html"&gt;skyrocketing memory prices&lt;/a&gt;. RAM kits have increased in price by over 200 percent in some cases, making large capacity kits more costly than top-tier GPUs. Some modest RAM options are even more expensive than gaming consoles.&lt;/p&gt;
&lt;p&gt;Consumer RAM is shooting up in price because all the major memory manufacturers are &lt;a href="https://www.pcworld.com/article/2998935/ram-is-so-expensive-samsung-wont-even-sell-it-to-samsung.html"&gt;inundated with orders for data center memory&lt;/a&gt;, like HBM and LPDDR. Some have begun pivoting their fabrication lines to these higher-margin memory types, leading to shortages of NAND chipsâ€”and, consequently, &lt;a href="https://www.pcworld.com/article/3003682/memory-prices-are-climbing-its-making-these-5-other-products-cost-more.html"&gt;shortages of consumer memory and SSDs&lt;/a&gt;.&lt;/p&gt;
&lt;img alt="Micron Crucial RAM modules sticks DDR memory" src="https://b2c-contenthub.com/wp-content/uploads/2025/12/Micron-Crucial-RAM-modules-sticks-DDR-memory.jpg?quality=50&amp;amp;strip=all&amp;amp;w=1200"/&gt;

&lt;a href="https://go.skimresources.com?id=111346X1569483&amp;amp;xs=1&amp;amp;url=https://www.shutterstock.com/image-photo/bangkok-thailand-april-28-2023-ddr3l-2295520815&amp;amp;xcust=2-1-3013044-1-0-0-0-0&amp;amp;sref=https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html"&gt;Nor Gal / Shutterstock.com&lt;/a&gt;
&lt;p&gt;Those shortages are making RAM and SSDs far more expensive. And yet, despite the increased margins and diminishing supply versus demand, &lt;a href="https://www.pcworld.com/article/2999460/micron-will-end-crucial-consumer-brand-in-another-ai-casualty.html"&gt;Micron just closed its Crucial brand&lt;/a&gt; of consumer RAM and SSDs.&lt;/p&gt;
&lt;p&gt;It was profitable, it was popular, it had a distinct market niche that served consumers and gamers for decades. But even Micron didnâ€™t see the point of keeping it going when it could instead make heaps more cash from selling Micron NAND chips and server memory.&lt;/p&gt;
&lt;p&gt;And &lt;a href="https://www.pcworld.com/article/3001184/i-hope-crucials-death-isnt-a-canary-in-a-pc-memory-coal-mine.html"&gt;if Micron is so willing to pull out&lt;/a&gt; of the consumer space due to AI-driven demand, how much more will Nvidia be tempted to do the same? Whatâ€™s stopping Nvidia from reaching the same conclusion?&lt;/p&gt;
&lt;p&gt;For further proof of this future, Nvidia is &lt;a href="https://go.skimresources.com?id=111346X1569483&amp;amp;xs=1&amp;amp;url=https://overclock3d.net/news/gpu-displays/nvidia-plans-heavy-cuts-to-gpu-supply-in-early-2026/&amp;amp;xcust=2-1-3013044-1-0-0-0-0&amp;amp;sref=https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html"&gt;rumored to be cutting its gaming GPU supply in 2026&lt;/a&gt; due to memory shortages. Itâ€™s especially notable how Nvidia appears to be cutting the more affordable mid-range graphics cards first, leaving ultra-budget and ultra-high-end lines intact for now. Is this just the first step in Nvidia leaving gamers behind?&lt;/p&gt;




&lt;h2&gt;Where things could go from here&lt;/h2&gt;
&lt;p&gt;There are some intriguing comparisons to make between Nvidia and other big businesses that found growth and revenue in avenues that werenâ€™t where they started. IBM went from being &lt;em&gt;the&lt;/em&gt; name in computing hardware to one that largely runs in the background. It sold off its core hardware businesses and became a software and services company thatâ€™s still worth tens of billions of dollars. It recently spun off again, creating a separate company to handle IT services while the core business refocused on cloud computing and AI.&lt;/p&gt;
&lt;p&gt;Nvidia could do that: spin or sell off its gaming divisions and license its GPU technology to that spun-or-sold-off subsidiary.&lt;/p&gt;
&lt;img alt="Nvidia GeForce Now Ultimate upgrade 1" src="https://b2c-contenthub.com/wp-content/uploads/2025/08/GeForce_NOW_Blackwell_KV_.jpg?quality=50&amp;amp;strip=all&amp;amp;w=1200"/&gt;

Notice the lack of graphics cards in this Nvidia promo image.&lt;p&gt;Nvidia&lt;/p&gt;
&lt;p&gt;Perhaps Nvidia could even end up like Adobe. In the mid-2010s, the developer of Photoshop launched Creative Cloud and slowly pushed all its once-in-perpetuity software licenses into a subscription model thatâ€™s still going on today. Could that apply to &lt;a href="https://www.pcworld.com/article/630335/geforce-now-review.html"&gt;Nvidiaâ€™s GeForce Now streaming service&lt;/a&gt;? It had 25 million subscribers as of 2023 and ran on GPUs designed for data center server racks. Nvidia could leave dedicated desktop and laptop GPUs behind entirely and pivot its gaming divisions into software/hardware-as-a-service firms.&lt;/p&gt;
&lt;p&gt;If gaming goes a similar way to TV and movie streaming, itâ€™s possible Nvidia could even pull a Netflix and slowly de-emphasize its DVD-like hardware business in favor of powering it all from the cloud.&lt;/p&gt;




&lt;h2&gt;Gaming wonâ€™t die, but it will change&lt;/h2&gt;
&lt;p&gt;As much as this article is heavy on the doom, Nvidia is unlikely to exit gaming &lt;em&gt;entirely&lt;/em&gt;. People want to play games and thereâ€™s money to be made there, so &lt;em&gt;someone&lt;/em&gt; will keep tapping that market. But how that revenue is extracted may changeâ€”dramatically so.&lt;/p&gt;
&lt;p&gt;Microsoft is already talking about making the &lt;a href="https://www.pcworld.com/article/2953703/did-pcs-win-the-console-war-the-next-xbox-will-run-windows-report-claims.html"&gt;next Xbox more of a PC/console hybrid&lt;/a&gt;. And with the latest Xbox consoles being the third wheel of this generation, it wouldnâ€™t be a surprise to see the future of Xbox focus more on &lt;em&gt;streaming&lt;/em&gt; games than buying/owning them. Xbox Game Pass already has over 37 million subscribersâ€”thatâ€™s more than the number of Xbox Series X/S consoles sold this generation.&lt;/p&gt;
&lt;p&gt;Nvidia could do something similar. Or it could spin off. Or it could stop making gaming GPUs entirely. The only thing we know for sure is this: when a gaming company starts making astronomical amounts of money due to AI-driven demand, itâ€™s hard to imagine it wouldnâ€™t be tempted to dive head-first into an AI-first strategy at the expense of gaming.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further reading:&lt;/strong&gt; &lt;a href="https://www.pcworld.com/article/2542269/pcs-vs-consoles-the-future-of-gaming-looks-blurrier-than-ever.html"&gt;PC vs. consoles? Gamingâ€™s future is blurrier than ever&lt;/a&gt;&lt;/p&gt;




 
&lt;h3&gt;
&lt;a href="https://www.pcworld.com/author/jon-martindale"&gt;
		Author: Jon Martindale&lt;/a&gt;, Contributor, PCWorld		&lt;/h3&gt;



&lt;img src="https://www.pcworld.com/wp-content/uploads/2025/12/author_photo_Jon-Martindale_1728563874-7.jpg?quality=50&amp;amp;strip=all&amp;amp;w=150&amp;amp;h=150&amp;amp;crop=1"/&gt;

&lt;p&gt;Jon Martindale is a voracious writer and technology fanboy who loves nothing more than digging into the specs of the latest graphics cards, processors, and displays. He's passionate about everything PC, but also enjoys experimenting with AIs, and covering new standing desks that can help avoid his worst posture habits.&lt;/p&gt;


&lt;h3&gt; Recent stories by Jon Martindale:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href="https://www.pcworld.com/article/3003682/memory-prices-are-climbing-its-making-these-5-other-products-cost-more.html"&gt;
									RAM costs are skyrocketingâ€”and these 5 other gadgets are paying the price								&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://www.pcworld.com/article/2975248/i-used-chatgpt-to-turn-my-old-laptop-into-an-alien-rpg-muthur-terminal.html"&gt;
									I used ChatGPT to turn my old laptop into an Alien RPG MUTHUR terminal								&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://www.pcworld.com/article/2945507/the-future-of-laptops-is-already-here-6-trends-making-2025-a-breakout-year.html"&gt;
									The future of laptops is already here: 6 trends making 2025 a breakout year								&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;







 


&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html</guid>
      <category>Hacker News</category>
      <pubDate>Sat, 20 Dec 2025 10:25:38 +0000</pubDate>
    </item>
    <item>
      <title>The Polyglot NixOS</title>
      <link>https://x86.lol/generic/2025/12/19/polyglot.html</link>
      <description>Recently a colleague mentioned building
NixOS images that run unchanged on multiple architectures. Given the past
adventures on this blog with systemd-repart and cross-compiling NixOS , I decide to give this a
go.</description>
      <content:encoded>&lt;article class="post h-entry" itemscope="" itemtype="http://schema.org/BlogPosting"&gt;


&lt;p&gt;Recently a &lt;a href="https://github.com/samueldr"&gt;colleague&lt;/a&gt; mentioned building
NixOS images that run unchanged on multiple architectures. Given the past
adventures on this blog with &lt;a href="https://x86.lol/generic/2024/08/28/systemd-sysupdate.html"&gt;systemd-repart&lt;/a&gt; and
&lt;a href="https://x86.lol/generic/2024/09/21/cross-compile-riscv.html"&gt;cross-compiling NixOS&lt;/a&gt;, I decide to give this a
go.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; You can find a quickâ€™nâ€™dirty implementation
&lt;a href="https://github.com/blitz/polyglot-image"&gt;here&lt;/a&gt;. Check the repo for
details on how to build and run it.&lt;/p&gt;
&lt;p&gt;So do we want to do: We want to build &lt;em&gt;one&lt;/em&gt; disk image that boots on
x86_64, ARM AArch64, and RISC-V 64-bit. We limit ourselves here to
UEFI platforms, which makes this pretty straight forward.&lt;/p&gt;
&lt;p&gt;From a high-level we need to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Have a NixOS configuration.&lt;/li&gt;
&lt;li&gt;Build the system closure for each target.&lt;/li&gt;
&lt;li&gt;Throw everything into one &lt;code&gt;/nix/store&lt;/code&gt; partition.&lt;/li&gt;
&lt;li&gt;Populate the &lt;a href="https://en.wikipedia.org/wiki/EFI_system_partition"&gt;ESP&lt;/a&gt; to
boot the right closure depending on the architecture.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All of this is surprisingly straight-forward. The ESP has
architecture-dependent default filenames for what the firmware should
boot, given no other configuration. This means we can build an
&lt;a href="https://wiki.archlinux.org/title/Unified_kernel_image"&gt;UKI&lt;/a&gt; per
architecture and drop it at the right place in the ESP
(&lt;code&gt;/EFI/BOOT/BOOTX64.EFI&lt;/code&gt; for 64-bit x86) and we are done!&lt;/p&gt;
&lt;p&gt;By linking the systemâ€™s UKI in these locations on the ESP, we skip
over having an actual bootloader and thus canâ€™t have multiple
generations, but it makes for a much leaner example!&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/blitz/polyglot-image"&gt;example repo&lt;/a&gt; puts the
closure for each architecture in a single Nix store partition. I
&lt;em&gt;thought&lt;/em&gt; this would bring some space savings, because files that are
not binary code should be largely the same. This doesnâ€™t really pan
out in this small example and we only save a couple of percent. Maybe
it makes a bigger difference for larger closures.&lt;/p&gt;
&lt;p&gt;If you want to dig into the details, the &lt;a href="https://github.com/blitz/polyglot-image"&gt;example
repo&lt;/a&gt; has the instructions
how to build and boot the image. Iâ€™m also eager to see someone
building a more comprehensive version of this that includes a fully
functioning bootloader and multiple generations!&lt;/p&gt;

&lt;a href="https://x86.lol/generic/2025/08/10/change-monitoring.html"&gt;Quick and Dirty Website Change Monitoring ğŸ‘ˆ&lt;/a&gt;
&lt;a href="https://x86.lol/generic/2025/12/19/polyglot.html"&gt;&lt;/a&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://x86.lol/generic/2025/12/19/polyglot.html</guid>
      <category>Hacker News</category>
      <pubDate>Sat, 20 Dec 2025 07:39:00 +0000</pubDate>
    </item>
    <item>
      <title>The Coffee Warehouse</title>
      <link>https://www.scopeofwork.net/the-coffee-warehouse/</link>
      <description>I have a bit of a love hate relationship with Starbucks. It feels expensive. The lines are long. And I resent the fact that I give them an interest-free loan every time I use their mobile app . But my go-to Pike and banana nut loaf are delicious, and the baristas at my preferred location are fun. They give me a hard time if I deviate from my usual, which I appreciate. When Iâ€™m feeling uncertain about the day, making a Starbucks run is a surprisingly good way to get my head on straight.</description>
      <content:encoded>&lt;article class="post tag-feature content-wrap post-access-paid"&gt;


&lt;p&gt;I have a bit of a love hate relationship with Starbucks. It feels expensive. The lines are long. And I resent the fact that I give them an &lt;a href="https://www.morningstar.com/news/marketwatch/20250430172/starbucks-customers-are-giving-the-company-over-200-million-of-free-money?ref=scopeofwork.net"&gt;interest-free loan every time I use their mobile app&lt;/a&gt;. But my go-to Pike and banana nut loaf are delicious, and the baristas at my preferred location are fun. They give me a hard time if I deviate from my usual, which I appreciate. When Iâ€™m feeling uncertain about the day, making a Starbucks run is a surprisingly good way to get my head on straight.&lt;/p&gt;&lt;p&gt;On a recent visit, I arrived at the pickup counter and found my order incomplete. The slice of banana bread had been warmed, bagged, and labeled, but my cup of black coffee had yet to be dispensed into its paper cup. Looking behind the bar, I saw the usual blur of green-aproned baristas moving from task to task. It was frenetic, but organized too, with defined work areas and clear routines. The scene reminded me of my days working at a distribution center. The area where we packed orders had a similar manic choreography, and watching the baristas go about their jobs I found myself trying to understand their order flows and processes. Specifically, I wanted to understand how Starbucks organizes and prioritizes its work.&lt;/p&gt;&lt;h3&gt;Customer Flexibility vs. Operational Complexity.&lt;/h3&gt;&lt;p&gt;Starbucks is in a bit of a slump. Sales in established locations have &lt;a href="https://www.wsj.com/business/hospitality/starbucks-says-its-making-progress-on-quest-to-fulfill-orders-more-quickly-39492de6?st=SHLAvf&amp;amp;reflink=desktopwebshare_permalink&amp;amp;ref=scopeofwork.net"&gt;fallen for 5 consecutive quarters&lt;/a&gt;, contributing to a recent change in leadership. In an attempt to win back customers, the new CEO, Brian Niccol, has made operations a focus and pledged to reduce wait times and improve the customer experience. They are investing heavily in their order sorting algorithms and store processes, with the topic getting conspicuous attention in recent &lt;a href="https://s203.q4cdn.com/326826266/files/doc_financials/2025/q2/SBUX-2Q25-Corrected-Transcript.pdf?utm_source=chatgpt.com"&gt;earnings calls&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Some of the operational challenges stem from &lt;a href="https://www.qsrmagazine.com/story/the-fix-at-starbucks-had-to-start-with-mobile-ordering/?ref=scopeofwork.net"&gt;the increasing importance of their mobile app&lt;/a&gt;. &lt;a href="https://archive.starbucks.com/record/going-mobile?ref=scopeofwork.net"&gt;Since 2015&lt;/a&gt;, Starbucks has allowed customers to place orders remotely, before they arrive at the store. This grants convenience and flexibility, offloads the labor associated with order entry, and as mentioned previously, encourages customers to give them free loans to earn â€œmobile rewards.â€ But this convenience and savings come at the cost of operational complexity. There are three sales channels at a typical Starbucks today: walk-ins, drive thru, and mobile. Drinks are processed in the order received, whether placed in person or through the mobile app. This first-in, first-out system creates challenges, particularly at busy times. Operational capacity is often devoted to mobile customers who have yet to arrive, while those already at the store grow impatient. The staging area gets crowded with completed drinks, leading to that awkward seek and find many of us have experienced.Â &lt;/p&gt;&lt;p&gt;Though mobile orders create challenges, they also represent a kind of operational opportunity. They are different from orders placed through the traditional sales channels, where customers are present at the restaurant and presumably want their coffee as soon as they can get it. With mobile, customers generally place the order before they arrive and donâ€™t care precisely when it is finished, as long as it is complete, and reasonably fresh, when they get there. This arrival delay makes it sensible to consider processing work &lt;em&gt;out of sequence&lt;/em&gt;.&lt;/p&gt;&lt;h3&gt;Warehouse work&lt;/h3&gt;&lt;p&gt;Back in my warehousing days, we thought a lot about how and when we processed work. Through our website, customers could place orders at any time of the day or night, and we committed to getting them their stuff in two days. The parcel carriers needed most of this time to get the shipment to the customer, but we generally had a few hours to fill the order, pack it, and load it onto the truck. We took advantage of this window to operate more efficiently. A few principles guided our thinking:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Urgent Work First:&lt;/strong&gt; We prioritized packages that had time constraints. If an orderâ€™s truck was leaving soon, weâ€™d complete it first â€” even if there were other orders that had come in before it.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Stay In Sync:&lt;/strong&gt; Many customers ordered more than one thing. Even if the items were stored in areas far from one another, the customer would still expect them to show up in the same box. We would begin processing all components of that order simultaneously and only when we had enough capacity for all the processes involved.Â  As a result, the components of an order would arrive in the packing area at a similar time, where they would be combined and put on a truck. This reduced the amount of incomplete work floating around the warehouse, which in turn reduced opportunities for error.&lt;br/&gt;&lt;br/&gt;A consequence of the above is that simple orders (with fewer work areas involved) tended to move through the system faster. They didnâ€™t get held up behind large and complex shipments that gobbled up lots of warehouse capacity.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Batch Work Where Appropriate&lt;/strong&gt;: The longer the work queue, the more likely you are to see collections of similar jobs in the queue. So we designed work areas to take advantage of this phenomenon, batching jobs that used the same skills and materials.Â &lt;br/&gt;&lt;br/&gt;Imagine a work area where people pack boxes. A packer will complete two identical boxes more quickly and accurately if she does them back to back, as the packages require the same materials (container, tape, infill, and packing surface) and skills. If she needs to shift to some other task in between, say to pack a different type of package, the process will naturally go more slowly. This is pretty much the same principle that makes assembly lines work: Keep people focused on the same task, and they will perform that task more efficiently.&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Starbucks as a Coffee Distribution Center&lt;/h3&gt;&lt;p&gt;The work queues at Starbucks are measured in minutes of work rather than hours. Still, there are a number of warehousing principles they could easily adopt.&lt;/p&gt;




&lt;p&gt;&lt;em&gt;Scope of Work is supported by members who love these kinds of operational deep dives. Join as a paid subscriber to support thoughtful, original work like this.&lt;/em&gt;&lt;/p&gt;

&lt;a href="http://scopeofwork.net/membership?ref=scopeofwork.net"&gt;
                            Support SOW &amp;amp; Unlock More â†’
                        &lt;/a&gt;





&lt;h3&gt;Read the full story&lt;/h3&gt;
&lt;p&gt;
      The rest of this post is for paid members only. Sign up now to read the full post â€” and all of Scope of Workâ€™s other paid posts.
    &lt;/p&gt;
&lt;a href="https://www.scopeofwork.net/signup/"&gt;Sign up now&lt;/a&gt;

Already have an account?
&lt;a href="https://www.scopeofwork.net/signin/"&gt;Sign in&lt;/a&gt;



&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://www.scopeofwork.net/the-coffee-warehouse/</guid>
      <category>Hacker News</category>
      <pubDate>Fri, 19 Dec 2025 20:16:27 +0000</pubDate>
    </item>
    <item>
      <title>Astrophotography Target Planner: Discover Hidden Nebulas</title>
      <link>https://astroimagery.com/techniques/imaging/astrophotography-target-planner/</link>
      <description>Posted by</description>
      <content:encoded>&lt;main class="wp-block-group alignfull no-margin no-padding is-layout-constrained wp-container-core-group-is-layout-da976f36 wp-block-group-is-layout-constrained" style="padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px"&gt;&lt;a href="https://astroimagery.com/techniques/imaging/"&gt;Astro Imaging&lt;/a&gt;&lt;h1&gt;Astrophotography Target Planner: Discover Hidden Nebulas with My New App&lt;/h1&gt;&lt;p&gt;Posted by&lt;/p&gt;&lt;p&gt;Karl Perera MA&lt;/p&gt;&lt;p&gt;â€“&lt;/p&gt;December 17, 2025&lt;p&gt;Have you ever gone out on a clear night, fired up &lt;a href="https://stellarium.org/"&gt;Stellarium&lt;/a&gt;, scrolled through endless objectsâ€¦ and still ended up shooting Andromeda for the seventh time?&lt;/p&gt;&lt;p&gt;That was me, over and over. I love M31, but at some point I realised I wasnâ€™t really exploring the sky anymore â€“ I was just defaulting to the same â€œsafeâ€ ten targets. So I ended up building my own astrophotography target planner. The process of finding something new that was visible, well placed, matched my focal length, and wasnâ€™t completely impossible from my Bortle 5 backyard was just too much friction.&lt;/p&gt;&lt;p&gt;So I built a tool to fix that:&lt;/p&gt;&lt;img alt="Astrophotography target planner app" src="https://astroimagery.com/wp-content/uploads/2025/12/Screenshot-2025-11-22-110231.png"/&gt;How my planner app looks on a desktop&lt;p&gt;My Astrophotography Target Planner helped me discover objects like NGC 7822 and the &lt;a href="https://www.constellation-guide.com/question-mark-nebula-ngc-7822/"&gt;Question Mark Nebula&lt;/a&gt; â€“ targets I genuinely didnâ€™t know existed a few months ago. In this article, Iâ€™ll break down the main ideas from the video so you can see how it works and whether it might actually help you plan your own sessions.&lt;/p&gt;&lt;p&gt;Watch the full walkthrough of my Astrophotography Target Planner here:&lt;/p&gt;
&lt;h3&gt;What Youâ€™ll Learn in This Video&lt;/h3&gt;&lt;p&gt;In the video, I walk through how Iâ€™ve gone from â€œwinging itâ€ every clear night to planning genuinely new targets in minutes.&lt;/p&gt;&lt;p&gt;Youâ€™ll see:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;How I used to plan targets manually with Stellarium (and why I kept ending up on the same objects).&lt;/li&gt;&lt;li&gt;What the Astro Target app does differently â€“ filtering by location, visibility, &lt;a href="https://astroimagery.com/equipment/how-to-choose-the-best-focal-length-for-deep-sky-objects/"&gt;focal length&lt;/a&gt;, and difficulty.&lt;/li&gt;&lt;li&gt;How â€œDiscovery Modeâ€ surfaces lesserâ€‘known nebulae that are actually well suited to your gear.&lt;/li&gt;&lt;li&gt;Examples of hidden gems like NGC 7822, the Headphone Nebula, and the Question Mark Nebula.&lt;/li&gt;&lt;li&gt;How the appâ€™s timing, RA/Dec, and difficulty notes help you decide if a target is realistic from your sky.&lt;/li&gt;&lt;li&gt;Why this change in workflow helped me go from ~10 repeat targets to over 40 different &lt;a href="https://astroimagery.com/astrophotography/deep-space-astrophotography/brightest-deep-sky-objects/"&gt;deep sky objects&lt;/a&gt; in a year.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Key Takeaways&lt;/h3&gt;&lt;h4&gt;From Manual Hunting to Smart Target Selection&lt;/h4&gt;&lt;p&gt;For years, my target selection routine looked like this:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Open Stellarium.&lt;/li&gt;&lt;li&gt;Scroll through a long list of objects.&lt;/li&gt;&lt;li&gt;Check whatâ€™s visible tonight.&lt;/li&gt;&lt;li&gt;Check the moon phase.&lt;/li&gt;&lt;li&gt;Check if it fits my focal length.&lt;/li&gt;&lt;li&gt;Repeat until my patience runs out.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;After 15â€“20 minutes of that, Iâ€™d usually shrug and point the rig back at something familiar. Usually Andromeda. Again.&lt;/p&gt;&lt;p&gt;The problem wasnâ€™t that thereâ€™s a lack of interesting objects â€“ itâ€™s the friction. Every step requires you to crossâ€‘check: altitude, timing, framing, difficulty, and sky conditions. None of that is hard, but together it adds up, especially on a work night when you just want to get something in the can.&lt;/p&gt;&lt;p&gt;Astro Target is basically my way of compressing that whole process into a couple of clicks. You put in:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Your sky quality (for me: suburban Bortle 5),&lt;/li&gt;&lt;li&gt;Your focal length (e.g. 650 mm),&lt;/li&gt;&lt;li&gt;Your camera (sensor size is stored automatically),&lt;/li&gt;&lt;li&gt;What types of targets you want (e.g. Nebula only),&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;â€¦and it just returns a short list of objects that are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Visible from your exact location tonight,&lt;/li&gt;&lt;li&gt;Well framed in your field of view,&lt;/li&gt;&lt;li&gt;Tagged by difficulty (beginner / intermediate / advanced),&lt;/li&gt;&lt;li&gt;And annotated with basic imaging notes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;How This Astrophotography Target Planner Works&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Try the Astrophotography Target Planner (beta) here:&lt;/strong&gt;&lt;/p&gt;&lt;a href="https://astrotarget-planner-30718389249.us-west1.run.app/"&gt;AstroTarget Planner&lt;/a&gt;&lt;h4&gt;What â€œDiscovery Modeâ€ Actually Does&lt;/h4&gt;&lt;p&gt;The real game changer for me has been Discovery Mode.&lt;/p&gt;&lt;p&gt;Instead of showing you the usual greatest hits, Discovery Mode prioritises objects that almost nobody photographs â€“ the hidden gems that are still practical for a typical backyard rig.&lt;/p&gt;&lt;p&gt;A few examples that popped up for me:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;NGC 7822&lt;/strong&gt; â€“ a beautiful emission nebula Iâ€™d literally never thought about imaging before.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;The Headphone Nebula (Jonesâ€“Emberson 1)&lt;/strong&gt; â€“ an object Iâ€™d never heard of until the astrophotography target planner for deep sky objects surfaced it.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;The Witch Head Nebula (IC 2118)&lt;/strong&gt; â€“ a large, faint reflection nebula that matches my FOV surprisingly well.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;The Question Mark Nebula&lt;/strong&gt; â€“ another one of those â€œhow did I not know this existed?â€ moments.&lt;/li&gt;&lt;/ul&gt;&lt;img alt="Results from the astrophotography target planning app" src="https://astroimagery.com/wp-content/uploads/2025/12/Untitled-7-6-1024x576.png"/&gt;A few targets I found on my planning app&lt;p&gt;For each object, this planner for new astrophotography targets gives you:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The optimal imaging time for &lt;em&gt;tonight&lt;/em&gt; (e.g. â€œBest around 4:00 a.m. when itâ€™s highest in the skyâ€).&lt;/li&gt;&lt;li&gt;RA/Dec coordinates so you can slew to it easily.&lt;/li&gt;&lt;li&gt;A size estimate and how it frames in your setup.&lt;/li&gt;&lt;li&gt;A difficulty rating and notes like â€œfaint, requires many hours of total exposure from Bortle 5â€ or â€œlarge, challenging to separate from sky glow.â€&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;That last bit is important. Itâ€™s honest about whether a target is going to be a slog in mediocre skies. You still have to decide if itâ€™s worth the effort, but at least youâ€™re making that decision up front instead of after three wasted hours.&lt;/p&gt;&lt;h4&gt;How This Changed My Imaging Year&lt;/h4&gt;&lt;p&gt;The difference in practice has been big.&lt;/p&gt;&lt;p&gt;When I did things manually, I timed it: about &lt;strong&gt;13 minutes&lt;/strong&gt; just to end up on a target Iâ€™d already shot before.&lt;/p&gt;&lt;p&gt;With Discovery Mode, it took about &lt;strong&gt;85 seconds&lt;/strong&gt; to find something entirely new that:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Was visible that night,&lt;/li&gt;&lt;li&gt;Fit my 650 mm FOV,&lt;/li&gt;&lt;li&gt;And wasnâ€™t completely insane to attempt from Bortle 5.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Thatâ€™s how I went from essentially cycling through the same 10 comfortable objectsâ€¦ to aiming for &lt;strong&gt;40+ different targets&lt;/strong&gt; in a year.&lt;/p&gt;&lt;p&gt;To put it another way:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;This is my &lt;strong&gt;seventh&lt;/strong&gt; Andromeda image.&lt;/li&gt;&lt;li&gt;This is my &lt;strong&gt;first&lt;/strong&gt; Question Mark Nebula.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Both are valid choices. But one of them feels like genuine exploration again.&lt;/p&gt;&lt;p&gt;And that, more than anything, is what I wanted back â€“ the sense that Iâ€™m still discovering things in the night sky, not just perfecting the same photos over and over.&lt;/p&gt;&lt;p&gt;A couple of notes for transparency:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The target planner app is still &lt;strong&gt;in beta&lt;/strong&gt;. I built it primarily for myself because I was frustrated with the manual workflow.&lt;/li&gt;&lt;li&gt;It currently works &lt;strong&gt;best for northern hemisphere&lt;/strong&gt; targets. Iâ€™m actively adding more southern targets.&lt;/li&gt;&lt;li&gt;Itâ€™s &lt;strong&gt;free while Iâ€™m testing it&lt;/strong&gt;. The link is in the video description (and Iâ€™ll add it here too when this goes live).&lt;/li&gt;&lt;li&gt;If you find bugs or have ideas, I genuinely want to hear them â€“ Iâ€™m shaping it around what you actually need out there.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Gear Used in This Video&lt;/h3&gt;&lt;p&gt;Some of the links I use are affiliate links, which means they help support the channel at no extra cost to you. I only recommend gear I actually use in my own imaging.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Imaging Rig&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Lens: &lt;strong&gt;Samyang 135mm F2.0&lt;/strong&gt; â€“ &lt;a href="https://amzn.to/3KYXUkn"&gt;Check on Amazon&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Mount: &lt;strong&gt;Ioptron CEM26&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;Imaging camera: &lt;strong&gt;ZWO ASI533 MCPRO&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;Guide scope: &lt;strong&gt;SVBony 60mm&lt;/strong&gt; â€“ &lt;a href="https://amzn.to/4j3ESWNhttps://amzn.to/4j3ESWN"&gt;Check same model by Astromania&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Guide camera: &lt;strong&gt;SV305&lt;/strong&gt; â€“ &lt;a href="https://amzn.to/4qlUZS5"&gt;Check on Amazon&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Control &amp;amp; Accessories&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Filter(s) : &lt;strong&gt;Optolong LPro&lt;/strong&gt; â€“ &lt;a href="https://amzn.to/44y5ucq"&gt;Check on Amazon&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Software&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Astro Target app (Discovery Mode) â€“ &lt;a href="https://astrotarget-planner-30718389249.us-west1.run.app/"&gt;AstroTarget Planner&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Stellarium (for comparison / star hopping) â€“ &lt;a href="https://stellarium.org/"&gt;Stellarium Astronomy Software&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Feel free to swap these for your own equivalents â€“ the app doesnâ€™t require this exact setup, it just needs your focal length and sensor size to do its thing.&lt;/p&gt;&lt;h3&gt;What to Watch / Read Next&lt;/h3&gt;&lt;p&gt;If this kind of planning and target discovery is interesting to you, here are some related topics Iâ€™d recommend exploring next:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A beginnerâ€‘friendly guide to choosing your &lt;strong&gt;&lt;a href="https://astroimagery.com/astrophotography/deep-space-astrophotography/brightest-deep-sky-objects/"&gt;first five deep sky targets&lt;/a&gt;&lt;/strong&gt; from lightâ€‘polluted skies.&lt;/li&gt;&lt;li&gt;A breakdown of &lt;a href="https://astroimagery.com/equipment/telescopes/light-pollution-filter-comparison-for-astrophotography/"&gt;&lt;strong&gt;light pollution vs. narrowband vs. broadband imaging&lt;/strong&gt; &lt;/a&gt;and when each makes sense.&lt;/li&gt;&lt;li&gt;A â€œreal worldâ€ comparison of &lt;strong&gt;&lt;a href="https://astroimagery.com/astrophotography/deep-space-astrophotography/nebula-photography-guide/"&gt;different focal lengths for nebulae&lt;/a&gt;&lt;/strong&gt; â€“ what 135 mm, and 650 mm actually look like on the same target.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Iâ€™ll link to the relevant videos and articles once this post is live so you can dive straight in.&lt;/p&gt;&lt;h3&gt;A Small Way to Carry the Night Sky with You&lt;/h3&gt;&lt;p&gt;Most of the time, our astrophotography lives on hard drives, in processing projects, or as big prints that only a few people ever see. Thatâ€™s great, but I also like having a quieter, everyday reminder of the night sky.&lt;/p&gt;&lt;a href="https://astroimagery.com/astroimagery-shop/"&gt;&lt;img src="https://astroimagery.com/wp-content/uploads/2025/12/Screenshot-2025-12-16-215849-1-152x300.png"/&gt;&lt;/a&gt;&lt;p&gt;Thatâ€™s why I started turning some of my favourite Astroimagery captures into &lt;strong&gt;phone cases&lt;/strong&gt;. Theyâ€™re not shouty or overâ€‘branded â€“ just subtle, highâ€‘resolution slices of nebulae and galaxies you can literally carry around in your hand.&lt;/p&gt;&lt;p&gt;If youâ€™d like a small, practical way to keep a bit of the night sky with you during the day, have a look here:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Browse the Astroimagery phone cases here: &lt;/strong&gt;&lt;a href="https://astroimagery.com/astroimagery-shop/"&gt;Astroimagery Shop&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
  &lt;img src="https://secure.gravatar.com/avatar/b335d8c790c525812a40fe540abf1a93e70e3fdc9cffecabe0b1817804633194?s=48&amp;amp;d=mm&amp;amp;r=g"/&gt;&lt;p&gt;Karl Perera MA&lt;/p&gt;Iâ€™m Karl Perera, an experienced astrophotographer, author, and blogger with a masterâ€™s degree in teaching. Iâ€™m a member of the British Astronomy Association. Welcome!
  &lt;h3&gt;&lt;strong&gt;Follow&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.instagram.com/karl_astroimagery/"&gt;karl_astroimagery&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.youtube.com/@Astroimagery"&gt;@Astroimagery&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.linkedin.com/company/astroimagery/"&gt;Astroimagery&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;strong&gt;Recent Posts&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://astroimagery.com/techniques/imaging/astrophotography-target-planner/"&gt;Astrophotography Target Planner: Discover Hidden Nebulas with My New App&lt;/a&gt;December 17, 2025&lt;/li&gt;&lt;li&gt;&lt;a href="https://astroimagery.com/techniques/post-processing/pleiades-two-filters/"&gt;&lt;img alt="Pleiades two filters workflow" src="https://astroimagery.com/wp-content/uploads/2025/12/Untitled-3-22-150x150.png"/&gt;&lt;/a&gt;&lt;a href="https://astroimagery.com/techniques/post-processing/pleiades-two-filters/"&gt;Better Images with Two Filters for the Pleiades (At 135mm)&lt;/a&gt;December 16, 2025&lt;/li&gt;&lt;li&gt;&lt;a href="https://astroimagery.com/techniques/astrophotography-skills-and-techniques/"&gt;&lt;img alt="astrophotography skills and techniques - shooting the Milky Way" src="https://astroimagery.com/wp-content/uploads/2025/11/astrophotography-skills-and-techniques-150x150.jpg"/&gt;&lt;/a&gt;&lt;a href="https://astroimagery.com/techniques/astrophotography-skills-and-techniques/"&gt;Master Astrophotography Skills and Techniques&lt;/a&gt;November 6, 2025&lt;/li&gt;&lt;li&gt;&lt;a href="https://astroimagery.com/astrophotography/deep-space-astrophotography/nebula-photography-guide/"&gt;Nebula Photography the Best Gear, Filters, and Processing&lt;/a&gt;November 1, 2025&lt;/li&gt;&lt;li&gt;&lt;a href="https://astroimagery.com/uncategorized/astrophotography-update/"&gt;Astrophotography Update: New Deepâ€‘Sky Images, Special Offers, and Print Releases | AstroImagery&lt;/a&gt;October 21, 2025&lt;/li&gt;&lt;/ul&gt;&lt;/main&gt;</content:encoded>
      <guid isPermaLink="false">https://astroimagery.com/techniques/imaging/astrophotography-target-planner/</guid>
      <category>Hacker News</category>
      <pubDate>Fri, 19 Dec 2025 19:44:41 +0000</pubDate>
    </item>
    <item>
      <title>Adobe Photoshop 1.0 Source Code (1990)</title>
      <link>https://computerhistory.org/blog/adobe-photoshop-source-code/</link>
      <description>Adobe Photoshop Source Code - CHM</description>
      <content:encoded>&lt;div class="right-content"&gt;

&lt;a href="https://donate.computerhistory.org"&gt;Donate&lt;/a&gt;
&lt;a href="https://connect.computerhistory.org"&gt;Tickets&lt;/a&gt;


Menu

&lt;/div&gt;</content:encoded>
      <guid isPermaLink="false">https://computerhistory.org/blog/adobe-photoshop-source-code/</guid>
      <category>Hacker News</category>
      <pubDate>Thu, 18 Dec 2025 15:37:18 +0000</pubDate>
    </item>
    <item>
      <title>Executorch: On-device AI across mobile, embedded and edge for PyTorch</title>
      <link>https://github.com/pytorch/executorch</link>
      <description>On-device AI inference powered by PyTorch</description>
      <content:encoded>&lt;article class="markdown-body entry-content container-lg" itemprop="text"&gt;
&lt;a href="https://github.com/pytorch/executorch/blob/main/docs/source/_static/img/et-logo.png"&gt;&lt;img alt="ExecuTorch logo mark" src="https://github.com/pytorch/executorch/raw/main/docs/source/_static/img/et-logo.png"/&gt;&lt;/a&gt;
&lt;h1&gt;ExecuTorch&lt;/h1&gt;&lt;a href="#executorch"&gt;&lt;/a&gt;
&lt;p&gt;&lt;strong&gt;On-device AI inference powered by PyTorch&lt;/strong&gt;&lt;/p&gt;


&lt;a href="https://pypi.org/project/executorch/"&gt;&lt;img alt="PyPI - Version" src="https://camo.githubusercontent.com/0a655d9cfc2a1d6c3f54fba80bc87ebea4321ade5755afd0e8dada56fe35f159/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6578656375746f7263683f7374796c653d666f722d7468652d626164676526636f6c6f723d626c7565"/&gt;&lt;/a&gt;
&lt;a href="https://github.com/pytorch/executorch/graphs/contributors"&gt;&lt;img alt="GitHub - Contributors" src="https://camo.githubusercontent.com/629c378cadd0fd62df5cf89107f8bc8b08b3f8d4fc5b939bde95d7acb9049cdb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f7079746f7263682f6578656375746f7263683f7374796c653d666f722d7468652d626164676526636f6c6f723d626c7565"/&gt;&lt;/a&gt;
&lt;a href="https://github.com/pytorch/executorch/stargazers"&gt;&lt;img alt="GitHub - Stars" src="https://camo.githubusercontent.com/461ec8564ce148802f8e6dc5bde3bc70997ad25ecc838c4c7b9813efd9533db4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7079746f7263682f6578656375746f7263683f7374796c653d666f722d7468652d626164676526636f6c6f723d626c7565"/&gt;&lt;/a&gt;
&lt;a href="https://discord.gg/Dh43CKSAdc"&gt;&lt;img alt="Discord - Chat with Us" src="https://camo.githubusercontent.com/5665a739b7459f532d6d1bdb198268464b4e52bbfa6f28b2f36bcd159467db62/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d4a6f696e25323055732d626c75653f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465267374796c653d666f722d7468652d6261646765"/&gt;&lt;/a&gt;
&lt;a href="https://docs.pytorch.org/executorch/main/index.html"&gt;&lt;img alt="Documentation" src="https://camo.githubusercontent.com/4ea00c7ce642fa2d5117120b3237ef4e7f310fdb8f96c6a7ed607d215348dcf9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f63756d656e746174696f6e2d626c75653f6c6f676f3d676f6f676c65646f6373266c6f676f436f6c6f723d7768697465267374796c653d666f722d7468652d6261646765"/&gt;&lt;/a&gt;

&lt;p&gt;&lt;strong&gt;ExecuTorch&lt;/strong&gt; is PyTorch's unified solution for deploying AI models on-deviceâ€”from smartphones to microcontrollersâ€”built for privacy, performance, and portability. It powers Meta's on-device AI across &lt;strong&gt;Instagram, WhatsApp, Quest 3, Ray-Ban Meta Smart Glasses&lt;/strong&gt;, and &lt;a href="https://docs.pytorch.org/executorch/main/success-stories.html"&gt;more&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Deploy &lt;strong&gt;LLMs, vision, speech, and multimodal models&lt;/strong&gt; with the same PyTorch APIs you already knowâ€”accelerating research to production with seamless model export, optimization, and deployment. No manual C++ rewrites. No format conversions. No vendor lock-in.&lt;/p&gt;

&lt;strong&gt;ğŸ“˜ Table of Contents&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#why-executorch"&gt;Why ExecuTorch?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-it-works"&gt;How It Works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#quick-start"&gt;Quick Start&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#export-and-deploy-in-3-steps"&gt;Export and Deploy in 3 Steps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#run-on-device"&gt;Run on Device&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#llm-example-llama"&gt;LLM Example: Llama&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#platform--hardware-support"&gt;Platform &amp;amp; Hardware Support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#production-deployments"&gt;Production Deployments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples--models"&gt;Examples &amp;amp; Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#community--contributing"&gt;Community &amp;amp; Contributing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Why ExecuTorch?&lt;/h2&gt;&lt;a href="#why-executorch"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ğŸ”’ Native PyTorch Export&lt;/strong&gt; â€” Direct export from PyTorch. No .onnx, .tflite, or intermediate format conversions. Preserve model semantics.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;âš¡ Production-Proven&lt;/strong&gt; â€” Powers billions of users at &lt;a href="https://engineering.fb.com/2025/07/28/android/executorch-on-device-ml-meta-family-of-apps/"&gt;Meta with real-time on-device inference&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ğŸ’¾ Tiny Runtime&lt;/strong&gt; â€” 50KB base footprint. Runs on microcontrollers to high-end smartphones.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ğŸš€ &lt;a href="https://docs.pytorch.org/executorch/main/backends-overview.html"&gt;12+ Hardware Backends&lt;/a&gt;&lt;/strong&gt; â€” Open-source acceleration for Apple, Qualcomm, ARM, MediaTek, Vulkan, and more.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ğŸ¯ One Export, Multiple Backends&lt;/strong&gt; â€” Switch hardware targets with a single line change. Deploy the same model everywhere.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;How It Works&lt;/h2&gt;&lt;a href="#how-it-works"&gt;&lt;/a&gt;
&lt;p&gt;ExecuTorch uses &lt;strong&gt;ahead-of-time (AOT) compilation&lt;/strong&gt; to prepare PyTorch models for edge deployment:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;ğŸ§© Export&lt;/strong&gt; â€” Capture your PyTorch model graph with &lt;code&gt;torch.export()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;âš™ï¸ Compile&lt;/strong&gt; â€” Quantize, optimize, and partition to hardware backends â†’ &lt;code&gt;.pte&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ğŸš€ Execute&lt;/strong&gt; â€” Load &lt;code&gt;.pte&lt;/code&gt; on-device via lightweight C++ runtime&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Models use a standardized &lt;a href="https://docs.pytorch.org/executorch/main/compiler-ir-advanced.html#intermediate-representation"&gt;Core ATen operator set&lt;/a&gt;. &lt;a href="https://docs.pytorch.org/executorch/main/compiler-delegate-and-partitioner.html"&gt;Partitioners&lt;/a&gt; delegate subgraphs to specialized hardware (NPU/GPU) with CPU fallback.&lt;/p&gt;
&lt;p&gt;Learn more: &lt;a href="https://docs.pytorch.org/executorch/main/intro-how-it-works.html"&gt;How ExecuTorch Works&lt;/a&gt; â€¢ &lt;a href="https://docs.pytorch.org/executorch/main/getting-started-architecture.html"&gt;Architecture Guide&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Quick Start&lt;/h2&gt;&lt;a href="#quick-start"&gt;&lt;/a&gt;
&lt;h3&gt;Installation&lt;/h3&gt;&lt;a href="#installation"&gt;&lt;/a&gt;
&lt;pre&gt;pip install executorch&lt;/pre&gt;
&lt;p&gt;For platform-specific setup (Android, iOS, embedded systems), see the &lt;a href="https://docs.pytorch.org/executorch/main/quick-start-section.html"&gt;Quick Start&lt;/a&gt; documentation for additional info.&lt;/p&gt;
&lt;h3&gt;Export and Deploy in 3 Steps&lt;/h3&gt;&lt;a href="#export-and-deploy-in-3-steps"&gt;&lt;/a&gt;
&lt;pre&gt;import torch
from executorch.exir import to_edge_transform_and_lower
from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner

# 1. Export your PyTorch model
model = MyModel().eval()
example_inputs = (torch.randn(1, 3, 224, 224),)
exported_program = torch.export.export(model, example_inputs)

# 2. Optimize for target hardware (switch backends with one line)
program = to_edge_transform_and_lower(
    exported_program,
    partitioner=[XnnpackPartitioner()]  # CPU | CoreMLPartitioner() for iOS | QnnPartitioner() for Qualcomm
).to_executorch()

# 3. Save for deployment
with open("model.pte", "wb") as f:
    f.write(program.buffer)

# Test locally via ExecuTorch runtime's pybind API (optional)
from executorch.runtime import Runtime
runtime = Runtime.get()
method = runtime.load_program("model.pte").load_method("forward")
outputs = method.execute([torch.randn(1, 3, 224, 224)])&lt;/pre&gt;
&lt;h3&gt;Run on Device&lt;/h3&gt;&lt;a href="#run-on-device"&gt;&lt;/a&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pytorch.org/executorch/main/using-executorch-cpp.html"&gt;C++&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;#include &amp;lt;executorch/extension/module/module.h&amp;gt;
#include &amp;lt;executorch/extension/tensor/tensor.h&amp;gt;

Module module("model.pte");
auto tensor = make_tensor_ptr({2, 2}, {1.0f, 2.0f, 3.0f, 4.0f});
auto outputs = module.forward(tensor);&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pytorch.org/executorch/main/ios-section.html"&gt;Swift (iOS)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;import ExecuTorch

let module = Module(filePath: "model.pte")
let input = Tensor&amp;lt;Float&amp;gt;([1.0, 2.0, 3.0, 4.0], shape: [2, 2])
let outputs = try module.forward(input)&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pytorch.org/executorch/main/android-section.html"&gt;Kotlin (Android)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;val module = Module.load("model.pte")
val inputTensor = Tensor.fromBlob(floatArrayOf(1.0f, 2.0f, 3.0f, 4.0f), longArrayOf(2, 2))
val outputs = module.forward(EValue.from(inputTensor))&lt;/pre&gt;
&lt;h3&gt;LLM Example: Llama&lt;/h3&gt;&lt;a href="#llm-example-llama"&gt;&lt;/a&gt;
&lt;p&gt;Export Llama models using the &lt;a href="https://docs.pytorch.org/executorch/main/llm/export-llm.html"&gt;&lt;code&gt;export_llm&lt;/code&gt;&lt;/a&gt; script or &lt;a href="https://github.com/huggingface/optimum-executorch"&gt;Optimum-ExecuTorch&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;# Using export_llm
python -m executorch.extension.llm.export.export_llm --model llama3_2 --output llama.pte

# Using Optimum-ExecuTorch
optimum-cli export executorch \
  --model meta-llama/Llama-3.2-1B \
  --task text-generation \
  --recipe xnnpack \
  --output_dir llama_model&lt;/pre&gt;
&lt;p&gt;Run on-device with the LLM runner API:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pytorch.org/executorch/main/llm/run-with-c-plus-plus.html"&gt;C++&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;#include &amp;lt;executorch/extension/llm/runner/text_llm_runner.h&amp;gt;

auto runner = create_llama_runner("llama.pte", "tiktoken.bin");
executorch::extension::llm::GenerationConfig config{
    .seq_len = 128, .temperature = 0.8f};
runner-&amp;gt;generate("Hello, how are you?", config);&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pytorch.org/executorch/main/llm/run-on-ios.html"&gt;Swift (iOS)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;import ExecuTorchLLM

let runner = TextRunner(modelPath: "llama.pte", tokenizerPath: "tiktoken.bin")
try runner.generate("Hello, how are you?", Config {
    $0.sequenceLength = 128
}) { token in
    print(token, terminator: "")
}&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Kotlin (Android)&lt;/strong&gt; â€” &lt;a href="https://docs.pytorch.org/executorch/main/javadoc/org/pytorch/executorch/extension/llm/package-summary.html"&gt;API Docs&lt;/a&gt; â€¢ &lt;a href="https://github.com/meta-pytorch/executorch-examples/tree/main/llm/android/LlamaDemo"&gt;Demo App&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;val llmModule = LlmModule("llama.pte", "tiktoken.bin", 0.8f)
llmModule.load()
llmModule.generate("Hello, how are you?", 128, object : LlmCallback {
    override fun onResult(result: String) { print(result) }
    override fun onStats(stats: String) { }
})&lt;/pre&gt;
&lt;p&gt;For multimodal models (vision, audio), use the &lt;a href="https://github.com/pytorch/executorch/blob/main/extension/llm/runner"&gt;MultiModal runner API&lt;/a&gt; which extends the LLM runner to handle image and audio inputs alongside text. See &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/llava/README.md"&gt;Llava&lt;/a&gt; and &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/voxtral/README.md"&gt;Voxtral&lt;/a&gt; examples.&lt;/p&gt;
&lt;p&gt;See &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/llama/README.md"&gt;examples/models/llama&lt;/a&gt; for complete workflow including quantization, mobile deployment, and advanced options.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next Steps:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ“– &lt;a href="https://docs.pytorch.org/executorch/main/getting-started.html"&gt;Step-by-step tutorial&lt;/a&gt; â€” Complete walkthrough for your first model&lt;/li&gt;
&lt;li&gt;âš¡ &lt;a href="https://colab.research.google.com/drive/1qpxrXC3YdJQzly3mRg-4ayYiOjC6rue3?usp=sharing"&gt;Colab notebook&lt;/a&gt; â€” Try ExecuTorch instantly in your browser&lt;/li&gt;
&lt;li&gt;ğŸ¤– &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/llama/README.md"&gt;Deploy Llama models&lt;/a&gt; â€” LLM workflow with quantization and mobile demos&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Platform &amp;amp; Hardware Support&lt;/h2&gt;&lt;a href="#platform--hardware-support"&gt;&lt;/a&gt;



&lt;strong&gt;Platform&lt;/strong&gt;
&lt;strong&gt;Supported Backends&lt;/strong&gt;




Android
XNNPACK, Vulkan, Qualcomm, MediaTek, Samsung Exynos


iOS
XNNPACK, MPS, CoreML (Neural Engine)


Linux / Windows
XNNPACK, OpenVINO, CUDA &lt;em&gt;(experimental)&lt;/em&gt;


macOS
XNNPACK, MPS, Metal &lt;em&gt;(experimental)&lt;/em&gt;


Embedded / MCU
XNNPACK, ARM Ethos-U, NXP, Cadence DSP



&lt;p&gt;See &lt;a href="https://docs.pytorch.org/executorch/main/backends-overview.html"&gt;Backend Documentation&lt;/a&gt; for detailed hardware requirements and optimization guides. For desktop/laptop GPU inference with CUDA and Metal, see the &lt;a href="https://github.com/pytorch/executorch/blob/main/desktop/README.md"&gt;Desktop Guide&lt;/a&gt;. For Zephyr RTOS integration, see the &lt;a href="https://github.com/pytorch/executorch/blob/main/zephyr/README.md"&gt;Zephyr Guide&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Production Deployments&lt;/h2&gt;&lt;a href="#production-deployments"&gt;&lt;/a&gt;
&lt;p&gt;ExecuTorch powers on-device AI at scale across Meta's family of apps, VR/AR devices, and partner deployments. &lt;a href="https://docs.pytorch.org/executorch/main/success-stories.html"&gt;View success stories â†’&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Examples &amp;amp; Models&lt;/h2&gt;&lt;a href="#examples--models"&gt;&lt;/a&gt;
&lt;p&gt;&lt;strong&gt;LLMs:&lt;/strong&gt; &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/llama/README.md"&gt;Llama 3.2/3.1/3&lt;/a&gt;, &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/qwen3/README.md"&gt;Qwen 3&lt;/a&gt;, &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/phi_4_mini/README.md"&gt;Phi-4-mini&lt;/a&gt;, &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/lfm2/README.md"&gt;LiquidAI LFM2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multimodal:&lt;/strong&gt; &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/llava/README.md"&gt;Llava&lt;/a&gt; (vision-language), &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/voxtral/README.md"&gt;Voxtral&lt;/a&gt; (audio-language), &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/gemma3"&gt;Gemma&lt;/a&gt; (vision-language)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vision/Speech:&lt;/strong&gt; &lt;a href="https://github.com/meta-pytorch/executorch-examples/tree/main/mv2"&gt;MobileNetV2&lt;/a&gt;, &lt;a href="https://github.com/meta-pytorch/executorch-examples/tree/main/dl3"&gt;DeepLabV3&lt;/a&gt;, &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/whisper/README.md"&gt;Whisper&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt; &lt;a href="https://github.com/pytorch/executorch/blob/main/examples"&gt;&lt;code&gt;examples/&lt;/code&gt;&lt;/a&gt; directory â€¢ &lt;a href="https://github.com/meta-pytorch/executorch-examples"&gt;executorch-examples&lt;/a&gt; out-of-tree demos â€¢ &lt;a href="https://github.com/huggingface/optimum-executorch"&gt;Optimum-ExecuTorch&lt;/a&gt; for HuggingFace models â€¢ &lt;a href="https://docs.unsloth.ai/new/deploy-llms-phone"&gt;Unsloth&lt;/a&gt; for fine-tuned LLM deployment &lt;/p&gt;
&lt;h2&gt;Key Features&lt;/h2&gt;&lt;a href="#key-features"&gt;&lt;/a&gt;
&lt;p&gt;ExecuTorch provides advanced capabilities for production deployment:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Quantization&lt;/strong&gt; â€” Built-in support via &lt;a href="https://docs.pytorch.org/ao"&gt;torchao&lt;/a&gt; for 8-bit, 4-bit, and dynamic quantization&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory Planning&lt;/strong&gt; â€” Optimize memory usage with ahead-of-time allocation strategies&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Developer Tools&lt;/strong&gt; â€” ETDump profiler, ETRecord inspector, and model debugger&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Selective Build&lt;/strong&gt; â€” Strip unused operators to minimize binary size&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Custom Operators&lt;/strong&gt; â€” Extend with domain-specific kernels&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Shapes&lt;/strong&gt; â€” Support variable input sizes with bounded ranges&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See &lt;a href="https://docs.pytorch.org/executorch/main/advanced-topics-section.html"&gt;Advanced Topics&lt;/a&gt; for quantization techniques, custom backends, and compiler passes.&lt;/p&gt;
&lt;h2&gt;Documentation&lt;/h2&gt;&lt;a href="#documentation"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.pytorch.org/executorch/main/index.html"&gt;&lt;strong&gt;Documentation Home&lt;/strong&gt;&lt;/a&gt; â€” Complete guides and tutorials&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.pytorch.org/executorch/main/api-section.html"&gt;&lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt; â€” Python, C++, Java/Kotlin APIs&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.pytorch.org/executorch/main/backend-delegates-integration.html"&gt;&lt;strong&gt;Backend Integration&lt;/strong&gt;&lt;/a&gt; â€” Build custom hardware backends&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.pytorch.org/executorch/main/support-section.html"&gt;&lt;strong&gt;Troubleshooting&lt;/strong&gt;&lt;/a&gt; â€” Common issues and solutions&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Community &amp;amp; Contributing&lt;/h2&gt;&lt;a href="#community--contributing"&gt;&lt;/a&gt;
&lt;p&gt;We welcome contributions from the community!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ’¬ &lt;a href="https://github.com/pytorch/executorch/discussions"&gt;&lt;strong&gt;GitHub Discussions&lt;/strong&gt;&lt;/a&gt; â€” Ask questions and share ideas&lt;/li&gt;
&lt;li&gt;ğŸ® &lt;a href="https://discord.gg/Dh43CKSAdc"&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt; â€” Chat with the team and community&lt;/li&gt;
&lt;li&gt;ğŸ› &lt;a href="https://github.com/pytorch/executorch/issues"&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/a&gt; â€” Report bugs or request features&lt;/li&gt;
&lt;li&gt;ğŸ¤ &lt;a href="https://github.com/pytorch/executorch/blob/main/CONTRIBUTING.md"&gt;&lt;strong&gt;Contributing Guide&lt;/strong&gt;&lt;/a&gt; â€” Guidelines and codebase structure&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;License&lt;/h2&gt;&lt;a href="#license"&gt;&lt;/a&gt;
&lt;p&gt;ExecuTorch is BSD licensed, as found in the &lt;a href="https://github.com/pytorch/executorch/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Part of the PyTorch ecosystem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href="https://github.com/pytorch/executorch"&gt;GitHub&lt;/a&gt; â€¢
    &lt;a href="https://docs.pytorch.org/executorch"&gt;Documentation&lt;/a&gt;
&lt;/p&gt;

&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://github.com/pytorch/executorch</guid>
      <category>Hacker News</category>
      <pubDate>Thu, 18 Dec 2025 13:51:11 +0000</pubDate>
    </item>
    <item>
      <title>Partial inlining</title>
      <link>https://xania.org/202512/18-partial-inlining</link>
      <description>Written by me, proof-read by an LLM. Details at end.</description>
      <content:encoded>&lt;div class="small-12 columns article"&gt;
&lt;h2&gt;Partial inlining&lt;/h2&gt;
&lt;p&gt;Written by me, proof-read by an LLM.
&lt;br/&gt;Details at end.&lt;/p&gt;
&lt;p&gt;Weâ€™ve learned how important inlining is to optimisation, but also that it might sometimes cause code bloat. Inlining doesnâ€™t have to be all-or-nothing!&lt;/p&gt;
&lt;p&gt;Letâ€™s look at a simple function that has a fast path and slow path; and then see how the compiler handles it&lt;a href="#fn:note"&gt;1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this example we have some &lt;code&gt;process&lt;/code&gt; function that has a really trivial fast case for numbers in the range 0-100. For other numbers it does something more expensive. Then &lt;code&gt;compute&lt;/code&gt; calls &lt;code&gt;process&lt;/code&gt; twice (making it less appealing to inline all of &lt;code&gt;process&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Looking at the assembly output, we see whatâ€™s happened: The compiler has split &lt;code&gt;process&lt;/code&gt; into two functions, a &lt;code&gt;process (part.0)&lt;/code&gt; that does the expensive part only. It then rewrites &lt;code&gt;process&lt;/code&gt; into the quick check for 100, returning double the value if less than 100. If not, it jumps to the &lt;code&gt;(part.0)&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process(unsigned int):
  cmp edi, 99                           ; less than or equal to 99?
  jbe .L7                               ; skip to fast path if so
  jmp process(unsigned int) (.part.0)   ; else jump to the expensive path
.L7:
  lea eax, [rdi+rdi]                    ; return `value * 2`
  ret
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This first step - extracting the cold path into a separate function - is called &lt;strong&gt;function outlining&lt;/strong&gt;. The original &lt;code&gt;process&lt;/code&gt; becomes a thin wrapper handling the hot path, delegating to the outlined &lt;code&gt;process (.part.0)&lt;/code&gt; when needed. This split sets up the real trick: &lt;strong&gt;partial inlining&lt;/strong&gt;. When the compiler later inlines &lt;code&gt;process&lt;/code&gt; into &lt;code&gt;compute&lt;/code&gt;, it inlines just the wrapper whilst keeping calls to the outlined cold path. External callers can still call &lt;code&gt;process&lt;/code&gt; and have it work correctly for all values.&lt;/p&gt;
&lt;p&gt;Letâ€™s see this optimisation in action in the &lt;code&gt;compute&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;compute(unsigned int, unsigned int):
  cmp edi, 99                   ; is a &amp;lt;= 99?
  jbe .L13                      ; if so, go to the inlined fast path for a
  call process(unsigned int) (.part.0) ; else, call expensive case
  mov r8d, eax                  ; save the result of process(a)
  cmp esi, 99                   ; is b &amp;lt;= 99?
  jbe .L14                      ; if so go to the inlined fast path for b
.L11:
  mov edi, esi                  ; otherwise, call expensive case for b
  call process(unsigned int) (.part.0)
  add eax, r8d                  ; add the two slow cases together
  ret                           ; return

.L13:                           ; case where a is fast case
  lea r8d, [rdi+rdi]            ; process(a) is just a + a
  cmp esi, 99                   ; is b &amp;gt; 99?
  ja .L11                       ; jump to b slow case if so
                                ; (falls through to...)
.L14:                           ; b fast case
  lea eax, [rsi+rsi]            ; double b
  add eax, r8d                  ; return 2*a + 2*b
  ret
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at &lt;code&gt;compute&lt;/code&gt;, we can see the benefits of this approach clearly: The simple range check and arithmetic (&lt;code&gt;cmp&lt;/code&gt;, &lt;code&gt;lea&lt;/code&gt;) are inlined directly, avoiding the function call overhead for the fast path. When a value is 100 or greater, it calls the outlined &lt;code&gt;process (.part.0)&lt;/code&gt; function for the more expensive computation.&lt;/p&gt;
&lt;p&gt;This is the best of both worlds: we get the performance benefit of inlining the lightweight check and simple arithmetic, whilst avoiding code bloat from duplicating the expensive computation&lt;a href="#fn:again"&gt;2&lt;/a&gt;. The original &lt;code&gt;process&lt;/code&gt; function remains intact and callable, so external callers still work correctly.&lt;/p&gt;
&lt;p&gt;Partial inlining lets the compiler make nuanced trade-offs about what to inline and what to keep shared. The compiler can outline portions of a function based on its heuristics about code size and performance&lt;a href="#fn:tradeoff"&gt;3&lt;/a&gt;, giving you benefits of inlining without necessarily paying the full code size cost. In this example, the simple check is duplicated whilst the complex computation stays shared.&lt;/p&gt;
&lt;p&gt;As with many optimisations, the compilerâ€™s heuristics&lt;a href="#fn:clang"&gt;4&lt;/a&gt; usually make reasonable choices about when to apply partial inlining, but itâ€™s worth checking your hot code paths to see if the compiler has made the decisions you expect. Taking a quick peek in &lt;a href="https://godbolt.org"&gt;Compiler Explorer&lt;/a&gt; is a good way to develop your intuition.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;See &lt;a href="https://youtu.be/STZb5K5sPDs"&gt;the video&lt;/a&gt; that accompanies this post.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This post is day 18 of &lt;a href="https://xania.org/AoCO2025"&gt;Advent of Compiler Optimisations 2025&lt;/a&gt;,
a 25-day series exploring how compilers transform our code.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This post was written by a human (&lt;a href="https://xania.org/MattGodbolt"&gt;Matt Godbolt&lt;/a&gt;) and reviewed and proof-read by LLMs and humans.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Support Compiler Explorer on &lt;a href="https://patreon.com/c/mattgodbolt"&gt;Patreon&lt;/a&gt;
or &lt;a href="https://github.com/sponsors/compiler-explorer"&gt;GitHub&lt;/a&gt;,
or by buying CE products in the &lt;a href="https://shop.compiler-explorer.com"&gt;Compiler Explorer Shop&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;I have had to cheat a little here to get the output I want: Iâ€™ve actually disabled GCCâ€™s main inlining pass, otherwise it chooses to inline the whole of &lt;code&gt;process&lt;/code&gt;. With a larger, more complex â€œslow pathâ€ that would be unnecessary, but in order to demonstrate the effect of partial inlining without generating tons of code, Iâ€™m using this slight cheat.Â &lt;a href="#fnref:note"&gt;â†©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Again, in this contrived example it probably &lt;em&gt;would&lt;/em&gt; be OK to inline &lt;code&gt;process&lt;/code&gt;, and the compiler really wants to, but for didactic purposes Iâ€™ve prevented that here. You can hopefully get the gist of this.Â &lt;a href="#fnref:again"&gt;â†©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Of course, nothing is free - duplicating code still takes up instruction cache space. The compilerâ€™s heuristics have to weigh the benefits against the costs, and different compilers make different choices.Â &lt;a href="#fnref:tradeoff"&gt;â†©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note that this varies substantially from compiler to compiler: I couldnâ€™t trick clang into making similar partial inlining decisions to gcc using flags, so I couldnâ€™t compare like with like. In my experience gcc and clang make quite different choices about inlining.Â &lt;a href="#fnref:clang"&gt;â†©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;</content:encoded>
      <guid isPermaLink="false">https://xania.org/202512/18-partial-inlining</guid>
      <category>Hacker News</category>
      <pubDate>Thu, 18 Dec 2025 13:39:41 +0000</pubDate>
    </item>
  </channel>
</rss>
