<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Hacker News Full Content Feed</title>
    <link>https://news.ycombinator.com/</link>
    <description>Hacker News front-page links with full article content.</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>en</language>
    <lastBuildDate>Wed, 24 Dec 2025 00:12:12 +0000</lastBuildDate>
    <item>
      <title>Is Northern Virginia Still the Least Reliable AWS Region?</title>
      <link>https://statusgator.com/blog/aws-least-reliable-region-in-2025/</link>
      <description>Published:</description>
      <content:encoded>&lt;article class="dynamic-content-template post-13389 post type-post status-publish format-standard has-post-thumbnail hentry category-outages tag-cloud-monitoring no-featured-image-padding" id="post-13389"&gt;

&lt;h1&gt;Is Northern Virginia Still the Least Reliable AWS Region in 2025? We Analyzed the Data&lt;/h1&gt;


&lt;p&gt;Published:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://statusgator.com/blog/aws-least-reliable-region-in-2025/"&gt;December 23, 2025&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;by &lt;a href="https://statusgator.com/blog/author/alibby/"&gt;Andy Libby&lt;/a&gt;&lt;/p&gt;



&lt;p&gt;Updated:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://statusgator.com/blog/aws-least-reliable-region-in-2025/"&gt;December 23, 2025&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;by &lt;a href="https://statusgator.com/blog/author/alibby/"&gt;Andy Libby&lt;/a&gt;&lt;/p&gt;



&lt;img alt="AWS N.Virginia reliability in 2025 StatusGator" src="https://statusgator.com/blog/wp-content/uploads/2025/12/Blog-thumbnail-7.png"/&gt;



Table of content


&lt;p&gt;This updated analysis is based on StatusGator outage data collected from January 1 to December 9, 2025. We decided to review our &lt;a href="https://statusgator.com/blog/is-north-virginia-aws-region-the-least-reliable-and-why/"&gt;AWS analysis of outages in 2022&lt;/a&gt; due to several new AWS incidents, especially another widely discussed AWS outage in us-east-1 (N. Virginia) that occurred on October 20, 2025.&lt;/p&gt;
&lt;p&gt;We’ve expanded the report with fresh 2025 regional data as well as a new breakdown of affected AWS services.&lt;/p&gt;
&lt;h2&gt;The Data Behind the Study&lt;/h2&gt;
&lt;p&gt;StatusGator continuously monitors the official AWS status pages and aggregates incidents across every public AWS Region. This analysis reflects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Major, publicly acknowledged AWS outages&lt;/li&gt;
&lt;li&gt;All commercial AWS regions (GovCloud excluded)&lt;/li&gt;
&lt;li&gt;Data timeframe: January 1, 2025 – December 9, 2025&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;AWS Outage Ranking by Region&lt;/h2&gt;
&lt;p&gt;So let’s take a look at the number of outages, duration, and components affected.&lt;/p&gt;



RegionNumber of outagesDurationComponents Affected




Regionless1231:55:1914


Canada-Central13:49:5719


Hyderabad10:44:5946


Ireland10:44:5110


N. Virginia1033:49:33126


Ohio21:20:452


Oregon32:59:413


Osaka12:15:0111


Sao Paulo10:44:519


Singapore10:54:591


Stockholm211:54:4981


Sydney10:50:001


Tokyo31:24:5118


Zurich14:54:557



&lt;!-- #tablepress-44 from cache --&gt;
&lt;h3&gt;Key Findings&lt;/h3&gt;
&lt;p&gt;N. Virginia (us-east-1) is once again the least reliable AWS Region.&lt;/p&gt;
&lt;p&gt;It leads the dataset in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Total number of outages (10)&lt;/li&gt;
&lt;li&gt;Total downtime (33 hours, 49 minutes)&lt;/li&gt;
&lt;li&gt;Total components affected (126)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;No other region even comes close. Stockholm ranks second in downtime (11+ hours). Despite only 2 outages, each incident had a massive regional impact.&lt;/p&gt;
&lt;p&gt;Regionless outages were unusually high. This category recorded &lt;strong&gt;12 outages&lt;/strong&gt; and &lt;strong&gt;32 hours of downtime&lt;/strong&gt;, indicating:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More widespread AWS service disruptions in 2025&lt;/li&gt;
&lt;li&gt;More failures affecting multiple regions simultaneously&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;AWS Services with the Most Outages&lt;/h2&gt;
&lt;p&gt;AWS doesn’t just experience regional outages. Service-level incidents are just as impactful.&lt;br/&gt;We analyzed the most frequently disrupted AWS services in 2025, ranked by the number of outages.&lt;/p&gt;



ServiceNumber of OutagesDuration




Amazon EC21419:14:01


Amazon SageMaker1120:40:21


AWS Glue1015:51:40


Amazon EMR1021:39:31


Amazon ECS1019:54:32



&lt;!-- #tablepress-45 from cache --&gt;
&lt;h3&gt;Key Findings&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Compute services dominated the outage list&lt;/strong&gt;, especially:&lt;br/&gt;
&lt;ul&gt;
&lt;li&gt;Amazon EC2 (core compute)&lt;/li&gt;
&lt;li&gt;Amazon ECS (containers)&lt;/li&gt;
&lt;li&gt;Amazon EMR (big data)&lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;EMR had &lt;strong&gt;the longest duration&lt;/strong&gt; among the top five (21 hours and 39 minutes).&lt;/li&gt;
&lt;li&gt;SageMaker experienced more outages than expected for an ML service, an emerging reliability trend.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;AWS Services With the Longest Outage Duration or Broadest Impact&lt;/h2&gt;
&lt;p&gt;These services didn’t always have the highest count, but had the longest or most severe incidents.&lt;/p&gt;



ServiceNumber of OutagesDuration




Amazon OpenSearch Service625:36:36


Amazon EMR Serverless725:30:08


Amazon CloudWatch624:58:49


Amazon Connect522:52:42


AWS STS522:48:39


Amazon VPC Lattice722:35:47


Amazon EMR1021:39:31


Amazon EventBridge521:24:32


Amazon Kinesis Data Streams521:15:00


AWS DataSync920:36:52


Amazon Elastic Load Balancing912:34:20


Amazon DynamoDB913:19:18


AWS Transit Gateway817:14:51


AWS Lambda813:50:15



&lt;!-- #tablepress-46 from cache --&gt;
&lt;h3&gt;Key Findings&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OpenSearch, EMR Serverless, and CloudWatch&lt;/strong&gt; each exceeded &lt;strong&gt;24+ hours&lt;/strong&gt; of cumulative downtime.&lt;/li&gt;
&lt;li&gt;Mission-critical systems like &lt;strong&gt;STS, DynamoDB, Lambda, and ELB&lt;/strong&gt; saw prolonged disruptions.&lt;/li&gt;
&lt;li&gt;EMR appears in &lt;strong&gt;both&lt;/strong&gt; spreadsheets, indicating it experienced frequent &lt;strong&gt;and&lt;/strong&gt; long-lasting ones.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Across Tables 1–3 above, we see a consistent pattern emerge:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Many of the affected components were concentrated in N. Virginia&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With 126 components affected, us-east-1 experienced the widest service disruption footprint.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Region-level outages and service-level outages are correlated&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Major incidents involving:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;EC2&lt;/li&gt;
&lt;li&gt;SageMaker&lt;/li&gt;
&lt;li&gt;EMR&lt;/li&gt;
&lt;li&gt;CloudWatch&lt;/li&gt;
&lt;li&gt;OpenSearch&lt;/li&gt;
&lt;li&gt;STS&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;…almost always touch N. Virginia due to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Higher customer density&lt;/li&gt;
&lt;li&gt;More service deployment fronts&lt;/li&gt;
&lt;li&gt;More inter-service dependency points&lt;/li&gt;
&lt;li&gt;Heavier API traffic&lt;/li&gt;
&lt;li&gt;Higher multi-AZ coordination complexity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;3. The longest-running outages disproportionately affected us-east-1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Duration-heavy outages (CloudWatch, OpenSearch, EMR Serverless) frequently included N. Virginia, driving up the region’s total downtime.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;N. Virginia is not only the region with the most outages, but it is also the region where service outages cascade the widest and run the longest.&lt;/p&gt;
&lt;h2&gt;AWS Outage on October 20, 2025&lt;/h2&gt;
&lt;p&gt;On October 20, 2025, AWS experienced one of the &lt;a href="https://statusgator.com/blog/statusgator-october-20-2025-outage-postmortem/"&gt;most significant cloud outages&lt;/a&gt; in its history. 76 individual AWS components in the N. Virginia region alone showed disruption, by far the most heavily affected region.&lt;/p&gt;
&lt;p&gt;Portions of Amazon Web Services were down for nearly 15 hours, causing cascading failures across thousands of SaaS platforms.&lt;/p&gt;
&lt;p&gt;StatusGator’s &lt;a href="https://statusgator.com/blog/october-2025-early-warning-signals/"&gt;Early Warning Signals&lt;/a&gt; detected the incident approximately ten minutes before AWS officially acknowledged it, ultimately identifying outages across more than 2,000 of the 6,000 services in our monitoring network.&lt;/p&gt;
&lt;p&gt;However, the magnitude of the event meant StatusGator was also impacted, experiencing two periods of dashboard and status page downtime due to a surge in global traffic and failures in upstream infrastructure.&lt;/p&gt;
&lt;p&gt;Despite these disruptions, StatusGator delivered over 100,000 outage notifications throughout the incident and has since implemented architectural improvements to strengthen reliability during future large-scale cloud failures.&lt;/p&gt;
&lt;h2&gt;Why Is N. Virginia Still the Least Reliable Region in 2025?&lt;/h2&gt;
&lt;p&gt;We revisited the three common theories from our &lt;a href="https://statusgator.com/blog/is-north-virginia-aws-region-the-least-reliable-and-why/"&gt;2023 AWS outage analysis&lt;/a&gt; and compared them against this year’s dataset.&lt;/p&gt;
&lt;h3&gt;Assumption 1: “N. Virginia Has More Services, So More Things Can Break”&lt;/h3&gt;
&lt;p&gt;In 2023, we found this explanation to be weak. But the 2025 “Components Affected” numbers tell a new story:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N. Virginia affected 126 components&lt;/li&gt;
&lt;li&gt;Next highest: Stockholm with 81&lt;/li&gt;
&lt;li&gt;Most regions affected ≤ 20 components&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This indicates:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Broader blast when outages occur in N. Virginia&lt;/li&gt;
&lt;li&gt;More interconnected or high-density service dependency&lt;/li&gt;
&lt;li&gt;More potential points of failure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Still, high service count alone doesn’t explain the full scale:&lt;br/&gt;Regions like Oregon and Ireland offer nearly as many services but have far fewer issues.&lt;/p&gt;
&lt;p&gt;So the number of components contributes to complexity, but not the root cause.&lt;/p&gt;
&lt;h3&gt;Assumption 2: “N. Virginia Is the Most Used and Most Heavily Loaded Region”&lt;/h3&gt;
&lt;p&gt;This remains the strongest and most likely explanation. StatusGator &lt;a href="https://statusgator.com/services/amazon-web-services/"&gt;monitoring AWS status&lt;/a&gt; data historically shows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N. Virginia is monitored by &lt;strong&gt;almost 2× as many users&lt;/strong&gt; as Oregon&lt;/li&gt;
&lt;li&gt;And &lt;strong&gt;over 3× as many&lt;/strong&gt; as many other U.S. and global regions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More customers → heavier load → more real-world stress → more outages that reach public visibility.&lt;/p&gt;
&lt;p&gt;So this assumption is very likely true, and reinforced by 2025 data.&lt;/p&gt;
&lt;h3&gt;Assumption 3: “N. Virginia Is Older and Built Differently”&lt;/h3&gt;
&lt;p&gt;AWS provides no evidence that us-east-1 uses a fundamentally different architecture. And our 2025 numbers don’t suggest “old region issues”:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tokyo and Sydney (both older) had minimal downtime&lt;/li&gt;
&lt;li&gt;Newer regions, like Zurich and Hyderabad, had multi-hour outages&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Like in 2023, we still see no evidence supporting this theory.&lt;/p&gt;
&lt;h2&gt;Summary: AWS Reliability in 2025&lt;/h2&gt;
&lt;p&gt;With only weeks left in 2025, the data is clear:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Us-east-1 (N. Virginia) remains the least reliable AWS Region&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Most outages&lt;/li&gt;
&lt;li&gt;Most downtime&lt;/li&gt;
&lt;li&gt;Most components affected&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Compute, analytics, and AI/ML services were the most outage-prone&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;EC2, SageMaker, Glue, EMR, and ECS led the list.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Several AWS services experienced extremely long-running disruptions&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;OpenSearch, CloudWatch, EMR Serverless, and STS had &lt;strong&gt;over 24 hours&lt;/strong&gt; of cumulative downtime.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Multi-region outages increased&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Regionless category shows a notable rise in cross-region or global incidents in 2025.&lt;/p&gt;
&lt;h2&gt;Get Notified of AWS Outages Before AWS Reports Them&lt;/h2&gt;
&lt;p&gt;StatusGator aggregates every AWS service and region into a single unified dashboard.&lt;br/&gt;We alert you instantly, often before AWS posts the incident publicly.&lt;/p&gt;
&lt;p&gt;Get instant, account-specific AWS outage alerts through StatusGator’s unified dashboard, now enhanced with AWS Health integration for Enterprise customers. It delivers trusted, direct notifications about incidents, outages, and maintenance affecting your services, with built-in filtering to reduce noise, and seamless delivery to Slack, Microsoft Teams, Discord, Google Chat, and more.&lt;/p&gt;
&lt;p&gt;Monitor AWS outages in real time with StatusGator — &lt;a href="https://statusgator.com/users/sign_up"&gt;free to try&lt;/a&gt;.&lt;/p&gt;



&lt;a href="https://statusgator.com/blog/category/outages/"&gt;Outages&lt;/a&gt;



&lt;a href="https://statusgator.com/blog/microsoft-teams-outage-on-december-19-2025/"&gt;❮  Previous article&lt;/a&gt;






&lt;p&gt;Share this&lt;/p&gt;

&lt;a href="https://www.addtoany.com/add_to/copy_link?linkurl=https%3A%2F%2Fstatusgator.com%2Fblog%2Faws-least-reliable-region-in-2025%2F&amp;amp;linkname=Is%20Northern%20Virginia%20Still%20the%20Least%20Reliable%20AWS%20Region%20in%202025%3F%20We%20Analyzed%20the%20Data"&gt;&lt;/a&gt;&lt;a href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fstatusgator.com%2Fblog%2Faws-least-reliable-region-in-2025%2F&amp;amp;linkname=Is%20Northern%20Virginia%20Still%20the%20Least%20Reliable%20AWS%20Region%20in%202025%3F%20We%20Analyzed%20the%20Data"&gt;&lt;/a&gt;&lt;a href="https://www.addtoany.com/add_to/x?linkurl=https%3A%2F%2Fstatusgator.com%2Fblog%2Faws-least-reliable-region-in-2025%2F&amp;amp;linkname=Is%20Northern%20Virginia%20Still%20the%20Least%20Reliable%20AWS%20Region%20in%202025%3F%20We%20Analyzed%20the%20Data"&gt;&lt;/a&gt;&lt;a href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fstatusgator.com%2Fblog%2Faws-least-reliable-region-in-2025%2F&amp;amp;linkname=Is%20Northern%20Virginia%20Still%20the%20Least%20Reliable%20AWS%20Region%20in%202025%3F%20We%20Analyzed%20the%20Data"&gt;&lt;/a&gt;&lt;a href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fstatusgator.com%2Fblog%2Faws-least-reliable-region-in-2025%2F&amp;amp;linkname=Is%20Northern%20Virginia%20Still%20the%20Least%20Reliable%20AWS%20Region%20in%202025%3F%20We%20Analyzed%20the%20Data"&gt;&lt;/a&gt;


&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://statusgator.com/blog/aws-least-reliable-region-in-2025/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 23:12:03 +0000</pubDate>
    </item>
    <item>
      <title>Microspeak: North Star – The Old New Thing (2015)</title>
      <link>https://devblogs.microsoft.com/oldnewthing/20151103-00/?p=91861</link>
      <description>I noted it in the interview with the Defrag Tools show , but I’ll make a proper Microspeak for it. Today’s term is North star .</description>
      <content:encoded>&lt;article class="middle-column pe-xl-198" data-clarity-region="article" id="post-91861"&gt;

&lt;p&gt;I noted it &lt;a href="https://channel9.msdn.com/Shows/Defrag-Tools/Defrag-Tools-142-Raymond-Chen-Old-New-Thing"&gt;in the interview with the Defrag Tools show&lt;/a&gt;, but I’ll make a proper Microspeak for it. Today’s term is North star. &lt;/p&gt;
&lt;p&gt;This term rose quickly to prominence in October 2015. My research suggests that it had been simmering below the surface for about a year. For example, &lt;a href="http://blogs.msdn.com/b/eric_brechner/archive/2015/06/01/solving-the-whole-problem.aspx"&gt;here’s an isolated citation from May 2015&lt;/a&gt;: &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The best you can do is paint a compelling picture of an improved world (your north star), and plan the long journey to it. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This citation is interesting because it seems to give a definition for “north star”: It means “a compelling picture of an improved world”. &lt;/p&gt;
&lt;p&gt;The term has become wildly popular of late at Microsoft. I guess a major executive used the term recently, so now it’s suddenly the cool thing to say. &lt;/p&gt;
&lt;p&gt;We had a team meeting a little while ago. One of the agenda items was  “Longer term North star topics”, which was itself rather intriguing. During the meeting, I noted¹ the following uses of the term: &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;There may be changes along the way, but your north star of the feature is intact. &lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;We have to decide where we want to go as a north star. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I raised my hand. “What do you mean by north star? Because if you follow the north star, you end up at the north pole, and not where you actually want to go.” &lt;/p&gt;
&lt;p&gt;The speaker seemed a bit frustrated by this question. “Who is this idiot who doesn’t know what a north star is? Certainly this person hasn’t been in &lt;a href="http://blogs.msdn.com/b/oldnewthing/archive/2013/12/11/10481036.aspx"&gt;all the meetings I’ve been in&lt;/a&gt;, where people are saying ‘north star’ all over the place.” &lt;/p&gt;
&lt;p&gt;The speaker noted that I might want to look it up in the dictionary, because it would have told me that the north star is the goal you have beyond your immediate goal. It’s a guiding principle that keeps you on the right path for your journey. (Curiously, this definition doesn’t appear anywhere in &lt;a href="http://www.merriam-webster.com/dictionary/north%20star"&gt;any&lt;/a&gt; &lt;a href="http://dictionary.reference.com/browse/north-star"&gt;online&lt;/a&gt; &lt;a href="http://www.oxforddictionaries.com/definition/english/North-Star"&gt;dictionary&lt;/a&gt; &lt;a href="https://en.wiktionary.org/wiki/North_Star"&gt;I could find&lt;/a&gt;. It also doesn’t match the citation at the top of this article.) &lt;/p&gt;
&lt;p&gt;So there you go. An explicit definition, as provided by somebody who used the term. I embarrassed myself in front of my whole team for you. &lt;/p&gt;
&lt;p&gt;Bonus chatter: Later that same day, a top executive sent mail to the entire company. It too used the term “north star”: &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;With &lt;a href="http://www.microsoft.com/en-us/about/default.aspx"&gt;Microsoft’s mission&lt;/a&gt; as our north star—to empower every person and every organization on the planet to achieve more—we have a… &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;¹ Yes, when I attend meetings, one of the things I pay particular attention to is new jargon, so I can add it to my collection of citations. If you see me pull out my phone and jot something down, it’s either because I’m writing down a question to ask later, or I’m preserving something you said so I can add it to my Microspeak citations. &lt;/p&gt;
&lt;!-- .entry-content --&gt;
&lt;!-- AI Disclaimer --&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://devblogs.microsoft.com/oldnewthing/20151103-00/?p=91861</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 22:23:49 +0000</pubDate>
    </item>
    <item>
      <title>Texas app store age verification law blocked by federal judge</title>
      <link>https://www.macrumors.com/2025/12/23/texas-app-store-law-blocked/</link>
      <description>A Texas federal judge today blocked an App Store age verification law that was set to go into effect on January 1, 2026, which means Apple may not have to support the changes after all.</description>
      <content:encoded>&lt;article class="article--2pJwZBkO js-article" expanded="true"&gt;&lt;h1&gt;Texas App Store Age Verification Law Blocked by Federal Judge&lt;/h1&gt;Tuesday December 23, 2025 12:36 pm PST by &lt;a href="https://www.macrumors.com/author/juli-clover/"&gt;Juli Clover&lt;/a&gt;&lt;p&gt;A Texas federal judge today blocked an &lt;a href="https://www.macrumors.com/guide/app-store/"&gt;App Store&lt;/a&gt; age verification law that was set to go into effect on January 1, 2026, which means Apple may not have to support the changes after all.&lt;/p&gt;
&lt;p&gt;&lt;img alt="iOS App Store General Feature Desaturated" src="https://images.macrumors.com/t/T5SjvgLdchGm5DEKzIfFVPQgFIQ=/400x0/article-new/2022/01/iOS-App-Store-General-Feature-Desaturated.jpg?lossy"/&gt;&lt;br/&gt;The Texas ‌App Store‌ Accountability Act (SB2420) requires Apple and other app marketplaces to confirm user age when a person creates an Apple Account. Apple Accounts for users under 18 would need to join a Family Sharing group, with new controls available for parents and restrictions for minors.&lt;/p&gt;
&lt;p&gt;In a preliminary injunction that delays the implementation of the act, Judge Robert Pitman said that it violates the First Amendment and is "more likely than not unconstitutional."&lt;br/&gt;
&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The Act is akin to a law that would require every bookstore to verify the age of every customer at the door and, for minors, require parental consent before the child or teen could enter and again when they try to purchase a book. As set out below, the Court finds a likelihood that, when considered on the merits, SB 2420 violates the First Amendment.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The injunction was in response to a motion filed by the Computer and Communications Industry Association (CCIA), a group that includes Apple and Google. Today's decision is a win for Apple, as Apple has been fighting against age assurance requirements in Texas and other states. Apple says that the Texas law impacts user privacy.&lt;br/&gt;
&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;While we share the goal of strengthening kids' online safety, we are concerned that SB2420 impacts the privacy of users by requiring the collection of sensitive, personally identifiable information to download any app, even if a user simply wants to check the weather or sports scores.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The court will move on to determining whether the law is facially invalid, which would mean that it is unconstitutional and will be entirely thrown out.&lt;/p&gt;
&lt;p&gt;Note: Due to the political or social nature of the discussion regarding this topic, the discussion thread is located in our &lt;a href="https://forums.macrumors.com/forums/political-news.218/"&gt;Political News&lt;/a&gt; forum.  All forum members and site visitors are welcome to read and follow the thread, but posting is limited to forum members with at least 100 posts.&lt;/p&gt;
Tag: &lt;a href="https://www.macrumors.com/guide/texas/"&gt;Texas&lt;/a&gt;[ &lt;a href="https://forums.macrumors.com/threads/texas-app-store-age-verification-law-blocked-by-federal-judge.2475130/"&gt;37 comments&lt;/a&gt; ]&lt;a href="https://twitter.com/share?url=http%3A%2F%2Fwww.macrumors.com%2F2025%2F12%2F23%2Ftexas-app-store-law-blocked%2F&amp;amp;text=Texas+App+Store+Age+Verification+Law+Blocked+by+Federal+Judge&amp;amp;related=macrumors"&gt;&lt;/a&gt;&lt;a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fwww.macrumors.com%2F2025%2F12%2F23%2Ftexas-app-store-law-blocked%2F&amp;amp;amp;src=sdkpreparse"&gt;&lt;/a&gt;&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://www.macrumors.com/2025/12/23/texas-app-store-law-blocked/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 22:03:46 +0000</pubDate>
    </item>
    <item>
      <title>Show HN: Claude Wrapped in the terminal, with a WASM raymarcher</title>
      <link>https://spader.zone/wrapped/</link>
      <description>Run to compare your Claude Code usage against the rest of the world while enjoying a spirited holiday Santa Claude rendered in fully lit 3D in your terminal with the power of WASM.</description>
      <content:encoded>&lt;main&gt;&lt;h1&gt;» &lt;em&gt;tldr&lt;/em&gt;&lt;/h1&gt;&lt;p&gt;Run to compare your &lt;code&gt;Claude Code&lt;/code&gt; usage against the rest of the world while enjoying a spirited holiday Santa Claude rendered in fully lit 3D in your terminal with the power of WASM.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;bun x @spader/claude-wrapped
&lt;/code&gt;&lt;/pre&gt;&lt;h1&gt;» &lt;em&gt;slightly less tldr&lt;/em&gt;&lt;/h1&gt;cpu-destroying wasm claudeClick to load...&lt;p&gt;I wrote a raymarcher that can render scenes of SDF functions and lights in plain C, compiled it with WASM, jammed it all into a Bun executable that grabs your &lt;code&gt;~/.claude/stats-cache.json&lt;/code&gt;, uploads it to a SQLite database on The Cloud so you can see how your Claude Code usage stacks up with the rest of the world.&lt;/p&gt;&lt;p&gt;You can see the aforementioned raymarched Claude above, on this very web page! As far as the data collected, the code’s on &lt;a href="https://github.com/tspader/claude-wrapped/"&gt;GitHub&lt;/a&gt;. It’s a spit of TypeScript and a WASM module that’s as close to pure computation as you can get. Clone and run with &lt;code&gt;bun start&lt;/code&gt;; the WASM’s already compiled, so you shouldn’t even need a C compiler.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Please reach out to &lt;a href="mailto:spader@spader.zone"&gt;spader@spader.zone&lt;/a&gt; if anything bugs out for you.&lt;/strong&gt; I cranked this out in a manic weekend, I’m sure it’s got plenty of bugs.&lt;/p&gt;&lt;p&gt;Read on if you’re interested in the experience of cranking out some rendering code that felt quite real with WASM!&lt;/p&gt;&lt;h1&gt;» &lt;em&gt;shill&lt;/em&gt;&lt;/h1&gt;&lt;img alt="Boon Bane" src="https://spader.zone/images/boon_bane.png"/&gt;deep copyA hand-painted science fiction point and click; for fans of Disco Elysium, Philip K. Dick, Pentiment&lt;a href="https://store.steampowered.com/app/2639990/Deep_Copy/"&gt;Wishlist on Steam&lt;/a&gt;&lt;h1&gt;» &lt;em&gt;wrapped season&lt;/em&gt;&lt;/h1&gt;&lt;p&gt;It’s that time of year, folks. Here in Atlanta, the leaves are turning to beautiful taupes and crimsons. Winter air blows cold and sweet over a thousand lakes in a thousand counties and cleanses mind, body, and soul. And, of course, people everywhere are whipped into a frenzy over the ability to quantify and commodotize even their most basic humanity.&lt;/p&gt;&lt;p&gt;That’s right. It’s Wrapped season!&lt;/p&gt;&lt;p&gt;I usually use &lt;a href="https://github.com/sst/opencode"&gt;opencode&lt;/a&gt;, which constantly impresses me with its attention to detail and quality in the terminal. It is better than most web apps; not better &lt;em&gt;for a TUI&lt;/em&gt;, just better. But I dip into the acutal &lt;code&gt;claude&lt;/code&gt; CLI sometimes. On one such occasion, I noticed something new: &lt;code&gt;/stats&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;&lt;img src="https://spader.zone/images/claude-stats.png"/&gt;&lt;/p&gt;&lt;p&gt;It’s fun! It looks good. It has a fun GitHub-style commit heatmap, and the silly Alex Horne comparative measurements. It’s the kind of thing that makes us love Claude Code. But…can we do better?&lt;/p&gt;&lt;h1&gt;» &lt;em&gt;we can do better&lt;/em&gt;&lt;/h1&gt;&lt;p&gt;I love a good Wrapped as much as the next person. I was a top 0.05% listener of the Grateful Dead last cycle. I don’t know what to &lt;em&gt;do&lt;/em&gt; with this information, but when did that ever stop me?&lt;/p&gt;&lt;p&gt;&lt;img alt="fortunately, spotify wasn’t able to detect that the time was split between me and mr. pig" src="https://spader.zone/images/dead-wrapped.png"/&gt;&lt;/p&gt;&lt;p&gt;A few minutes of poking around &lt;code&gt;$HOME/.claude&lt;/code&gt; unearthed &lt;code&gt;stats-cache.json&lt;/code&gt;! And while this file wasn’t quite as interesting as I’d hoped, it did have more than enough to make a decent wrapped:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Token and message counts by day and model&lt;/li&gt;&lt;li&gt;Invocations by hour-of-day&lt;/li&gt;&lt;li&gt;Costs&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The idea hit me like lightning. Grab these stats, make a wrapped. The stats were a little strange – they only went back a month or so. But I assumed this was because Arch wasn’t a first class citizen, and that some &lt;code&gt;paru -Syu&lt;/code&gt; had wiped some cache or something. Unfortunately, these stats were the best I had. There wasn’t an easy API to get these stats more rigorously. I’m still not entirely sure if they’re per machine or per user (I think the former).&lt;/p&gt;&lt;p&gt;But still, this was more than enough. I had a free weekend, so I slapped it all together.&lt;/p&gt;&lt;h1&gt;» &lt;em&gt;a very good stack&lt;/em&gt;&lt;/h1&gt;&lt;p&gt;This is my first serious (i.e. someone’s actually gonna use it) project with the web. And god damn it, I was &lt;em&gt;impressed&lt;/em&gt;. I had a fantastic experience writing this thing.&lt;/p&gt;&lt;h2&gt;» &lt;em&gt;opentui&lt;/em&gt;&lt;/h2&gt;&lt;p&gt;The frontend is written in TypeScript using the excellent &lt;a href="https://github.com/sst/opentui"&gt;OpenTUI&lt;/a&gt;. It’s the renderer for OpenCode, but is completely decoupled. I’d used it once before for a frontend for a small personal utility, but there I’d used SolidJS since I was just rendering plain UI. They use Yoga to lay out your HTML and CSS, and for my simple cases everything worked flawlessly.&lt;/p&gt;&lt;p&gt;Thankfully, OpenTUI also exposes a plain frame buffer you can write whatever you want to. Which is exactly what I wanted! This had the benefit of being able to use HTML (i.e. flex box) on &lt;em&gt;top&lt;/em&gt; of my pseudo-canvas!&lt;/p&gt;&lt;h2&gt;» &lt;em&gt;wasm&lt;/em&gt;&lt;/h2&gt;&lt;p&gt;The renderer is simple, purely computational C. I was surprised to learn that SIMD works just fine in WASM. The compilation experience was trivially easy; it worked out of the box with clang.&lt;/p&gt;&lt;p&gt;Ironically, when I swapped to my MacBook, the bundled clang was too old to have WASM as a target. I switched to &lt;code&gt;zig cc&lt;/code&gt;, and everything worked flawlessly. In general, &lt;code&gt;flawless&lt;/code&gt; is the word I would use to describe my WASM experience. It’s a great spec, and it has a great ecosystem. It’s ready!&lt;/p&gt;&lt;h2&gt;» &lt;em&gt;bun&lt;/em&gt;&lt;/h2&gt;&lt;p&gt;Oh, Bun. I emailed their founder for a job on a whim after seeing a posting in his Twitter bio. And I got an interview! Unfortunately, my deepest tryhard instincts took over on the software part of the interview, and I added too many features before making sure the core was rock solid. Time slipped away, I submitted a piece of junk, and I rightfully didn’t hear back.&lt;/p&gt;&lt;p&gt;A week later, they got acquired by Anthropic. Did I mention the offer came with a little equity?&lt;/p&gt;&lt;p&gt;Ah, well. Life goes on. I had to get that one out. The fact remains that the reason I reached out in the first place is that Bun is in a very rare tier of universally beloved software. It is fast, it does just what you want, and it has clearly been designed with a love of software and a deep technical competence. Building and deploying the TypeScript stuff was a breeze. Unless pressed by, I don’t know, a Mossad agent with some incriminating photos of me from the Epstein files, I’m a Bun lifer. Interview be damned.&lt;/p&gt;&lt;h2&gt;» &lt;em&gt;cloudflare&lt;/em&gt;&lt;/h2&gt;&lt;p&gt;I flirted with spinning up SQLite on an existing ultracheap VPS that I used for &lt;a href="https://store.steampowered.com/app/2639990/Deep_Copy/"&gt;Deep Copy&lt;/a&gt;. But I was worried that this might have five minutes of fame, and when I say cheap, I mean &lt;em&gt;cheap&lt;/em&gt;. So, I did the next best thing, which is spinning up SQLITE on someone &lt;em&gt;else’s&lt;/em&gt; server.&lt;/p&gt;&lt;p&gt;I am &lt;em&gt;thoroughly&lt;/em&gt; impressed. &lt;code&gt;wrangler&lt;/code&gt;, minus a brief hiccups with IPv6 on my Arch desktop, Just Worked. I manage my domains through Cloudflare, and it’s absolutely gorgeous inside this walled garden. Everything just slots together. It’s the same feeling I get when building a computer; I have no idea what these components &lt;em&gt;really&lt;/em&gt; do, but after you do it a few times you just know it’ll work when you snap it all together. Maybe when I have to do real engineering in this domain my feelings will change.&lt;/p&gt;&lt;p&gt;I’ve put up a D1 instance running in the ethereal hyperplane, a cron job to recalculate global stats every fifteen minutes, and I was off.&lt;/p&gt;&lt;h1&gt;» &lt;em&gt;i’m a moron&lt;/em&gt;&lt;/h1&gt;&lt;p&gt;Unfortunately, there was one fatal flaw. I was running a few tests, and I noticed my token count going down. When I checked the aforementioned &lt;code&gt;stats-cache.json&lt;/code&gt;, I had a horrifying realization. These stats are only for the last month!&lt;/p&gt;&lt;p&gt;I’d put probably 12 or 15 hours into the damn thing at this point. I thought the renderer was quite beautiful. The thought of having wasted all this effort, and to have been such a fool, was pretty rough.&lt;/p&gt;&lt;p&gt;But, as all of my problems resolve, I went downstairs to sort my screws into my new toolbox, which is where my wife found me; dejected, low, on the brink of abandoning life. But, sweet angel that she is, she told me that people wouldn’t care that it was just the last month.&lt;/p&gt;&lt;p&gt;Because, damn it, people like stats. People like Wrapped! And I hope you like mine.&lt;/p&gt;&lt;/main&gt;</content:encoded>
      <guid isPermaLink="false">https://spader.zone/wrapped/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 21:59:28 +0000</pubDate>
    </item>
    <item>
      <title>X-ray: a Python library for finding bad redactions in PDF documents</title>
      <link>https://github.com/freelawproject/x-ray</link>
      <description>GitHub - freelawproject/x-ray: A tool to detect whether a PDF has a bad redaction</description>
      <content:encoded>&lt;article class="markdown-body entry-content container-lg" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/freelawproject/x-ray/main/redacted.png"&gt;&lt;img alt="Image of REDACTED STAMP" src="https://raw.githubusercontent.com/freelawproject/x-ray/main/redacted.png"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;x-ray is a Python library for finding bad redactions in PDF documents.&lt;/p&gt;
&lt;h2&gt;Why?&lt;/h2&gt;&lt;a href="#why"&gt;&lt;/a&gt;
&lt;p&gt;At Free Law Project, we collect millions of PDFs. An ongoing problem
is that people fail to properly redact things. Instead of doing it the right
way, they just draw a black rectangle or a black highlight on top of black
text and call it a day. Well, when that happens you just select the text under
the rectangle, and you can read it again. Not great.&lt;/p&gt;
&lt;p&gt;After witnessing this problem for years, we decided it would be good to figure
out how common it is, so, with some help, we built this simple tool. You give
the tool the path to a PDF. It tells you if it has worthless redactions in it.&lt;/p&gt;
&lt;h2&gt;What next?&lt;/h2&gt;&lt;a href="#what-next"&gt;&lt;/a&gt;
&lt;p&gt;Right now, &lt;code&gt;x-ray&lt;/code&gt; works pretty well and we are using it to analyze documents
in our collections. It could be better though. Bad redactions take many forms.
See the issues tab for other examples we don't yet support. We'd love your
help solving some of tougher cases.&lt;/p&gt;
&lt;h2&gt;Installation&lt;/h2&gt;&lt;a href="#installation"&gt;&lt;/a&gt;
&lt;p&gt;With uv, do:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;uv add x-ray
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With pip, that'd be:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install x-ray
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Usage&lt;/h2&gt;&lt;a href="#usage"&gt;&lt;/a&gt;
&lt;p&gt;&lt;code&gt;uvx&lt;/code&gt; lets you run this without even installing it. For example, here's an amicus brief we filed that doesn't have any bad redactions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;uvx --from x-ray xray https://storage.courtlistener.com/recap/gov.uscourts.ca3.125346/gov.uscourts.ca3.125346.45.0.pdf
{}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once you &lt;em&gt;do&lt;/em&gt; install x-ray, you can easily use it on the command line. Once installed, just:&lt;/p&gt;
&lt;pre&gt;% xray path/to/your/file.pdf
{
  "1": [
    {
      "bbox": [
        58.550079345703125,
        72.19873046875,
        75.65007781982422,
        739.3987426757812
      ],
      "text": "The Ring travels by way of Cirith Ungol"
    }
  ]
}&lt;/pre&gt;
&lt;p&gt;Or if you have the file on a server somewhere, give it a URL. If it starts with &lt;code&gt;https://&lt;/code&gt;, it will be interpreted as a PDF to download. Here's congressional testimony our directory made (it doesn't have any bad redactions):&lt;/p&gt;
&lt;pre&gt;% xray https://free.law/pdf/congressional-testimony-michael-lissner-free-law-project-hearing-on-ethics-and-transparency-2021-10-26.pdf
{}&lt;/pre&gt;
&lt;p&gt;A fun trick you can do is to make a file with one URL per line, call it &lt;code&gt;urls.txt&lt;/code&gt;. Then you can run this to check each URL:&lt;/p&gt;
&lt;pre&gt;xargs -n 1 xray  &amp;lt; urls.txt&lt;/pre&gt;
&lt;p&gt;However you run &lt;code&gt;xray&lt;/code&gt; on the command line, you'll get JSON as output. When you have that, you can use it with tools like &lt;a href="https://stedolan.github.io/jq/"&gt;&lt;code&gt;jq&lt;/code&gt;&lt;/a&gt;. The format is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It's a dict.&lt;/li&gt;
&lt;li&gt;The keys are page numbers.&lt;/li&gt;
&lt;li&gt;Each page number maps to a list of dicts.&lt;/li&gt;
&lt;li&gt;Each of those dicts maps to two keys.&lt;/li&gt;
&lt;li&gt;The first key is &lt;code&gt;bbox&lt;/code&gt;. This is a four-tuple that indicates the x,y positions of the upper left corner and then lower right corners of the bad redaction.&lt;/li&gt;
&lt;li&gt;The second key is &lt;code&gt;text&lt;/code&gt;. This is the text under the bad rectangle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Simple enough.&lt;/p&gt;
&lt;p&gt;You can also use it as a Python module, if you prefer the long-form:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% python -m xray some-file.pdf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But that's not as easy to remember.&lt;/p&gt;
&lt;p&gt;If you want a bit more, you can, of course, use &lt;code&gt;xray&lt;/code&gt; in Python:&lt;/p&gt;
&lt;pre&gt;from pprint import pprint
import xray
bad_redactions = xray.inspect("some/path/to/your/file.pdf")  # Pathlib works too
pprint(bad_redactions)
{1: [{'bbox': (58.550079345703125,
               72.19873046875,
               75.65007781982422,
               739.3987426757812),
      'text': 'Aragorn is the one true king.'}]}&lt;/pre&gt;
&lt;p&gt;The output is the same as above, except it's a Python object, not a JSON object.&lt;/p&gt;
&lt;p&gt;If you already have the file contents as a &lt;code&gt;bytes&lt;/code&gt; object, that'll work too:&lt;/p&gt;
&lt;pre&gt;some_bytes = requests.get("https://lotr-secrets.com/some-doc.pdf").content
bad_redactions = xray.inspect(some_bytes)&lt;/pre&gt;
&lt;p&gt;Note that because the &lt;code&gt;inspect&lt;/code&gt; method uses the same signature no matter what,
the type of the object you give it is essential:&lt;/p&gt;



Input
&lt;code&gt;xray&lt;/code&gt;'s Assumption




&lt;code&gt;str&lt;/code&gt; or Pathlib &lt;code&gt;Path&lt;/code&gt;
local file


&lt;code&gt;str&lt;/code&gt; that starts with &lt;code&gt;https://&lt;/code&gt;
URL to download


&lt;code&gt;bytes&lt;/code&gt;
PDF in memory



&lt;p&gt;This means that if you provide the filename on disk as a bytes object instead
of a &lt;code&gt;str&lt;/code&gt;, it's not going to work. This will fail:&lt;/p&gt;
&lt;pre&gt;xray.inspect(b"some-file-path.pdf")&lt;/pre&gt;
&lt;p&gt;That's pretty much it. There are no configuration files or other variables to
learn. You give it a file name. If there is a bad redaction in it, you'll soon
find out.&lt;/p&gt;
&lt;h2&gt;How it works&lt;/h2&gt;&lt;a href="#how-it-works"&gt;&lt;/a&gt;
&lt;p&gt;Under the covers, &lt;code&gt;xray&lt;/code&gt; uses the high-performant &lt;a href="https://pymupdf.readthedocs.io/"&gt;PyMuPDF project&lt;/a&gt; to parse PDFs. It has been a wonderful project to work with.&lt;/p&gt;
&lt;p&gt;You can read the source to see how it works, but the general idea is to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Find rectangles in a PDF.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Find letters in the same location&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Render the rectangle as an image&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inspect the rectangle to see if it's all one color. If it is, then that's a
bad redaction. If not, then we assume you can see a mix of text and
drawings, indicating a redaction that's OK.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The PDF format is a big and complicated one, so it's difficult to do all this perfectly. We do our best, but there's always more to do to make it better. &lt;a href="https://free.law/donate/"&gt;Donations&lt;/a&gt; and sponsored work help.&lt;/p&gt;
&lt;h2&gt;Contributions&lt;/h2&gt;&lt;a href="#contributions"&gt;&lt;/a&gt;
&lt;p&gt;Please see the issues list on Github for things we need, or start a conversation if you have questions. Before you do your first contribution, we'll need a signed contributor license agreement. See the template in the repo.&lt;/p&gt;
&lt;h2&gt;Deployment&lt;/h2&gt;&lt;a href="#deployment"&gt;&lt;/a&gt;
&lt;p&gt;Releases happen automatically via Github Actions. To trigger an automated build:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Update CHANGES.md&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update the version in pyproject.toml&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tag the commit with something like "v0.0.0".&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you wish to create a new version manually, the process is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Update version info in &lt;code&gt;pyproject.toml&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure your Pypi credentials &lt;a href="https://python-poetry.org/docs/repositories/#configuring-credentials"&gt;with Poetry&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Build and publish the version:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;poetry publish --build&lt;/pre&gt;
&lt;h2&gt;License&lt;/h2&gt;&lt;a href="#license"&gt;&lt;/a&gt;
&lt;p&gt;This repository is available under the permissive BSD license, making it easy and safe to incorporate in your own libraries.&lt;/p&gt;
&lt;p&gt;Pull and feature requests welcome. Online editing in GitHub is possible (and easy!).&lt;/p&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://github.com/freelawproject/x-ray</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 21:54:30 +0000</pubDate>
    </item>
    <item>
      <title>HTTP Caching, a Refresher</title>
      <link>https://danburzo.ro/http-caching-refresher/</link>
      <description>Published Dec 22, 2025</description>
      <content:encoded>&lt;article class="post" data-pagefind-body=""&gt;
&lt;h1&gt;HTTP caching, a refresher&lt;/h1&gt;
&lt;p&gt;
		Published
		Dec 22, 2025
&lt;/p&gt;
&lt;p&gt;This is a reading of &lt;a href="https://www.rfc-editor.org/rfc/rfc9111"&gt;RFC 9111&lt;/a&gt; (2022), the latest iteration of the HTTP Caching standard.&lt;/p&gt;
&lt;p&gt;It defines the &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/Cache-Control"&gt;&lt;code&gt;Cache-Control&lt;/code&gt;&lt;/a&gt; HTTP header as a way to prescribe how caches should store and reuse HTTP responses, with regards to not just the browser cache, but to any other intermediary caches, such as proxies and content delivery networks, that may exist between the client and the origin server.&lt;/p&gt;
&lt;img alt="A crude illustration depicting a browser with its private cache, two intermediary services with their shared caches, and the origin server" src="https://danburzo.ro/img/http-caching-refresher/cache-chain.png"/&gt;
&lt;p&gt;The &lt;code&gt;Cache-Control&lt;/code&gt; header accepts a set of comma-separated directives, some of which are meant to be added to HTTP requests, and others to HTTP responses. A typical response header:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;HTTP/2 200
Cache-Control: max-age=0, must-revalidate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some of these directives specifically target &lt;em&gt;shared caches&lt;/em&gt;, that is intermediary caches that serve the same cached responses to many users, while others also apply to &lt;em&gt;private caches&lt;/em&gt; such as the browser cache.&lt;/p&gt;
&lt;h2&gt;Whatâs fresh?&lt;/h2&gt;
&lt;p&gt;Whenever the cache receives a request, it must figure out if the cached response is still &lt;strong&gt;fresh&lt;/strong&gt; and can therefore be reused without incurring the performance tax of an HTTP request, or whether it has gone &lt;strong&gt;stale&lt;/strong&gt; and should be validated with the server.&lt;/p&gt;
&lt;p&gt;To decide on freshness, the cache compares the age of the response to the responseâs so-called freshness timeline.&lt;/p&gt;
&lt;p&gt;The age of a cached response is the time elapsed since it was last generated or revalidated by the origin server. To the time spent in its own cache, the browser will add any &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/Age"&gt;&lt;code&gt;Age: &amp;lt;seconds&amp;gt;&lt;/code&gt;&lt;/a&gt; header received from intermediary caches.&lt;/p&gt;
&lt;p&gt;The freshness timeline is a duration beyond which the cached response is to be considered stale. Itâs usually signaled by the server via the appropriate response headers, but may also be guesstimated by the cache in the absence of explicit, valid cues. In order of precedence:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the server establishes a freshness timeline, in seconds, with the &lt;a href="#max-age-response"&gt;&lt;code&gt;Cache-Control: max-age=&amp;lt;number&amp;gt;&lt;/code&gt;&lt;/a&gt; directive on the response; otherwise,&lt;/li&gt;
&lt;li&gt;the cache falls back to computing the interval between the &lt;code&gt;Expires: &amp;lt;date&amp;gt;&lt;/code&gt; and &lt;code&gt;Date: &amp;lt;date&amp;gt;&lt;/code&gt; response headers, if available; otherwise,&lt;/li&gt;
&lt;li&gt;if thereâs no &lt;code&gt;Expires&lt;/code&gt; header, the response lacks an explicit expiration, and a heuristic freshness based on the &lt;code&gt;Last-Modified&lt;/code&gt; response header might be applicable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For shared caches, the special &lt;a href="#s-maxage-response"&gt;&lt;code&gt;s-maxage=&amp;lt;number&amp;gt;&lt;/code&gt;&lt;/a&gt; directive takes precedence over all others.&lt;/p&gt;
&lt;h2&gt;Going past expiration&lt;/h2&gt;
&lt;p&gt;Just because a response has gone stale, it doesnât mean it needs to be thrown out.&lt;/p&gt;
&lt;p&gt;When it receives a request for a stale cached response, the cache should validate it with its upstream server. Although validation always generates an HTTP request, it avoids a data transfer when thereâs no newer version of the cached response on the server, so it can still be faster than a regular request.&lt;/p&gt;
&lt;p&gt;Validation uses a mechanism known as a conditional HTTP request, which includes one or more special headers called preconditions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if the precondition with the highest precedence is met, the server responds with &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Status/200"&gt;HTTP 200 OK&lt;/a&gt; and an updated response body; otherwise,&lt;/li&gt;
&lt;li&gt;it responds with &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Status/304"&gt;HTTP 304 Not Modified&lt;/a&gt; and an empty body, confirming the existing response can be reused.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To generate the preconditions needed for these conditional requests, which the server uses to compare the cached response to the freshest version available, responses must be tagged in a way thatâs unique to each version:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;historically, this was done with the &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/Last-Modified"&gt;&lt;code&gt;Last-Modified: &amp;lt;date&amp;gt;&lt;/code&gt;&lt;/a&gt; header, corresponding to the latest update to the content;&lt;/li&gt;
&lt;li&gt;a more flexible and robust alternative is the &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/ETag"&gt;&lt;code&gt;ETag: "&amp;lt;value&amp;gt;"&lt;/code&gt;&lt;/a&gt; header, which stores an arbitrary ASCII string that uniquely identifies the response. This string is usually a hash incorporating one or more aspects: the modification time, the file size, and the file content.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When performing the validation, the cached response headers are mirrored as preconditions for the conditional request:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Last-Modified: &amp;lt;date&amp;gt;&lt;/code&gt; becomes &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/If-Modified-Since"&gt;&lt;code&gt;If-Modified-Since: &amp;lt;date&amp;gt;&lt;/code&gt;&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ETag: "&amp;lt;value&amp;gt;"&lt;/code&gt; becomes &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/If-None-Match"&gt;&lt;code&gt;If-None-Match: "&amp;lt;value&amp;gt;"&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When both preconditions are present, only &lt;code&gt;If-None-Match&lt;/code&gt; is evaluated.&lt;/p&gt;
&lt;p&gt;Regardless of the result of the validation request, the cached response headers are updated with the new values received from the server, and the fresh-o-meter on the cached response is reset.&lt;/p&gt;
&lt;p&gt;Certain caches may be set up to serve stale responses in some circumstances, such as when losing the connection to the server or in the event of an HTTP 5xx server error. There are also &lt;code&gt;Cache-Control&lt;/code&gt; directives that influence how stale responses are handled, covered in the next section.&lt;/p&gt;
&lt;h2&gt;Cache-Control response directives&lt;/h2&gt;
&lt;h3&gt;
&lt;p&gt;&lt;code&gt;max-age=&amp;lt;number&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;max-age&lt;/code&gt; response directive defines the responseâs freshness timeline in seconds, after which the response should be considered stale. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-max-age-2"&gt;â RFC 9111 Â§ 5.2.2.1&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;
&lt;p&gt;&lt;code&gt;must-revalidate&lt;/code&gt;&lt;/p&gt;
&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;must-revalidate&lt;/code&gt; response directive indicates that the cache must not reuse a stale response until itâs been successfully validated by the origin server. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-must-revalidate"&gt;â RFC 9111 Â§ 5.2.2.2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If the server throws an error, the cache must surface that instead of reusing a stale response. If the cache is disconnected, it must produce an error with the &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Status/504"&gt;HTTP 504 Gateway Timeout&lt;/a&gt; status code, or another more applicable error code.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Side effects:&lt;/strong&gt; &lt;code&gt;must-revalidate&lt;/code&gt; is one of the directives, along with &lt;code&gt;s-maxage&lt;/code&gt; and &lt;code&gt;public&lt;/code&gt;, that allow shared caches to &lt;a href="#caching-authenticated-responses"&gt;store and reuse a response to a request containing an &lt;code&gt;Authorization&lt;/code&gt; header&lt;/a&gt;, which they are generally prohibited from doing.&lt;/p&gt;
&lt;h3&gt;
&lt;p&gt;&lt;code&gt;no-cache&lt;/code&gt;&lt;/p&gt;
&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;no-cache&lt;/code&gt; response directive indicates that the cache must not reuse &lt;em&gt;any&lt;/em&gt; response until itâs successfully validated by the origin server. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-no-cache-2"&gt;â RFC 9111 Â§ 5.2.2.4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is similar to &lt;code&gt;must-revalidate&lt;/code&gt; but refers to all cached responses, not just stale ones. In effect, &lt;code&gt;no-cache&lt;/code&gt; is a sort of &lt;code&gt;max-age=0, must-revalidate&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;
&lt;p&gt;&lt;code&gt;no-store&lt;/code&gt;&lt;/p&gt;
&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;no-store&lt;/code&gt; response directive indicates that private and shared caches must not store any part of the request or the response, and to never reuse the response. The standard is quick to warn that the effect is not guaranteed:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;âMUST NOT storeâ in this context means that the cache MUST NOT intentionally store the information in non-volatile storage and MUST make a best-effort attempt to remove the information from volatile storage as promptly as possible after forwarding it. This directive is not a reliable or sufficient mechanism for ensuring privacy. In particular, malicious or compromised caches might not recognize or obey this directive, and communications networks might be vulnerable to eavesdropping. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-no-store-2"&gt;â RFC 9111 Â§ 5.2.2.4&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Side effects:&lt;/strong&gt; The directive can also influence non-HTTP caches. Most browsers will &lt;a href="https://web.dev/articles/bfcache#minimize-no-store"&gt;exclude from the back/forward cache&lt;/a&gt; pages having the &lt;code&gt;no-store&lt;/code&gt; response directive. Chrome, however, has recently started to make &lt;em&gt;some&lt;/em&gt; of these pages &lt;a href="https://developer.chrome.com/docs/web-platform/bfcache-ccns"&gt;eligible for bfcache&lt;/a&gt; when the browser deems it safe.&lt;/p&gt;
&lt;h3&gt;
&lt;p&gt;&lt;code&gt;must-understand&lt;/code&gt;&lt;/p&gt;
&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;must-understand&lt;/code&gt; response directive indicates that the cache shouldnât store or reuse responses with HTTP status codes whose semantics the cache doesnât understand and conform to. The directive is meant to future-proof existing implementations from status codes that might have special requirements in regards to caching. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-must-understand"&gt;â RFC 9111 Â§ 5.2.2.3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Itâs recommended to use &lt;code&gt;must-understand, no-store&lt;/code&gt; together as a fallback, and caches are encouraged to ignore the &lt;code&gt;no-store&lt;/code&gt; directive if they do understand the semantics of the HTTP status code. This ensures older caches that donât recognize the &lt;code&gt;must-understand&lt;/code&gt; directive donât cache the response at all, although by 2025 this should be an exceedingly rare sight.&lt;/p&gt;
&lt;h3&gt;
&lt;p&gt;&lt;code&gt;private&lt;/code&gt;&lt;/p&gt;
&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;private&lt;/code&gt; response directive indicates that the response is meant for a single user. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-private"&gt;â RFC 9111 Â§ 5.2.2.7&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a shared cache must not store the response; and&lt;/li&gt;
&lt;li&gt;a private cache may store the response even if the response wouldnât otherwise be heuristically cacheable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;private&lt;/code&gt; directive can be used to guard against other directives that might inadvertently make &lt;a href="#caching-authenticated-responses"&gt;authenticated responses available to shared caches&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;
&lt;p&gt;&lt;code&gt;public&lt;/code&gt;&lt;/p&gt;
&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;public&lt;/code&gt; response directive indicates two things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a shared cache may &lt;a href="#caching-authenticated-responses"&gt;store and reuse a response to a request containing an &lt;code&gt;Authorization&lt;/code&gt; header&lt;/a&gt;, which itâs generally prohibited from doing; and&lt;/li&gt;
&lt;li&gt;a private cache may store the response even if the response wouldnât otherwise be heuristically cacheable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-public"&gt;â RFC 9111 Â§ 5.2.2.9&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;
&lt;p&gt;&lt;code&gt;s-maxage=&amp;lt;number&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;s-maxage=&amp;lt;number&amp;gt;&lt;/code&gt; response directive is analogous to &lt;code&gt;max-age&lt;/code&gt;, but only affects shared caches. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-s-maxage"&gt;â RFC 9111 Â§ 5.2.2.10&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The directive also incorporates the semantics of the &lt;code&gt;proxyârevalidate&lt;/code&gt; response directive, in that a shared cache must not use a stale response until it has been successfully validated with the origin server.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Side effects:&lt;/strong&gt; &lt;code&gt;s-maxage&lt;/code&gt; is one of the directives, along with &lt;code&gt;must-revalidate&lt;/code&gt; and &lt;code&gt;public&lt;/code&gt;, that allow shared caches to &lt;a href="#caching-authenticated-responses"&gt;store and reuse a response to a request containing an &lt;code&gt;Authorization&lt;/code&gt; header&lt;/a&gt;, which they are generally prohibited from doing.&lt;/p&gt;
&lt;h3&gt;
&lt;p&gt;&lt;code&gt;proxy-revalidate&lt;/code&gt;&lt;/p&gt;
&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;proxy-revalidate&lt;/code&gt; response directive is analogous to &lt;code&gt;must-revalidate&lt;/code&gt;, but only affects shared caches. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-proxy-revalidate"&gt;â RFC 9111 Â§ 5.2.2.8&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;
&lt;p&gt;&lt;code&gt;no-transform&lt;/code&gt;&lt;/p&gt;
&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;no-transform&lt;/code&gt; response directive indicates that intermediaries, regardless of whether they implement a cache or not, must not transform the response content, such as optimizing images or compressing stylesheets and scripts. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-no-transform-2"&gt;â RFC 9111 Â§ 5.2.2.6&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;
&lt;p&gt;&lt;code&gt;stale-while-revalidate=&amp;lt;number&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;stale-while-revalidate&lt;/code&gt; response directive was defined in &lt;a href="https://www.rfc-editor.org/rfc/rfc5861"&gt;RFC 5861: HTTP Cache-Control Extensions for Stale Content&lt;/a&gt; (2010). It indicates that the cache may use a cached response if it hasnât exceeded its freshness lifetime by more than the specified number of seconds.&lt;/p&gt;
&lt;p&gt;Whenever the presence of this directive causes a stale response to be served, the cache should trigger a background revalidation of the response.&lt;/p&gt;
&lt;p&gt;The author of the RFC, Mark Nottingham, &lt;a href="https://www.mnot.net/blog/2014/06/01/chrome_and_stale-while-revalidate"&gt;has written a rationale&lt;/a&gt; for this directive.&lt;/p&gt;
&lt;h3&gt;
&lt;p&gt;&lt;code&gt;stale-if-error=&amp;lt;number&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;/h3&gt;
&lt;p&gt;Also defined in the RFC 5861 extension, the &lt;code&gt;stale-if-error&lt;/code&gt; response directive indicates that the cache may use a cached response if it hasnât exceeded its freshness lifetime by more than the specified number of seconds, if the attempt to validate the stale response results in an error.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://cache-tests.fyi/"&gt;HTTP Caching Tests&lt;/a&gt; suggests this directive is not well supported.&lt;/p&gt;
&lt;h2&gt;Cache-Control request directives&lt;/h2&gt;
&lt;p&gt;As web developers, we most often deal with &lt;code&gt;Cache-Control&lt;/code&gt; in HTTP responses, but this header can also be included on HTTP requests. Browsers, for example, use them when the user refreshes the page.&lt;/p&gt;
&lt;p&gt;When used in HTTP requests, &lt;code&gt;Cache-Control&lt;/code&gt; directives express the clientâs preference in regards to the freshness or age of the response. Caches reconcile these requests with the &lt;code&gt;Cache-Control&lt;/code&gt; response directives of its cached responses.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: the &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/RequestInit#cache"&gt;&lt;code&gt;cache&lt;/code&gt; option&lt;/a&gt; for &lt;code&gt;fetch()&lt;/code&gt; has a separate set of values that map to &lt;code&gt;Cache-Control&lt;/code&gt; request directives, but the mappings are not always intuitive. For example, &lt;code&gt;cache: 'no-cache'&lt;/code&gt; maps to &lt;code&gt;Cache-Control: max-age=0&lt;/code&gt;. For the curious, the mappings are &lt;a href="https://fetch.spec.whatwg.org/#http-network-or-cache-fetch"&gt;defined here&lt;/a&gt;. You can always set your &lt;code&gt;Cache-Control&lt;/code&gt; headers directly with the &lt;code&gt;headers&lt;/code&gt; option.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;code&gt;max-age=&amp;lt;number&amp;gt;&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;max-age&lt;/code&gt; request directive indicates that the client wants a fresh response whose age is less than or equal to the specified number of seconds. When combined with &lt;code&gt;max-stale&lt;/code&gt;, the client will accept some stale responses. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-max-age"&gt;â RFC 9111 Â§ 5.2.1.1&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;code&gt;max-stale=&amp;lt;number&amp;gt;&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;max-stale&lt;/code&gt; request directive indicates that the client will accept a stale response that has exceeded its freshness lifetime by no more than the specified number of seconds. When used without an argument, &lt;code&gt;max-stale&lt;/code&gt; indicates that the client will accept any stale response, no matter how old. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#section-5.2.1.2"&gt;â RFC 9111 Â§ 5.2.1.2&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;code&gt;min-fresh=&amp;lt;number&amp;gt;&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;min-fresh&lt;/code&gt; request directive indicates that the client prefers a response that still has at least the specified number of seconds of freshness left. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-min-fresh"&gt;â RFC 9111 Â§ 5.2.1.3&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;code&gt;no-cache&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;no-cache&lt;/code&gt; request directive indicates that the client prefers caches not to use a stored response without successfully validating it with the origin server. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-no-cache"&gt;â RFC 9111 Â§ 5.2.1.4&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;code&gt;no-store&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;no-store&lt;/code&gt; request directive indicates that a cache must not store any part of either this request or any response to it. The same caveats as to &lt;a href="#no-store-response"&gt;its response counterpart&lt;/a&gt; apply. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-no-store"&gt;â RFC 9111 Â§ 5.2.1.5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If a cache serves this request with a response that was previously stored, the &lt;code&gt;no-store&lt;/code&gt; request directive doesnât cause the cache to remove the response after serving it.&lt;/p&gt;
&lt;h3&gt;&lt;code&gt;no-transform&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;no-transform&lt;/code&gt; request directive indicates that the client is asking for intermediaries to avoid transforming the content. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-no-transform"&gt;â RFC 9111 Â§ 5.2.1.6&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;code&gt;only-if-cached&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;only-if-cached&lt;/code&gt; request directive indicates that the client only wants a stored response. Caches should respond with either a stored response that satisfies all the other constraints, or an HTTP 504 Gateway Timeout status code. &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-only-if-cached"&gt;â RFC 9111 Â§ 5.2.1.7&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;code&gt;stale-if-error=&amp;lt;number&amp;gt;&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Similarly to &lt;a href="#stale-if-error-response"&gt;its response counterpart&lt;/a&gt;, the &lt;code&gt;stale-if-error&lt;/code&gt; request directive indicates that the client will accept a stale response that has exceeded its freshness lifetime by no more than the specified number of seconds, if an attempt to validate it resulted in a server error.&lt;/p&gt;
&lt;h2&gt;Browser refresh mechanisms&lt;/h2&gt;
&lt;p&gt;Browsers typically offer two refresh mechanisms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;soft reloads&lt;/em&gt;, triggered by the reload button, a corresponding menu item and keyboard shortcut, and the pull-to-refresh gesture in mobile browsers, are meant to get an updated representation of the page, for example getting the latest posts on a social media timeline.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;hard reloads&lt;/em&gt;, enabled with a modifier key, skip the cache altogether and are meant to fix interrupted loads, outdated cached responses, and other broken states.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hereâs how some browsers on macOS implement these behaviors.&lt;/p&gt;
&lt;h3&gt;Soft reloads&lt;/h3&gt;
&lt;p&gt;Triggered by Ctrl + R on Windows/Linux and Command + R on macOS.&lt;/p&gt;
&lt;p&gt;Firefox triggers a conditional request to revalidate the cached response for the main resource (the HTML file). Sub-resources such as stylesheets, scripts, and images are reloaded as usual, according to their cache directives.&lt;/p&gt;
&lt;p&gt;Chrome behaves similarly, with the difference that the validation request for the main resource also includes a &lt;code&gt;Cache-Control: max-age=0&lt;/code&gt; directive (which canât hurt).&lt;/p&gt;
&lt;p&gt;Instead of revalidating its cached response, Safari performs a non-conditional request for the main resource, then loads sub-resources as usual.&lt;/p&gt;
&lt;h3&gt;Hard reloads&lt;/h3&gt;
&lt;p&gt;Triggered by Ctrl + Shift + R on Windows/Linux and Command + Shift + R on macOS except Safari, which uses Command + Option + R. (If youâve applied your muscle memory to Safari before, you know all too well that the common shortcut opens Reader Mode insteadâ¦)&lt;/p&gt;
&lt;p&gt;On a hard reload, all three browsers trigger non-conditional requests with the &lt;code&gt;Cache-Control: no-cache&lt;/code&gt; directive on the HTML page and its sub-resources.&lt;/p&gt;
&lt;p&gt;Curiously, once you perform a hard reload in Safari, subsequent soft reloads will still use the &lt;code&gt;Cache-Control: no-cache&lt;/code&gt; request directive to fetch the main resource, which is probably an unintended, but otherwise benign behavior.&lt;/p&gt;
&lt;h3&gt;The &lt;code&gt;immutable&lt;/code&gt; response directive&lt;/h3&gt;
&lt;p&gt;Reloading a web page didnât always work like this. Historically, when performing a soft reload, all the sub-resources would be revalidated along with the main resource, in effect freshening up the cache for the current page.&lt;/p&gt;
&lt;p&gt;Circa 2015, Facebook was seeing several HTTP 304 Not Modified responses on long-lived resources like scripts and stylesheets whenever a user would refresh their feed page with the browserâs reload button.&lt;/p&gt;
&lt;p&gt;To address this issue, Patrick McManus from Mozilla &lt;a href="https://bitsup.blogspot.com/2016/05/cache-control-immutable.html"&gt;proposed&lt;/a&gt; the &lt;code&gt;immutable&lt;/code&gt; response directive, which later became &lt;a href="https://www.rfc-editor.org/rfc/rfc8246"&gt;RFC 8246: HTTP Immutable Responses&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The directive indicates that the origin server wonât update a resource during the freshness lifetime of the cached response, so a cache shouldnât issue conditional requests for responses that are still fresh when the user reloads the page, unless the user really, really wants an updated response (e.g. a hard reload).&lt;/p&gt;
&lt;p&gt;Around the time that &lt;a href="https://hacks.mozilla.org/2017/01/using-immutable-caching-to-speed-up-the-web/"&gt;support for &lt;code&gt;immutable&lt;/code&gt;&lt;/a&gt; landed in Firefox 49 and Facebook began to use it to great effect, Chrome introduced a &lt;a href="https://blog.chromium.org/2017/01/reload-reloaded-faster-and-leaner-page_26.html"&gt;new way to perform reloads&lt;/a&gt; that solved the problem without introducing additional directives: instead of revalidating everything on a soft reload, just revalidate the main resource and load sub-resources as usual. Safari switched over to the new reload policy soon after [&lt;a href="https://bugs.webkit.org/show_bug.cgi?id=169756"&gt;Webkit#169756&lt;/a&gt;], and Firefox eventually did with &lt;a href="https://www.firefox.com/en-US/firefox/100.0/releasenotes/"&gt;Firefox 100&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;That leaves the &lt;code&gt;immutable&lt;/code&gt; directive in an awkward place. Safari added support [&lt;a href="https://bugs.webkit.org/show_bug.cgi?id=167497"&gt;Webkit#167497&lt;/a&gt;] but Chrome representatives remain unconvinced that it offers a significant benefit on top of the current reload behavior [&lt;a href="https://issues.chromium.org/issues/41253661"&gt;Chromium#41253661&lt;/a&gt;].&lt;/p&gt;
&lt;h2&gt;
Caching responses to authenticated requests
&lt;/h2&gt;
&lt;p&gt;One of the more confusing aspects of HTTP caching is how various &lt;code&gt;Cache-Control&lt;/code&gt; response directives affect the way shared caches treat responses to requests that contain an &lt;a href="https://www.rfc-editor.org/rfc/rfc9110#name-authorization"&gt;&lt;code&gt;Authorization&lt;/code&gt; header&lt;/a&gt;, which are understood as specific to a single user.&lt;/p&gt;
&lt;p&gt;As per &lt;a href="https://www.rfc-editor.org/rfc/rfc9111#name-storing-responses-to-authen"&gt;RFC 9111 Â§ 3.5&lt;/a&gt;, shared caches are not allowed to store these responses unless the response contains a Cache-Control field with a response directive that allows it to be stored by a shared cache, and the cache conforms to the requirements of that directive for that response.&lt;/p&gt;
&lt;p&gt;The three directives that enable shared caches to store authenticated responses, and which must therefore be carefully evaluated before deploying, are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#public-response"&gt;&lt;code&gt;public&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#s-maxage-response"&gt;&lt;code&gt;s-maxage=&amp;lt;number&amp;gt;&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#must-revalidate-response"&gt;&lt;code&gt;must-revalidate&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Conversely, a &lt;a href="#private-response"&gt;&lt;code&gt;private&lt;/code&gt;&lt;/a&gt; directive prevents any other directive from making authenticated responses eligible to shared caches.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I wrote this article to clarify for myself what the various cache directives stand for and how they overlap and interact. It only covers the main ideas, without delving into the more obscure corners of HTTP semantics. I approached the subject with a âclear cacheâ, and mainly used the normative references (RFC 9111 and its extensions), aided by various guides from different eras:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.mnot.net/cache_docs/"&gt;Caching Tutorial for Web Authors and Webmasters&lt;/a&gt; (1998â) by Mark Nottingham;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jakearchibald.com/2016/caching-best-practices/"&gt;Caching best practices &amp;amp; max-age gotchas&lt;/a&gt; (2016) by Jake Archibald;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://web.dev/articles/http-cache"&gt;Prevent unnecessary network requests with the HTTP Cache&lt;/a&gt; (2018) by Ilya Grigorik and Jeff Posnik;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://csswizardry.com/2019/03/cache-control-for-civilians/"&gt;Cache control for civilians&lt;/a&gt; (2019â2025) and &lt;a href="https://csswizardry.com/2025/03/why-do-we-have-a-cache-control-request-header/"&gt;Why Do We Have a Cache-Control Request Header?&lt;/a&gt; (2025) by Harry Roberts;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://grayduck.mn/2021/09/13/cache-control-recommendations/"&gt;Cache-Control Recommendations&lt;/a&gt; (2021) by April King;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/Caching"&gt;Web Caching&lt;/a&gt; on MDN.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Whatâs interesting about these guides is that the recommendations donât just encode an interpretation of the specs, but also incorporate safeguards against non-conformant or outdated browser caches and intermediares.&lt;/p&gt;
&lt;p&gt;In light of developments as recent as 2022, it would be cool to figure out to what extent things have improved, and which of these safeguards can be discarded. &lt;a href="https://cache-tests.fyi/"&gt;HTTP Caching Tests&lt;/a&gt; seems to be a good resource for assessing the situation.&lt;/p&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://danburzo.ro/http-caching-refresher/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 19:41:39 +0000</pubDate>
    </item>
    <item>
      <title>Fixed-Wing Runway Design</title>
      <link>https://www.wbdg.org/building/aviation/fixed-wing-runway-design</link>
      <description>Runways are the primary operating surface at airfields and essential to fixed-wing aircraft operations.  Fixed-wing runways are built in a variety of lengths, widths, and pavement types depending on a large number of factors, including:</description>
      <content:encoded>&lt;article class="node node--type-wbdg-pages node--view-mode-full clearfix" data-history-node-id="1164"&gt;








&lt;h2&gt;Overview&lt;/h2&gt;

&lt;h4&gt;Within This Page&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#over"&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#desc"&gt;Runway Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rcas"&gt;Relevant Codes and Standards&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Runways are the primary operating surface at airfields and essential to fixed-wing aircraft operations.  Fixed-wing runways are built in a variety of lengths, widths, and pavement types depending on a large number of factors, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Aircraft Type (Operating Characteristics, Wingspan, Weight)&lt;/li&gt;
&lt;li&gt;Mission&lt;/li&gt;
&lt;li&gt;Number of Operations&lt;/li&gt;
&lt;/ul&gt;


&lt;img alt="C-17 touching down on runway, Pope Army Airfield" src="https://www.wbdg.org/images/fixedwing_01.jpg"/&gt;
&lt;p&gt;C-17 touchdown on runway
&lt;em&gt;Source: Defense Visual Information Distribution Service&lt;/em&gt;&lt;/p&gt;


&lt;p&gt;The primary reference describing requirements for DoD Fixed Wing Runways is Chapter 3 of &lt;a href="https://www.wbdg.org/ffc/dod/unified-facilities-criteria-ufc/ufc-3-260-01"&gt;UFC 3-260-01, &lt;em&gt;Airfield and Heliport Planning and Design&lt;/em&gt;&lt;/a&gt;. Other special use runways (Landing Zones, STOVL Facilities and UAS runways) are defined in Chapters 7, 8 and 9 of the UFC.&lt;/p&gt;


&lt;img alt="typical airport diagram, JB Andrews" src="https://www.wbdg.org/images/fixedwing_02.png"/&gt;
&lt;p&gt;Typical airport diagram&lt;br/&gt;&lt;a href="https://www.wbdg.org/images/fixedwing_02_fullsize.png"&gt;&lt;em&gt;(View enlarged diagram)&lt;/em&gt;&lt;/a&gt;
&lt;em&gt;Source: Federal Aviation Administration&lt;/em&gt;&lt;/p&gt;


&lt;p&gt;Each DoD Service classifies fixed-wing runways into two primary categories—Class A or Class B—depending primarily on the type(s) of aircraft using the runway.  That classification drives many required construction features of the runway, including length, width, transverse slopes, and longitudinal grades.  It should be noted that civilian runways are classified by a very different system, defined in Federal Aviation Administration Advisory Circular 150/5300-13, &lt;em&gt;Airport Design&lt;/em&gt;, with a classification system based on the critical aircraft's wingspan and landing approach speed.&lt;/p&gt;
&lt;p&gt;Fixed-wing runways are usually constructed with a rigid pavement surface (Portland cement concrete) or flexible pavement surface (asphalt cement concrete), but in special cases may be surfaced with compacted soil, aggregates, or segmented aluminum mats, depending on the mission requirements.&lt;/p&gt;
&lt;p&gt;Taxiways are used by aircraft to enter and exit a runway and transit to an aircraft parking position.  Taxiways connect directly to runways, most often at the runway ends.&lt;/p&gt;
&lt;p&gt;In addition to the runway pavement surface, there are many ground surface areas immediately surrounding the runway that improve safety for the operating aircraft by limiting the risk of damage should an aircraft accidentally depart from the runway surface.&lt;/p&gt;
&lt;p&gt;Not only must objects be restricted from close proximity to the runway surface, the airspace surrounding a runway must also be protected from development that encroaches on the airspace needed for safe aircraft operations.  The protected areas are defined by what are known as "imaginary surfaces."  These are generally planar or conical surfaces in the air, defined by a length, width, and slope up to a specified elevation.&lt;/p&gt;
&lt;h2&gt;Runway Design&lt;/h2&gt;
&lt;p&gt;There are many different factors that impact runway design and are dependent on many different data inputs.  All components should be determined early in the planning process to avoid unexpected challenges or constraints later in the design development.
&lt;/p&gt;&lt;h3&gt;Runway Heading&lt;/h3&gt;
&lt;p&gt;Runways are oriented to provide the best conditions for an aircraft on takeoff and landing.  An aircraft moving directly into the wind has the highest airspeed across the wing, thereby increasing lift, and the least sideways forces on the aircraft.  Therefore, the ideal orientation of the runway (often referred to as the &lt;em&gt;heading&lt;/em&gt;) is determined by analyzing historical wind data (10 years or more) at a location.  Wind heading and speed data is graphically displayed on a &lt;em&gt;Wind Rose&lt;/em&gt;, and this tool can be used to determine a runway heading that provides the greatest percentage of time with favorable winds for aircraft operations.  The objective is to find a heading that allows operations more than 95% of the time with a crosswind less than 19.5 km/hr (10.5 knot).  When a single runway cannot provide this coverage, then a crosswind runway may be required.  UFC 3-260-01, Appendix B, Section 4 explains this process in detail.&lt;/p&gt;


&lt;img alt="windrose used for runway heading analysis" src="https://www.wbdg.org/images/fixedwing_03.png"/&gt;
&lt;p&gt;Windrose used for Runway Heading Analysis
&lt;em&gt;Source: UFC 3-260-01, Appendix B, Section 4&lt;/em&gt;&lt;/p&gt;


&lt;p&gt;In addition to prevailing winds, other factors may affect the selection of a runway heading, including terrain, obstructions, restricted airspace, noise effects, built-up areas, or operational procedures.&lt;/p&gt;
&lt;h3&gt;Runway Length&lt;/h3&gt;
&lt;p&gt;Each service determines the required runway length using their own procedures that generally take into account the mission aircraft performance requirements, altitude, and typical temperature range.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Air Force:&lt;/strong&gt; For both Class A and B runways, the length will be determined by the Major Command responsible for the airfield.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Army:&lt;/strong&gt; The Class A runway length requirement is listed in UFC 3-260-01, Table 3-3, but for Class B Runways, runway length is determined by the using aircraft operator, in conjunction with HQ Department of the Army.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Navy and Marine Corps:&lt;/strong&gt; UFC 2-000-05N describes the process for determining runway length to accommodate the critical aircraft in both takeoff and landing operations under stated load and environmental conditions.  Minimum lengths by aircraft type are listed in Table 11110-1; then adjusted for altitude, temperature, and effective gradient with a safety factor applied to the result.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Runway Width&lt;/h3&gt;
&lt;p&gt;For the DoD, the runway width is dependent on the type of aircraft planned to use the airfield.  Table 3-1 in UFC 3-260-01 classifies aircraft into Class A or B.  Then Table 3-2 defines the required runway width for each class.  There are some exceptions to the standard widths, as defined in Item 2.  Runways for Bombers like the B-52 are 300-ft wide, and some training runways for small aircraft are only 75-ft wide.  Special-use runways (&lt;a href="https://www.wbdg.org/building-types/aviation/landing-zone-design"&gt;Landing Zones&lt;/a&gt;, Short Takeoff and Vertical Landing (STOVL) facilities, and Unmanned Aircraft Systems (UAS) runways have their own requirements, defined in UFC 3-260-01, Chapters 7, 8 and 9.)&lt;/p&gt;
&lt;h3&gt;Clear Zones and Accident Potential Zones&lt;/h3&gt;
&lt;p&gt;Clear Zones are areas on the ground at the ends of runways that have a high potential for accidents.  Other uses of the clear zone are restricted to be compatible with aircraft operations.  Each DoD service defines clear zones differently, so UFC 3-260-01, Table 3-5 should be carefully considered to provide the appropriate dimensions.  Clear Zones should be owned and controlled by the agency to prevent incompatible development within the areas.&lt;/p&gt;
&lt;p&gt;Accident Potential Zones (APZs) are land-use control areas, mandated by the Air Installation Compatibility Use Zones (AICUZ) program, and intended to promote only compatible development in areas under the approach and departure surfaces for fixed-wing runways.  The APZs usually stretch beyond the base property boundaries, so coordination with the local communities is essential to avoid building high population development in these areas where an aircraft accident is more likely to occur.&lt;/p&gt;
&lt;h3&gt;Imaginary Surfaces and Obstructions&lt;/h3&gt;
&lt;p&gt;The area above the ground surrounding a runway that must be kept clear of objects that might damage an aircraft operating around the airfield (approach, departure or circling) is defined by Imaginary Surfaces (planar or conical surfaces in the airspace).  An object that projects above an imaginary surface is an obstruction.  Typical terminology for imaginary surfaces includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Primary surface&lt;/li&gt;
&lt;li&gt;Approach-departure surface&lt;/li&gt;
&lt;li&gt;Inner horizontal surface&lt;/li&gt;
&lt;li&gt;Conical surface&lt;/li&gt;
&lt;li&gt;Outer horizontal surface&lt;/li&gt;
&lt;li&gt;Transitional surface&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of these surfaces is defined in UFC 3-260-01, Chapter 3.&lt;/p&gt;


&lt;img alt="isometric view of airspace around fixed-wing runway" src="https://www.wbdg.org/images/fixedwing_04.jpg"/&gt;
&lt;p&gt;Isometric view of airspace around Fixed-Wing Runway &lt;a href="https://www.wbdg.org/images/fixedwing_04_fullsize.jpg"&gt;&lt;em&gt;(View enlarged)&lt;/em&gt;&lt;/a&gt;
&lt;em&gt;Source: UFC 3-260-01, Chapter 3&lt;/em&gt;&lt;/p&gt;


&lt;h3&gt;Grades&lt;/h3&gt;
&lt;p&gt;There are strict requirements for the slope or grade of the runway pavement surfaces and ground surfaces surrounding the runway.  These surfaces are dependent on the performance requirements of the aircraft (Class A or B) and to promote good drainage as well as aircraft safety in the event that an aircraft accidentally departs from the runway surface.  UFC 3-260-01, Chapter 3 defines the requirements for the grades of the following items:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Longitudinal Grades and Rate of Grade Change for Runways, Shoulders, Overruns and Lateral Clearance Zone&lt;/li&gt;
&lt;li&gt;Transverse Slopes for Runway, Shoulders, Overruns and Lateral Clearance Zone&lt;/li&gt;
&lt;li&gt;Longitudinal and Transverse Grades in the Clear Zone&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Pavement Thickness Design&lt;/h3&gt;
&lt;p&gt;Runway pavements for military airfields are designed following the procedures described in UFC 3-260-02, &lt;em&gt;Pavement Design for Airfields&lt;/em&gt;.  Typically, runways are constructed with a flexible pavement structure (asphaltic concrete pavement) or rigid pavement structure (Portland cement concrete).&lt;/p&gt;
&lt;p&gt;For pavement thickness design, each Service divides an airfield and the runway into different types of traffic areas, (e.g., Type A, Type C, Primary, or Secondary).  The traffic type then correlates to different traffic patterns (aircraft load, number of repetitions, and the typical "wander" of the aircraft traffic).  When traffic is combined with the subgrade strength, the required pavement thickness can be determined.&lt;/p&gt;
&lt;h3&gt;Pavement Markings&lt;/h3&gt;
&lt;p&gt;To improve the visibility of runways during both day and night, standard markings are painted on the pavement surface.  There are three primary runway marking schemes—Visual, Non-Precision Instrument, and Precision Instrument—with progressively more markings.  Runway markings are white (as compared to yellow markings on taxiways) and include embedded glass beads to provide reflectivity.  At each end, the runway heading number is painted, and when there are one or more parallel runways, a left (L), right (R) or center (C) designation is also applied.&lt;/p&gt;


&lt;img alt="marking schemes for fixed-wing runways" src="https://www.wbdg.org/images/fixedwing_05.png"/&gt;
&lt;p&gt;Marking schemes for Fixed-Wing Runways&lt;br/&gt;&lt;a href="https://www.wbdg.org/images/fixedwing_05_fullsize.png"&gt;&lt;em&gt;(View enlarged)&lt;/em&gt;&lt;/a&gt;
&lt;em&gt;Source: UFC 3260-04, Chapter 5&lt;/em&gt;&lt;/p&gt;


&lt;p&gt;UFC 3-260-04, &lt;em&gt;Airfield and Heliport Marking&lt;/em&gt;, fully defines pavement marking requirements for Army and Air Force fixed-wing runways.  Navy and Marine Corps requirements are defined in NAVAIR 51-50AAA-2, &lt;em&gt;General Requirements for Shorebased Airfield Marking and Lighting&lt;/em&gt;.  Each Service's requirements very closely match the Federal Aviation Administration runway marking requirements.&lt;/p&gt;
&lt;h3&gt;Runway Lighting and Signs&lt;/h3&gt;
&lt;p&gt;For low visibility and night operations on runways, lights and signs are used to provide visibility of the runway to pilots when operating on the ground and in the air.  The most basic fixed-wing runway lighting system consists of edge lights, threshold lights and end lights, used to outline the lateral and longitudinal limits of the usable surface of the runway.  These lights are required for visual flight rules (VFR) night operations and for all categories of instrument operations.  In some cases where minimal visibility operational capability is needed, the runway perimeter lighting is augmented with touchdown zone and centerline lighting in-pavement light fixtures.&lt;/p&gt;


&lt;img alt="runway threshold lighting and markings" src="https://www.wbdg.org/images/fixedwing_06.jpg"/&gt;
&lt;p&gt;Runway threshold lighting and markings
&lt;em&gt;Source: Crawford, Murphy &amp;amp; Tilly, Inc.&lt;/em&gt;&lt;/p&gt;


&lt;p&gt;Approach light systems provide visual guidance to pilots aligning their aircraft with the runway and attempting final corrections before landing at night or during low visibility.  There are several different types of approach lighting systems (MALSR, SSALR, ALSF-1, ALSF-2), each with a different number of lights and different configurations.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.wbdg.org/ffc/dod/unified-facilities-criteria-ufc/ufc-3-535-01"&gt;UFC 3-535-01, &lt;em&gt;Visual Air Navigation Facilities&lt;/em&gt;&lt;/a&gt;, fully defines lighting and signage requirements for Army and Air Force fixed-wing runways.  Navy and Marine Corps requirements are defined in &lt;a href="https://www.wbdg.org/ffc/dod/supplemental-technical-documents/tsewg-navair-51-50AAA-2"&gt;NAVAIR 51-50AAA-2, &lt;em&gt;General Requirements for Shorebased Airfield Marking and Lighting&lt;/em&gt;&lt;/a&gt;.  Each Service's requirements very closely match the Federal Aviation Administration runway lighting and signage requirements.&lt;/p&gt;
&lt;h3&gt;Electronic Navigational Aids (NAVAIDs)&lt;/h3&gt;
&lt;p&gt;Some aircraft are equipped with electronic devices that can use radio signals to provide direction, distance, and glide slope data to help the pilot guide the aircraft to the runway.  These systems are called NAVAIDs and consist of a wide variety of antennas installed in various configurations surrounding the runway.  For example, an Instrument Landing System (ILS) consists of a Localizer antenna and Glide Slope antenna.  The Localizer transmits a radio signal down the centerline of the runway into the approach zone, and the pilot can use the signal to align on the runway.  The Glide Slope transmits a radio signal upwards from the runway surface at the correct approach angle the aircraft should follow to touchdown at the appropriate location on the runway surface.  Many different types of NAVAID systems have been developed over the years and are deployed at DoD installations.  UFC 4-141-10, Airfield Operations Support Facilities describes the different types of NAVAIDs, including installation requirements for each system.&lt;/p&gt;
&lt;h2&gt;Relevant Codes and Standards&lt;/h2&gt;
&lt;h3&gt;Federal Aviation Administration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.faa.gov/airports/resources/advisory_circulars/index.cfm/go/document.current/documentNumber/150_5300-13"&gt;Advisory Circular 150/5300-13 &lt;em&gt;Airport Design&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Naval Air Systems Command&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.wbdg.org/ffc/dod/supplemental-technical-documents/tsewg-navair-51-50AAA-2"&gt;NAVAIR 51-50AAA-2 &lt;em&gt;General Requirements for Shorebased Airfield Marking and Lighting&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Unified Facility Criteria&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.wbdg.org/ffc/dod/unified-facilities-criteria-ufc/ufc-2-000-05n"&gt;UFC 2-000-05N &lt;em&gt;Facility Planning for Navy and Marine Corps Shore Installations&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.wbdg.org/ffc/dod/unified-facilities-criteria-ufc/ufc-3-260-01"&gt;UFC 3-260-01 &lt;em&gt;Airfield and Heliport Planning and Design&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.wbdg.org/ffc/dod/unified-facilities-criteria-ufc/ufc-3-260-02"&gt;UFC 3-260-02 &lt;em&gt;Pavement Design for Airfields&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.wbdg.org/ffc/dod/unified-facilities-criteria-ufc/ufc-3-260-04"&gt;UFC 3-260-04 &lt;em&gt;Airfield and Heliport Marking&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.wbdg.org/ffc/dod/unified-facilities-criteria-ufc/ufc-3-535-01"&gt;UFC 3-535-01 &lt;em&gt;Visual Air Navigation Facilities&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.wbdg.org/ffc/dod/unified-facilities-criteria-ufc/ufc-4-141-10"&gt;UFC 4-141-10 &lt;em&gt;Airfield Operations Support Facilities&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt; 













&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://www.wbdg.org/building/aviation/fixed-wing-runway-design</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 19:41:14 +0000</pubDate>
    </item>
    <item>
      <title>Terrence Malick's Disciples</title>
      <link>https://yalereview.org/article/bilge-ebiri-terrence-malick</link>
      <description>In the winter of 2024, the photographer and filmmaker RaMell Ross released Nickel Boys , a masterful adaptation of a novel by Colson Whitehead. In a fragmentary, impressionistic style, the film portrays the friendship of two African American teens at a brutal Florida reform academy during the Jim Crow era. Acclaimed as a visionary movie, it ended up on many critics’ best-of-the-year lists and earned an Oscar nomination for Best Picture.</description>
      <content:encoded>&lt;main&gt;






&lt;a href="https://yalereview.org/reviews"&gt;                Film
            &lt;/a&gt; 
&lt;h1&gt;Terrence Malick’s Disciples&lt;/h1&gt;
&lt;h2&gt;Why the auteur is the most influential director in Hollywood&lt;/h2&gt;
&lt;a href="https://yalereview.org/author/bilge-ebiri"&gt;Bilge Ebiri&lt;/a&gt;
















&lt;img src="https://d181q449nqu6en.cloudfront.net/content/craft/articles/_850xAUTO_crop_center-center_none/Ebiri-Hamnet.jpg"/&gt;
Paul Mescal in &lt;em&gt;Hamnet&lt;/em&gt;, directed by Chloé Zhao, 2025. Photo by Bruno Engler, courtesy Paramount Pictures


&lt;p&gt;In the winter of 2024, the photographer and filmmaker RaMell Ross released &lt;em&gt;&lt;a href="https://www.mgm.com/movies/nickel-boys"&gt;Nickel Boys&lt;/a&gt;&lt;/em&gt;, a masterful adaptation of a novel by Colson Whitehead. In a fragmentary, impressionistic style, the film portrays the friendship of two African American teens at a brutal Florida reform academy during the Jim Crow era. Acclaimed as a visionary movie, it ended up on many critics’ best-of-the-year lists and earned an Oscar nomination for Best Picture.&lt;/p&gt;
&lt;p&gt;Ross is a fiercely independent artist. His first film, the lyrical 2018 documentary &lt;em&gt;&lt;a href="https://www.halecountyfilm.com/"&gt;Hale County This Morning, This Evening&lt;/a&gt;&lt;/em&gt;, was also nominated for an Oscar. Afterward, he refused Hollywood’s overtures for years. So why did he take a meeting with the producers who reached out to him about making a studio-financed, big-budget adaptation of &lt;em&gt;Nickel Boys&lt;/em&gt;? Ross’s explanation was simple: because one of them had produced Terrence Malick’s 2011 film, &lt;em&gt;The Tree of Life&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Ross’s reverence for Malick is plain in his films, which, like Malick’s, rely on extended montages of the everyday and do away with the conventional rules of cinematic storytelling, hovering instead between distant, melancholy reverie and hyperfocused, lived-in specificity. And he is not the only recent filmmaker who has fallen under Malick’s spell. Indeed, Malick’s sensibility, visual style, and working methods have had a profound influence on some of today’s best and most interesting directors.&lt;/p&gt;


&lt;p&gt;Take Chloé Zhao, the director of the Oscar-winning &lt;em&gt;&lt;a href="https://youtu.be/6sxCFZ8_d84?si=Spz64UoSqoF2ynD5"&gt;Nomadland&lt;/a&gt;&lt;/em&gt; (2020). Her early films, all set in the American heartland, were regularly compared to Malick’s, and she herself pointed to &lt;em&gt;The Tree of Life&lt;/em&gt; and Malick’s 2005 film, &lt;em&gt;The New World&lt;/em&gt;, as influences on her 2021 Marvel superhero movie, &lt;em&gt;&lt;a href="https://youtu.be/x_me3xsvDgk?si=nL9gBh2F6ZqK9OVH"&gt;Eternals&lt;/a&gt;&lt;/em&gt;. Those overtones persist in her latest, &lt;em&gt;&lt;a href="https://youtu.be/xYcgQMxQwmk?si=l1wBmajhpCH-lJUM"&gt;Hamnet&lt;/a&gt;&lt;/em&gt;, a film about the death of William Shakespeare’s only son and his subsequent creation of &lt;em&gt;Hamlet&lt;/em&gt;. The movie may take place in Elizabethan England, but it is replete with lyrical passages and visions of nature that recall Malick’s work. &lt;/p&gt;
&lt;p&gt;The same is true of the director Clint Bentley’s newest film, &lt;em&gt;&lt;a href="https://youtu.be/_Nk8TrBHOrA?si=mO_kVRP4GGmDBk4S"&gt;Train Dreams&lt;/a&gt;&lt;/em&gt;, an adaptation of Denis Johnson’s 2011 novella about the unremarkable life of a logger and railroad worker in the early years of the twentieth century. Weaving episodes from its character’s life into an elegiac collage that incorporates domestic bliss, harrowing tragedy, and melancholic resignation, &lt;em&gt;Train Dreams&lt;/em&gt;—which premiered at the Sundance Film Festival in January and was quickly acquired by Netflix—unfolds across 102 minutes, yet seems to contain a whole world. Its protagonist, played by a reserved Joel Edgerton, is a simple man who occasionally questions his place in the universe but never understands it, save for a brief moment near the end when he takes a ride in an airplane—something he’s never done before—and, in one shining (and recognizably Malickian) instant, sees the shape of his life and feels something like transcendence.&lt;/p&gt;


&lt;p&gt;Malick’s influence is intriguing in part because he is not an obvious choice for filmmakers to emulate. He has had, to be sure, a fascinating career: a publicity-shy Harvard philosophy grad, Rhodes Scholar, former MIT lecturer, and &lt;em&gt;New Yorker&lt;/em&gt; writer, he made two brilliant and highly acclaimed films in the 1970s—the lovers-on-the-run drama &lt;em&gt;&lt;a href="https://www.criterion.com/films/28406-badlands?"&gt;Badlands&lt;/a&gt;&lt;/em&gt; and the visually striking romantic tragedy &lt;em&gt;&lt;a href="https://www.criterion.com/films/213-days-of-heaven?srsltid=AfmBOoouFpCvAOG7BaRoQWdFniUoq-h4H-iEFs6titI0gYDjrfT5PDjD"&gt;Days of Heaven&lt;/a&gt;&lt;/em&gt;—before stepping away from filmmaking for twenty years. In 1998, he returned with &lt;em&gt;&lt;a href="https://youtu.be/mKl5_OxKBn8?si=9kaWNR1ul8Ku1va6"&gt;The Thin Red Line&lt;/a&gt;&lt;/em&gt;, a dreamy, diffuse adaptation of James Jones’s World War II novel, and followed that with two more ruminative epics: &lt;em&gt;&lt;a href="https://www.criterion.com/films/28713-the-new-world?"&gt;The New World&lt;/a&gt;&lt;/em&gt;, about the settlement of Jamestown and the romance between John Smith and Pocahontas, and &lt;em&gt;&lt;a href="https://youtu.be/RrAz1YLh8nY?si=BuVLnECxS9cvJFjb"&gt;The Tree of Life&lt;/a&gt;&lt;/em&gt;, a massive autobiographical film that frames a mid-century Texas coming-of-age tale against the spectacular origins of the universe and of life on Earth. His films since then have been less ambitious in scope but, in some ways, more stylistically bold.&lt;/p&gt;
&lt;p&gt;Many of Malick’s films have been critically acclaimed, and two have received Oscar nominations for Best Picture (albeit without much chance of winning). But none could be called box-office hits, and some have been savaged by critics. Indeed, thanks to his fondness for oblique storytelling, poetic voice-over, and overt spiritual themes, Malick’s oeuvre has become one of the more contentious in cinema. Each new release inspires debate over whether the film at hand is a deep, philosophical masterpiece or boring, pretentious drivel. Young directors looking for heroes tend not to gravitate toward divisive religious artists whose movies don’t make money or win awards. So what accounts for Malick’s impact on twenty-first-century American film?&lt;/p&gt;


&lt;img src="https://d181q449nqu6en.cloudfront.net/content/craft/articles/_850xAUTO_crop_center-center_none/Ebiri_Tree-of-Life.jpg"/&gt;
Brad Pitt in &lt;em&gt;The Tree of Life&lt;/em&gt;, directed by Terrence Malick, 2011. Courtesy Fox Searchlight


&lt;p&gt;particularly since his return to filmmaking, Malick has sought to reconnect American cinema to a lost spirituality, earnestly tackling questions about faith and the design of the world at a time when most mainstream cinema has avoided such topics. Malick is a devout Episcopalian. But the spirituality in his films is rarely illustrative or prescriptive. He doesn’t use religion as a cudgel or a doctrinaire superstructure with which to explain the world. Rather, he sees it as an inner light in people. In &lt;em&gt;The Thin Red Line&lt;/em&gt;, for instance, soldiers, in voice-over, speak solemnly of inner longing. These otherwise inarticulate men’s voices read heartfelt love letters, or dabble in poetry, or edge their way into philosophical inquiries about the cruelty and redemptiveness of nature. A soldier remembers his mother reaching for an angel at the instant of her death; another recalls the serenity he experienced with his wife before he had to leave her behind. The effect is like eavesdropping on a kind of Emersonian oversoul. Malick endows even his most minor characters with humanity, which he views as a kind of holiness. Amid the gaunt and haunted faces of these soldiers, Malick finds grace.&lt;/p&gt;
&lt;p&gt;This kind of earnestness stood out in an age of relentless irony and snark. It served as a corrective to the glossy productions of Hollywood in its imperial phase, that period of the late 1990s and early 2000s, when budgets ballooned and American cinema, armed with state-of-the-art CGI and desperate to service a growing international market, became increasingly driven by fantasy spectacle and special effects. Malick’s films were a rebuke to even the hip grittiness of independent films of the era. He had an eye for light and an ear for music, he immersed viewers in color and texture, and he used his classical scores to underscore the glory of what he saw. Handcrafted, personal, achingly sincere, and at times proudly “flawed,” his pictures stood out against both the mainstream and the underground.&lt;/p&gt;


&lt;p&gt;
    By the end, we are overwhelmed with emotion for this unremarkable life lived in near anonymity.
    &lt;/p&gt;


&lt;p&gt;This proved irresistible for a certain kind of filmmaker frustrated with the options available to them. In 2000, for instance, the director David Gordon Green released &lt;em&gt;&lt;a href="https://www.criterionchannel.com/george-washington"&gt;George Washington&lt;/a&gt;&lt;/em&gt;, a drifting, multicharacter drama featuring young African American kids in a dead-end North Carolina steel town. Despite his impoverished setting, Green avoids miserabilist clichés and gives his characters a romantic grandeur. He takes their hopes and desires at face value. The title comes from the fact that one of the kids, named George, dreams of being president of the United States, a fact that Green does not treat with bitter irony or fashionable cynicism.&lt;/p&gt;
&lt;p&gt;Malick’s effect on &lt;em&gt;George Washington&lt;/em&gt; is undeniable—rare was the review that didn’t mention the connection—and it is also clear in Green’s second feature, &lt;em&gt;All the Real Girls&lt;/em&gt; (2003), an atmospheric and largely uneventful romance defined by the passions of the two shy lovers at its center. Noel (Zooey Deschanel) and Paul (Paul Schneider), like Malick’s characters in &lt;em&gt;Badlands&lt;/em&gt; and &lt;em&gt;Days of Heaven&lt;/em&gt;, are not extroverted or articulate. But Green’s film thrums with a visual splendor that reflects the characters’ longing, turning another depressed Southern town into a vibrant emotional landscape.&lt;/p&gt;
&lt;p&gt;Zhao’s films also highlight the great beauty of the otherwise unremarkable. Her masterpiece, 2017’s &lt;em&gt;&lt;a href="https://youtu.be/AlrWRttLTkg?si=vC0muaGpfuuiMNit"&gt;The Rider&lt;/a&gt;&lt;/em&gt;, follows a wounded rodeo cowboy (played by Brady Jandreau, a real-life rodeo star who sustained a career-ending head injury) from a Lakota Sioux reservation in South Dakota as he struggles with his inability to ride again. The film is made up of small moments, highlighting brief interactions and quotidian actions, but Zhao’s shooting and cutting, much like Malick’s, elevate these scenes toward the transcendent, finding a sacredness in the existence of a character who has lost his sense of purpose.&lt;/p&gt;
&lt;p&gt;The same could be said of Bentley’s &lt;em&gt;Train Dreams&lt;/em&gt;, which follows a man with very little direction in the world: he’s an orphan, raised in poverty, who finds work as a logger and spends his years felling trees and building railroads. Though he sees racism and murder around him, he can do nothing about it. He finds happiness by starting a family but then loses that family to a raging wildfire. The film’s rhythms are not those of a typical drama; for all the squalls of guilt and grief, the movie moves with a steady cadence that suggests that the mysteries, tragedies, and glories of life are all part of the same thing. This seems like it would result in a cold, opaque film, yet by the end, we are overwhelmed with emotion for this unremarkable life lived in near anonymity, a life that is more like our own than we might want to admit.&lt;/p&gt;
&lt;p&gt;You can also see Malick’s philosophical influence in three films directed by Laura Dunn (all of which he produced): &lt;em&gt;&lt;a href="https://twobirdsfilm.com/films/theunforeseen"&gt;The Unforeseen&lt;/a&gt;&lt;/em&gt; (2007), about the dire social and environmental consequences of a mining company’s development of a vast patch of Austin real estate,&lt;em&gt; &lt;a href="https://www.lookandseefilm.com/"&gt;Look &amp;amp; See: A Portrait of Wendell Berry&lt;/a&gt;&lt;/em&gt; (2016), about the life of the titular Kentucky farmer, writer, and activist, and &lt;em&gt;&lt;a href="https://allillusionsmustbebroken.com/"&gt;All Illusions Must Be Broken&lt;/a&gt;&lt;/em&gt; (2024), about the American cultural anthropologist Ernest Becker’s ideas around the human denial of mortality and self-knowledge. In each, Dunn portrays a society that is fraying at the seams owing to its increasing disconnection from the natural world and the organic patterns of life. Her films avoid the density of political and philosophical jargon. Instead, they create meaning through images of ordinary people: children playing, adults working in the fields, reconnecting viewers with a different state of being. The films’ form embodies her overall thesis that, despite our endless efforts to deny it, we humans are not separate from nature but inextricably part of it.&lt;/p&gt;


&lt;p&gt;&lt;br/&gt;malick’s humanism is refracted through his visual style—the aspect of his films that’s most obviously influential. He loves to shoot with natural light whenever possible: “Vermeer yourself ” is a common direction he gives to actors, indicating that they should lean into the available light during a take. His fondness for shooting at the “magic hour,” that time when the sun is setting and the sky emits a distinctive dark glow, is legendary. He also talks about “quail hunting”: capturing unscripted moments when the light happens to be perfect and you find something unexpected and real. Then there are “rabbit holes”: quick scenes and exchanges shot when the light isn’t perfect. Natural metaphors, found moments, a dogged pursuit of real light—the way Malick approaches the act of shooting enacts his philosophical view of the world.&lt;/p&gt;


&lt;p&gt;
    The lilting, fairy-tale surfaces of that film speak to a search for beauty that the characters cannot find.
    &lt;/p&gt;


&lt;p&gt;Malick’s influence on the way movies look has become a cliché. (A short 2015 video titled “&lt;a href="https://vimeo.com/144447762"&gt;Not Directed by Terrence Malick&lt;/a&gt;,” compiled by Jacob T. Swinney, features a collection of clips of films apparently influenced by Malick; it includes movies like &lt;em&gt;Up in the Air&lt;/em&gt;, &lt;em&gt;Beasts of No Nation&lt;/em&gt;, and &lt;em&gt;Ex Machina&lt;/em&gt;.) But anybody with some skill can shoot with natural light or cut away to a field of wheat. What distinguishes Malick’s work—what makes it truly revelatory to viewers—emerges from the harmony between a film’s images and its sensibility. In &lt;em&gt;George Washington&lt;/em&gt;, Green frames his characters in gorgeous light and scores their interactions with symphonic drones that suggest something heroic. And in &lt;em&gt;Nickel Boys&lt;/em&gt;, Ross tells a tale filled with injustice, racism, torture, and murder—a story that should be the very height of despair—yet finds an almost overwhelming humanity with his probing camera. Like Malick in &lt;em&gt;The Thin Red Line&lt;/em&gt;, Ross sees evidence of grace in the basest of places.&lt;/p&gt;
&lt;p&gt;By contrast, it’s jarring—if fascinating—when a film’s visual approach borrows from Malick but doesn’t match the sensibility at work. That’s the case with &lt;em&gt;&lt;a href="https://watch.afi.com/movie/the-assassination-of-jesse-james-by-the-coward-robert-ford"&gt;The Assassination of Jesse James by the Coward Robert Ford&lt;/a&gt;&lt;/em&gt; (2007), a remarkably beautiful Western directed by Andrew Dominik, who worked as an uncredited cameraman on &lt;em&gt;The New World&lt;/em&gt;. The film has a twilight grandeur and a fascination with the natural world that suggests Dominik learned quite a bit working for Malick. But despite the unmistakable surface similarities, Dominik’s dark moral vision bears little resemblance to Malick’s. The outlaws of the James gang live in a universe of endless, savage scrutiny, fearful of both the law and their own viral, panopticist distrust, with each member set against the others. The lilting, fairy-tale surfaces of that film speak to a search for beauty that the characters cannot find; Dominik longs for Malick’s vision of grace but sees no evidence of it. Or maybe he just doesn’t really want to find it.&lt;/p&gt;


&lt;img src="https://d181q449nqu6en.cloudfront.net/content/craft/articles/_850xAUTO_crop_center-center_none/Ebiri-Nickel-Boys.jpg"/&gt;
Ethan Herisse (&lt;em&gt;left&lt;/em&gt;) and Brandon Wilson in &lt;em&gt;Nickel Boys&lt;/em&gt;, directed by RaMell Ross, 2024. Courtesy Orion Pictures / Plan B Entertainment / Album


&lt;p&gt;malick’s working style is also appealing to many filmmakers. He shoots incessantly, improvises constantly, pays more attention to capturing footage of flora and fauna than he does to scripted scenes with actors, and then spends months in postproduction with teams of editors assembling his movies in unorthodox ways. This approach is inviting not just because it is unusually creative and collaborative but because it is rooted in the nature of cinema itself. &lt;/p&gt;
&lt;p&gt;Malick does not rely on the nineteenth-century theatrical conventions that most moviemakers remain bound to, with their focus on acts and protagonists and inciting incidents and A and B storylines. His films also avoid the novelistic, flowing instead like a series of thoughts, or memories, or maybe rivers. His is an intuitive and almost abstract filmmaking process that deprioritizes the presentational and the narrative. Malick focuses on collecting images, ideas, offhand moments, and sounds that can then be used during editing, applied almost like brushstrokes in a painting.&lt;/p&gt;
&lt;p&gt;He also welcomes spontaneous suggestions on set and encourages experimentation. There are three editors credited on &lt;em&gt;The Thin Red Line&lt;/em&gt;, four on &lt;em&gt;The New World&lt;/em&gt;, and five on &lt;em&gt;The Tree of Life&lt;/em&gt;; for the latter, the director reportedly invited students from the University of Southern California and the University of Texas at Austin to come in and try their hand at cutting footage. “Kids don’t censor themselves—their brains are in a different place,” the editor Billy Weber, one of Malick’s longtime collaborators, told me at the time. “So we had students give it a shot. And we’d have interns come in at night and cut scenes.” This is, to be clear, nothing like the way most other movies are put together. All too often, a film production is like a train that can’t be stopped or set on a different course once it leaves the station. But Malick has found ways to guide the train gently off the tracks—and in new, unexpected, undiscovered directions. It’s easy to see why other directors might be drawn to his less regimented approach.&lt;/p&gt;


&lt;img src="https://d181q449nqu6en.cloudfront.net/content/craft/articles/_850xAUTO_crop_center-center_none/Ebiri-Days-of-Heaven.jpg"/&gt;
Richard Gere in &lt;em&gt;Days of Heaven&lt;/em&gt;, directed by Terrence Malick, 1978. Courtesy Universal Pictures


&lt;p&gt;This working process re-creates the filmmaking method that Malick chanced upon with &lt;em&gt;Days of Heaven&lt;/em&gt;, which, for all the acclaim it garnered, was something of a salvage job. Coming off the critical success of &lt;em&gt;Badlands&lt;/em&gt;, Malick had gone into &lt;em&gt;Days of Heaven&lt;/em&gt; with a dense, detailed, ambitious script. But as described in John Bleasdale’s excellent 2024 biography, &lt;em&gt;&lt;a href="https://bookshop.org/a/16648/9781985901193"&gt;The Magic Hours: The Films and Hidden Life of Terrence Malick&lt;/a&gt;&lt;/em&gt;, the director found himself unhappy with the results he was getting over the course of production. He didn’t like how his dialogue sounded. His scenes felt phony. The shoot ran wildly over budget and behind schedule, as the Canada-based production team spent days trying to get the light perfect, and Malick’s original cinematographer, Néstor Almendros (who would go on to win an Oscar for the film), left halfway through and was replaced by Haskell Wexler. Meanwhile, Malick sent a photographer friend to capture nature footage that he could intersperse throughout the movie.&lt;/p&gt;
&lt;p&gt;Along the way, the director found himself fascinated with the off-the-cuff observations made by another one of his leads, fifteen-year-old Linda Manz, and recorded her describing scenes from the movie in her own words; he eventually shaped that into one of the most indelible voice-over narrations in cinema history, an offbeat series of childlike reflections that provide a poetic counterpoint to the elemental storyline. Everything about Malick’s evolving approach speaks to a heightened sense of possibility, and to a desire to reinvigorate the frustrating rhythm of film production with openness, spontaneity, and discovery.&lt;/p&gt;
&lt;p&gt;What’s remarkable about this approach is that despite his seemingly scattershot and impulsive methods, Malick’s films possess an aesthetic unity. Ross suggests something similar when talking about his own work. In an interview around the release of &lt;em&gt;Nickel Boys&lt;/em&gt;, he described to me the collage-like quality of his film: “It’s jumping time, and jumping textures, and jumping images, and points of view, and focal lengths, and sounds, but also it’s coherent.”&lt;/p&gt;


&lt;p&gt;&lt;br/&gt;influence can be a straitjacket. In 2014, A. J. Edwards, who had worked as an editor on two of Malick’s films, released &lt;em&gt;&lt;a href="https://youtu.be/NYYqvNW_c5M?si=Oac116TeEB97Q9NI"&gt;The Better Angels&lt;/a&gt;&lt;/em&gt;, a black-and-white meditation on Abraham Lincoln’s years as a young man living in rural Indiana. It’s a bold movie in many ways, almost confrontationally nonnarrative and context-free; aside from a brief coda set immediately after his assassination, we see almost nothing of Lincoln as a grown man or president. And yet there’s a curious emptiness at its heart. Filled with handheld reveries bathed in heavenly light, it replicates the yearning style of Malick’s work without the instances of genuine humanity that undergird his cinematic tapestries; though the characters in &lt;em&gt;The Better Angels&lt;/em&gt; are based on real historical personages, they never come across as real people.&lt;/p&gt;
&lt;p&gt;A similar emptiness afflicts David Lowery’s crime melodrama &lt;em&gt;&lt;a href="https://youtu.be/ga0c0v-stK0?si=4UxmyRoV840h_C_a"&gt;Ain’t Them Bodies Saints&lt;/a&gt;&lt;/em&gt; (2013), which has ravishing cinematography, ethereal music, and an elliptical narrative, all of which clearly owe something to Malick’s work. It follows the return of a fugitive to the woman he once loved and the child he has never met, and the tortured romance that ensues. Lowery explained at the time that he was not interested in another story about a crime but instead wanted to explore its emotional aftermath. But for all its loveliness, the film’s glancing storytelling has the opposite effect of Malick’s openness to the world; it dulls the senses, makes the characters and their feelings seem smaller and less significant. (Lowery’s more recent films owe little to Malick and are the better for it.)&lt;/p&gt;
&lt;p&gt;Think of it this way: What use is Malick’s liberated style of working if a filmmaker merely replicates it? The most successful Malickian films borrow from his work but find ways to transcend it and to convey new ideas. Take RaMell Ross. In his first film, he used fleeting, beautiful glimpses into mundane moments to convey, in just seventy-six short minutes, the arc of his subjects’ lives. In &lt;em&gt;Nickel Boys&lt;/em&gt;, he expanded the fragmented lyricism of his earlier film by crossing it with a first-person camera: the story is told almost entirely through shots that appropriate the perspectives of the two characters. The result is a work that is immersive and experiential, otherworldly and mythic. It’s also entirely his.&lt;/p&gt;


&lt;p&gt;&lt;br/&gt;malick has always been frustrated with the typical methods of making movies. In fact, he seems to become restless even with his own methods. If he’s helped liberate other filmmakers, he has also continuously sought to liberate himself. That may be why what has remained constant throughout his work has been &lt;em&gt;change&lt;/em&gt;. Even if certain aspects of his films—his love of natural light, his attention to found moments, his use of voice-over—have recurred, his subject matter, and his style, have never been fixed. The films that made his reputation in the 1970s—&lt;em&gt;Badlands&lt;/em&gt; and &lt;em&gt;Days of Heaven&lt;/em&gt;—are very different from the epics he made after his return to filmmaking, which are even less conventional in terms of narrative. The earlier pictures, compact and diamond-sharp, dance around their ideas, and their young protagonists don’t always grasp the gravity of their stories. Holly, the narrator of &lt;em&gt;Badlands&lt;/em&gt;, is just as likely to talk about a movie star or a photograph as she is to talk about the fact that her boyfriend is a serial killer; Linda, the narrator of &lt;em&gt;Days of Heaven&lt;/em&gt;, generally talks about everything and anything aside from the fact that her brother and his lover are cruelly betraying a dying man. This is very different from the prayerlike directness we find in the voice-overs for &lt;em&gt;The Thin Red Line&lt;/em&gt;, &lt;em&gt;The New World&lt;/em&gt;, and &lt;em&gt;The Tree of Life&lt;/em&gt;. Many-voiced and at times even rambling, those later period films are pointedly diffuse: each scene, each thought feels like it could expand into a whole other movie; all the characters seem so resolutely alive.&lt;/p&gt;
&lt;p&gt;Malick then pivoted again, following his trio of epics with a trilogy of low-budget works—&lt;em&gt;&lt;a href="https://youtu.be/52M41FF70Kc?si=qHFingfAW9JSZKmr"&gt;To the Wonder&lt;/a&gt;&lt;/em&gt; (2012), &lt;em&gt;&lt;a href="https://youtu.be/SI2j1FHCjtM?si=CrGqIqEbYZweTcoV"&gt;Knight of Cups&lt;/a&gt;&lt;/em&gt; (2015), and &lt;em&gt;&lt;a href="https://youtu.be/O4SrVkj84zc?si=zfAi4iLk5iXSt2YM"&gt;Song to Song&lt;/a&gt;&lt;/em&gt; (2017)—which seemed at times to be not-so-veiled dramatizations of events from the director’s own life. These works were Malick’s first films to be set in something like the present. They are messier, more frenzied. The characters in these later pictures are rootless, always searching. And the filmmaking in them is centered more on movement than meditations on nature.&lt;/p&gt;


&lt;p&gt;
    In his vision, our endless seeking makes us human and therefore holy.
    &lt;/p&gt;


&lt;p&gt;&lt;em&gt;To the Wonder&lt;/em&gt;, for instance, tells the story of the breakdown of the marriage between an American man and a Ukrainian woman after they return from Paris (where they met) to his home in Oklahoma. The film eschews dialogue, relying instead on characters’ movements to express their emotions and changing relationships. A mother and daughter, newly arrived in the United States, twirl and leap through the aisles of an enormous supermarket, the likes of which they’ve never seen before; the stolid shoulders of a frustrated husband dominate the foreground of the frame, while his effervescent wife moves daintily before him; a flirtation is expressed with a quick curtsy, shame with a penitent bow. It can almost be seen as a dance film. &lt;/p&gt;
&lt;p&gt;Instructing the film’s team of editors, Malick gave them copies of Margaret Anne Doody’s introduction to a Penguin Classics edition of Samuel Richardson’s 1740 epistolary novel, &lt;em&gt;Pamela&lt;/em&gt;, and pointed them to a line about how the author loved “the formless, the radiant zigzag becoming.” The phrase “radiant zigzag becoming” became their own unofficial title for the film, the editors told me; it spoke to the project’s energetic sense of movement. It also reflected the fact that Malick’s characters were always in the process of self-actualizing without ever fully doing so.&lt;/p&gt;


&lt;p&gt;Something similar could be said for Malick’s films themselves. &lt;em&gt;To the Wonder&lt;/em&gt;, in fact, led directly to one of the most intriguing of Malick-influenced movies, a hybrid on multiple levels. In 2018, the veteran photographer and documentarian Eugene Richards premiered a mesmerizing forty-three-minute film called &lt;em&gt;&lt;a href="https://eugenerichards.com/thykingdomcome"&gt;Thy Kingdom Come&lt;/a&gt;&lt;/em&gt;, which consists of footage Richards shot for &lt;em&gt;To the Wonder&lt;/em&gt;, featuring Javier Bardem as a priest who has lost his faith ministering to the impoverished residents of an Oklahoma town. &lt;/p&gt;
&lt;p&gt;Malick had Bardem go into real people’s lives—into trailer-park homes, a county jail, a homeless shelter—and had Richards document those people speaking to the actor’s clearly fictional priest. Only a small portion of the footage would be used in the finished feature, so Richards and Bardem developed a plan to make a separate film out of the material. In &lt;em&gt;Thy Kingdom Come&lt;/em&gt;, Bardem says little; most of the picture consists of these people—drug addicts, inmates, a homeless couple, a former Ku Klux Klan member, a woman grieving a dead baby, and more—describing their experiences and their thoughts. There is no narrative, nor even much of an emotional through line. Aside from a couple of brief exchanges, there is barely any mention of God. And yet spirituality is ever present. These people know the priest isn’t real, but they open up to him as if he were; they do not, in any way, seem to be acting. &lt;/p&gt;
&lt;p&gt;“Is this a true story?” Bardem asks in the opening narration. “Yes, I would say so. Is the priest a real priest? No. But it’s as if they were waiting for him.” The onrush of faces and lives that then ensues suggests the anticipation goes both ways: it’s as if the film were waiting for them. These people might not have found grace, but the camera eye—Richards’s but also Malick’s—finds grace in them. Thus, this riff on Malick reveals something essential about Malick’s work. In his vision, our endless seeking makes us human and therefore holy. The search for God is not a search for meaning; the meaning lies in the search itself. Through the films he’s made and the ways he’s made them, Malick has turned cinema into the vessel for that search.&lt;/p&gt;



&lt;a href="https://yalereview.org/author/bilge-ebiri"&gt;Bilge Ebiri&lt;/a&gt; is a film critic for &lt;em&gt;Vulture&lt;/em&gt; and &lt;em&gt;New York &lt;/em&gt;magazine. His work has also appeared in &lt;em&gt;The New York Times&lt;/em&gt; and the Criterion Collection.
                            


&lt;h2&gt;A Literary Gift in Print&lt;/h2&gt;
Give a year of &lt;em&gt;The Yale Review&lt;/em&gt;—four beautifully printed issues featuring new literature and ideas.
&lt;a href="https://shop.yalereview.org/products/the-yale-review-print-subscription"&gt;Give a Subscription&lt;/a&gt;





TAGS
&lt;a href="https://yalereview.org/search/?tag=32409"&gt;Film&lt;/a&gt;
&lt;a href="https://yalereview.org/search/?tag=16771"&gt;Arts &amp;amp; Culture&lt;/a&gt;
&lt;a href="https://yalereview.org/issues/winter-2025"&gt;Winter 2025&lt;/a&gt; 

Originally published:
December 15, 2025
&lt;a href="https://yalereview.org/issues/winter-2025"&gt;See this issue&lt;/a&gt; 

&lt;h2&gt;
            Featured
    &lt;/h2&gt;


&lt;a href="https://yalereview.org/reviews"&gt;            Books
        &lt;/a&gt; 
&lt;h3&gt;
&lt;a href="https://yalereview.org/article/elisa-gonzalez-searching-for-seamus-heaney"&gt;Searching for Seamus Heaney&lt;/a&gt;
&lt;/h3&gt;
What I found when I resolved to read him

Elisa Gonzalez





&lt;a href="https://yalereview.org/essays"&gt;            Essays
        &lt;/a&gt; 
&lt;h3&gt;
&lt;a href="https://yalereview.org/article/dan-fox-learning-welsh"&gt;What Happened When I Began to Speak Welsh&lt;/a&gt;
&lt;/h3&gt;
By learning my family's language, I hoped to join their conversation.

Dan Fox





&lt;a href="https://yalereview.org/essays"&gt;            Essays
        &lt;/a&gt; 
&lt;h3&gt;
&lt;a href="https://yalereview.org/article/anahid-nersessian-divorce"&gt;When Does a Divorce Begin?&lt;/a&gt;
&lt;/h3&gt;
Most people think of it as failure. For me it was an achievement.

Anahid Nersessian





&lt;h2&gt;
            You Might Also Like
    &lt;/h2&gt;


                    Film
            
&lt;h3&gt;
&lt;a href="https://yalereview.org/article/film-review-cinematic-afterlife-james-baldwin"&gt;The Cinematic Afterlife of James Baldwin&lt;/a&gt;
&lt;/h3&gt;
Film in review

Ayten Tartici





&lt;a href="https://yalereview.org/reviews"&gt;            Film
        &lt;/a&gt; 
&lt;h3&gt;
&lt;a href="https://yalereview.org/article/christopher-hawthorne-america-the-brutal"&gt;America the Brutal&lt;/a&gt;
&lt;/h3&gt;
Why &lt;em&gt;The Brutalist&lt;/em&gt; isn’t really about architecture

Christopher Hawthorne





&lt;a href="https://yalereview.org/essay-of-the-week"&gt;            Essay of the Week
        &lt;/a&gt; 
&lt;h3&gt;
&lt;a href="https://yalereview.org/article/wyman-what-after-the-hunt-gets-right"&gt;What “After the Hunt” Gets Right&lt;/a&gt;
&lt;/h3&gt;
I left academia. Luca Guadagnino’s new film reminds me why.

Annie Julia Wyman







&lt;h2&gt;A Literary Gift in Print&lt;/h2&gt;
Give a year of &lt;em&gt;The Yale Review&lt;/em&gt;—four beautifully printed issues featuring new literature and ideas.
&lt;a href="https://shop.yalereview.org/products/the-yale-review-print-subscription"&gt;Give a Subscription&lt;/a&gt;





&lt;/main&gt;</content:encoded>
      <guid isPermaLink="false">https://yalereview.org/article/bilge-ebiri-terrence-malick</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 19:35:20 +0000</pubDate>
    </item>
    <item>
      <title>Un-Redactor</title>
      <link>https://github.com/kvthweatt/unredactor</link>
      <description>A PDF editing tool that lets you put your own information over a redaction box.</description>
      <content:encoded>&lt;article class="markdown-body entry-content container-lg" itemprop="text"&gt;&lt;h1&gt;Un-Redactor&lt;/h1&gt;&lt;a href="#un-redactor"&gt;&lt;/a&gt;
&lt;p&gt;A PDF editing tool that lets you put your own information over a redaction box.&lt;/p&gt;
&lt;p&gt;This tool is for forensics purposes. It does not "recover" data truly destroyed by redaction tools.&lt;/p&gt;
&lt;p&gt;What it &lt;strong&gt;does&lt;/strong&gt; do is allow you to &lt;strong&gt;write over&lt;/strong&gt; a redaction box. Like white-out.&lt;/p&gt;
&lt;p&gt;You can select a redaction box, and select all of the exact dimensions and replace at once.&lt;/p&gt;
&lt;p&gt;I am not responsible for your use of this tool.&lt;/p&gt;
&lt;p&gt;Republishing altered documents is illegal, and you should not use this to do so.&lt;/p&gt;
&lt;p&gt;By using this tool you claim all legal liability for any documents you create with it.&lt;/p&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://github.com/kvthweatt/unredactor</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 19:26:36 +0000</pubDate>
    </item>
    <item>
      <title>Help My c64 caught on fire</title>
      <link>https://c0de517e.com/026_c64fire.htm</link>
      <description>2025-12-22, Monday, December (updated: 2025-12-23, Tuesday, December) [Home]</description>
      <content:encoded>&lt;body&gt;
&lt;h1&gt;Help! My c64 caught on fire!A n00b weekend holiday project.&lt;/h1&gt;


&lt;a href="https://c0de517e.com/index.htm"&gt;&lt;img alt="logo: back home" src="https://c0de517e.com/logo.png"/&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/intent/tweet?text=@kenpex%20http://c0de517e.com/./026_c64fire.htm"&gt;&lt;img alt="tweet" src="https://c0de517e.com/tweet.png"/&gt;&lt;/a&gt;
&lt;a href="https://s2f.kytta.dev/?text=@c0de517e@mastodon.gamedev.place%20http://c0de517e.com/./026_c64fire.htm"&gt;&lt;img alt="toot" src="https://c0de517e.com/toot.png"/&gt;&lt;/a&gt;

&lt;!-- BEGIN txt2web generated body --&gt;
I flew back to Italy for the Christmas holidays, as I usually do. Here I have my childhood c64, on which I learend how to program, and which in the last few years I took to refurbishing.&lt;br/&gt;
In general, everytime I'm back to my parent's place I spend some time fixing and sorting out things, and this is one of them.&lt;br/&gt;
&lt;br/&gt;
Now it works like a charm, and of course I also added some &lt;a href="https://c64os.com/buyersguide/"&gt;bells and whistles&lt;/a&gt;, mostly stuff that allows me to easily tranfer programs from a PC (a kung-fu cart and a pi1541) - so this year I thought it was time to actually do something with it! &lt;br/&gt;
&lt;br/&gt;
I decided to turn it into a cozy fireplace:&lt;br/&gt;
&lt;a href="https://c0de517e.com/026_c64fire/cozy.jpg"&gt;&lt;img alt="(click on the image for the full-color version)" src="https://c0de517e.com/026_c64fire/THUMBS/cozy.png"/&gt;&lt;/a&gt;(click on the image for the full-color version)&lt;br/&gt;
I was quite surprised that it worked, that I managed to complete this project in a few hours over two days, and that it was, most of all, fun! &lt;br/&gt;
&lt;br/&gt;
I expected... more friction, having to work with arcane cross-platform toolchains and the like, but instead I completed almost of all it in a web-based IDE/emulator combo!&lt;br/&gt;
&lt;br/&gt;
Moreover, I am far from being an expert when it comes to c64 coding. Yes, I used to program with one... when I was six or seven! And yes, I do follow its demoscene, and over the years I've read quite a bit about its chips and inner workings, but I've never written any demo effect for it. &lt;br/&gt;
&lt;br/&gt;
In other words... if I managed, you can too! That's what made me want to write this post...&lt;br/&gt;
&lt;br/&gt;
All you need for Christmas...&lt;br/&gt;
...is a 6502.&lt;br/&gt;
&lt;br/&gt;
Here, I'll give you a crash course on the c64 - the key points of what I knew before starting this.&lt;br/&gt;
&lt;br/&gt;
If you know how modern CPUs work, and how to optimize for them - try to forget all of that. The 64 comes from an era where RAM was faster than compute. Lookup tables are your friend, as it is fully unrolling loops / code generation. We have no caches!&lt;br/&gt;
&lt;br/&gt;
Don't be surprised then to learn that the famous 6502 CPU has only one arithmetic register, the "accumulator". There are also two "index" registers used to offset memory locations, a status register, and a program counter - but you can't do math on any of these, at best, test or increment/decrement.&lt;br/&gt;
Moreover, the program counter is the only 16-bit register, all the others are 8-bit.&lt;br/&gt;
&lt;br/&gt;
But who needs registers when you have fast RAM? Memory is your register file: pretty much all 6502 instructions operate between the accumulator and memory locations (or numeric constants, that are still memory, just part of the instruction itself).&lt;br/&gt;
&lt;br/&gt;
To further facilitate this, the 6502 comes with a rich set of addressing modes, most instructions can fetch memory in few different ways: at absolute addresses, at addresses offset with one of the two index registers or even at indirect addresses (addresses contained in memory locations).&lt;br/&gt;
There is also a special memory area, called the "zero page", the first 256 bytes of memory (a page is, unsurprisingly, 256 bytes), which has an extra optimization: addressing there takes one cycle less, because the address can be encoded in the instruction in a single byte instead of two.&lt;br/&gt;
&lt;br/&gt;
Have a look at the &lt;a href="https://www.masswerk.at/6502/6502_instruction_set.html"&gt;6502 instruction set&lt;/a&gt;, it's very simple! Won't take more than 15 minutes to skim over them all.&lt;br/&gt;
&lt;br/&gt;
The plan.&lt;br/&gt;
&lt;br/&gt;
Where things get less simple is to deal with all the c64 custom chips, the SID (sound) and the VIC-II (graphics). That's how demo-scene effects are done! Manipulating these chips in very precise ways to cause them to generate crazy stuff, most of which was never considered possible by the c64 designers back then!&lt;br/&gt;
&lt;br/&gt;
The average c64 demo effect is all about this - generating lookup tables and unrolled assembly to then be able to exactly time internal chip status changes as the video signal is being generated line by line ("racing the beam"). Usually, what is shown on screen is not at all what it seems - i.e. it's not how you would create a similar effect on a PC.&lt;br/&gt;
&lt;br/&gt;
&lt;a href="https://c0de517e.com/026_c64fire/demo.jpg"&gt;&lt;img alt="We won't be porting Second Reality..." src="https://c0de517e.com/026_c64fire/THUMBS/demo.png"/&gt;&lt;/a&gt;We won't be porting Second Reality...&lt;br/&gt;
I know almost nothing of any of this - so my plan was to avoid it all! I wanted to set the VIC-II in some graphic mode that gave me a decently simple, linear framebuffer, and from there on write the code like I would have done on any other computer, hoping that a fire effect is simple enough to compute that the 64 would just be able to cope with it.&lt;br/&gt;
&lt;br/&gt;
Luckily, there is one such mode. The default, vanilla, character mode! Here, we have 40x25 characters on screen, chosen from a set of 256. So, one byte per "pixel", and 1000 pixels in total - great!&lt;br/&gt;
&lt;br/&gt;
&lt;a href="https://c0de517e.com/026_c64fire/petscii.png"&gt;&lt;img alt='The default "petscii" character set.' src="https://c0de517e.com/026_c64fire/THUMBS/petscii.png"/&gt;&lt;/a&gt;The default "petscii" character set.&lt;br/&gt;
Now, the default character set does not really lean itself to creating a demo effect, but I knew I could create a custom one. My idea was to simply making a dither pattern, and as much as possible work like I had a 8-bit "grayscale" screen. &lt;br/&gt;
&lt;br/&gt;
C64 characters are 8x8, so I could create a dither pattern that has a number of "on" pixels in the character corresponding to its position in the charset: 0 being fully off (background color), 64 being fully on (foreground color), and everything in between being a mix.&lt;br/&gt;
Of course though this would give us only 64 values, ideally, I wanted to utilize the whole 8-bit space... On obvious idea is that we could add colors to the mix. For example, having the first 64 values go from black (background) to brown (foreground), then the next 64 have brown as background and red as foreground, then red and yellow, and finally yellow and white.&lt;br/&gt;
&lt;br/&gt;
&lt;a href="https://c0de517e.com/026_c64fire/chars.png"&gt;&lt;img alt="My custom charset. Made by hand in C64Studio." src="https://c0de517e.com/026_c64fire/THUMBS/chars.png"/&gt;&lt;/a&gt;My custom charset. Made by hand in C64Studio.&lt;br/&gt;
This can't be achieved in the default character mode, unfortunately, as only the foreground color is controllable per-character (via another memory location, still using one byte per character - easy), while the background is shared. Luckily though, there is an &lt;a href="https://www.c64-wiki.com/wiki/Extended_color_mode"&gt;"extended color mode"&lt;/a&gt; that fits the bill exactly. In this mode the character set is limited to 64, but the two high bits of each character can be used to control the background color, while the foreground remains in the separate memory location as usual.&lt;br/&gt;
&lt;br/&gt;
Implementation.&lt;br/&gt;
&lt;br/&gt;
All development was done in the &lt;a href="https://ide.retrogamecoders.com/"&gt;retrogamecoders c64 IDE&lt;/a&gt;, which handly couples the &lt;a href="https://cc65.github.io/"&gt;cc65 compiler&lt;/a&gt; with an emulator.&lt;br/&gt;
&lt;br/&gt;
Writing c64 code in C is generally a terrible idea, and cc65 is not even the "best" compiler out there (is quite old and barely does optimize the generated code - &lt;a href="https://llvm-mos.org/wiki/Welcome"&gt;llvm-mos&lt;/a&gt; might be better), but being able to test in C and then gradually "port" to assembly was crucial for a noob like me. If I were to keep working on this, I'd probably move to &lt;a href="https://theweb.dk/KickAssembler/Main.html#frontpage"&gt;Kick Assembler&lt;/a&gt;, which is particularly suited for the kind of code-generation that you want to do in demo-coding.&lt;br/&gt;
&lt;br/&gt;
Anyhow, here are the .c files, step by step, in chronological order. You can just copy and paste them in the retrogamecoders IDE and see how things work, start tinkering if you like! Enjoy!&lt;br/&gt;
&lt;br/&gt;
&lt;a href="https://c0de517e.com/026_c64fire/final.png"&gt;&lt;img src="https://c0de517e.com/026_c64fire/THUMBS/final.png"/&gt;&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://c0de517e.com/026_c64fire/test.c"&gt;First test of the ECM charset idea.&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://c0de517e.com/026_c64fire/slow.c"&gt;Slowest fire-effect ever in pure C.&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://c0de517e.com/026_c64fire/fire_0.c"&gt;Still in C. Starting to "unroll".&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://c0de517e.com/026_c64fire/fire_1.c"&gt;Starting to port to assembly.&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://c0de517e.com/026_c64fire/fire_2.c"&gt;Ported to assembly, with side-by-side C.&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://c0de517e.com/026_c64fire/fire_3.c"&gt;Assembly only, some more ECM tricks.&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://c0de517e.com/026_c64fire/fire_4.c"&gt;"Final" version.&lt;/a&gt;&lt;br/&gt;
&lt;!-- END txt2web generated body --&gt;

&lt;p&gt;2025-12-22, Monday, December (updated: 2025-12-23, Tuesday, December) &lt;a href="https://c0de517e.com/index.htm"&gt;[Home]&lt;/a&gt;&lt;/p&gt;
&lt;/body&gt;</content:encoded>
      <guid isPermaLink="false">https://c0de517e.com/026_c64fire.htm</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 19:09:45 +0000</pubDate>
    </item>
    <item>
      <title>LAVD: Meta's New Default Scheduler [pdf]</title>
      <link>https://lpc.events/event/19/contributions/2099/attachments/1875/4020/lpc-2025-lavd-meta.pdf</link>
      <description>&lt;a href="https://news.ycombinator.com/item?id=46368235"&gt;Comments&lt;/a&gt;</description>
      <guid isPermaLink="false">https://lpc.events/event/19/contributions/2099/attachments/1875/4020/lpc-2025-lavd-meta.pdf</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 19:04:13 +0000</pubDate>
    </item>
    <item>
      <title>We replaced H.264 streaming with JPEG screenshots (and it worked better)</title>
      <link>https://blog.helix.ml/p/we-mass-deployed-15-year-old-screen</link>
      <description>Part 2 of our video streaming saga. Read Part 1: How we replaced WebRTC with WebSockets →</description>
      <content:encoded>&lt;article class="typography newsletter-post post"&gt;&lt;h1&gt;We Mass-Deployed 15-Year-Old Screen Sharing Technology and It's Actually Better&lt;/h1&gt;&lt;h3&gt;Or: How JPEG Screenshots Defeated Our Beautiful H.264 WebCodecs Pipeline&lt;/h3&gt;&lt;a href="https://substack.com/@lewq"&gt;&lt;img alt="Luke Marsden's avatar" src="https://substackcdn.com/image/fetch/$s_!fz2p!,w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe83a5474-4943-4f2d-ae84-070ba6dd2042_400x400.jpeg"/&gt;&lt;/a&gt;&lt;a href="https://substack.com/@lewq"&gt;Luke Marsden&lt;/a&gt;Dec 18, 2025311Share&lt;p&gt;&lt;em&gt;Part 2 of our video streaming saga. &lt;a href="https://blog.helix.ml/p/we-killed-webrtc-and-nobody-noticed"&gt;Read Part 1: How we replaced WebRTC with WebSockets →&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;The Year is 2025 and We’re Sending JPEGs&lt;/h2&gt;&lt;p&gt;Let me tell you about the time we spent three months building a gorgeous, hardware-accelerated, WebCodecs-powered, 60fps H.264 streaming pipeline over WebSockets...&lt;/p&gt;&lt;p&gt;Thanks for reading HelixML! Subscribe for free to receive new posts and support my work.&lt;/p&gt;&lt;p&gt;...and then replaced it with &lt;code&gt;grim | curl&lt;/code&gt; when the WiFi got a bit sketchy.&lt;/p&gt;&lt;p&gt;I wish I was joking.&lt;/p&gt;&lt;h2&gt;Act I: Hubris (Also Known As “Enterprise Networking Exists”)&lt;/h2&gt;&lt;p&gt;We’re building &lt;a href="https://github.com/helixml/helix"&gt;Helix&lt;/a&gt;, an AI platform where autonomous coding agents work in cloud sandboxes. Users need to watch their AI assistants work. Think “screen share, but the thing being shared is a robot writing code.”&lt;/p&gt;&lt;p&gt;Last week, we explained how we replaced WebRTC with a custom WebSocket streaming pipeline. This week: why that wasn’t enough.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The constraint that ruined everything:&lt;/strong&gt; It has to work on enterprise networks.&lt;/p&gt;&lt;p&gt;You know what enterprise networks love? HTTP. HTTPS. Port 443. That’s it. That’s the list.&lt;/p&gt;&lt;p&gt;You know what enterprise networks hate?&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;UDP&lt;/strong&gt; — Blocked. Deprioritized. Dropped. “Security risk.”&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;WebRTC&lt;/strong&gt; — Requires TURN servers, which requires UDP, which is blocked&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Custom ports&lt;/strong&gt; — Firewall says no&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;STUN/ICE&lt;/strong&gt; — NAT traversal? In &lt;em&gt;my&lt;/em&gt; corporate network? Absolutely not&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Literally anything fun&lt;/strong&gt; — Denied by policy&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We tried WebRTC first. Worked great in dev. Worked great in our cloud. Deployed to an enterprise customer.&lt;/p&gt;&lt;p&gt;“The video doesn’t connect.”&lt;/p&gt;&lt;p&gt;&lt;em&gt;checks network&lt;/em&gt; — Outbound UDP blocked. TURN server unreachable. ICE negotiation failing.&lt;/p&gt;&lt;p&gt;We could fight this. Set up TURN servers. Configure enterprise proxies. Work with IT departments.&lt;/p&gt;&lt;p&gt;Or we could accept reality: &lt;strong&gt;Everything must go through HTTPS on port 443.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;So we built a &lt;strong&gt;pure WebSocket video pipeline&lt;/strong&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;H.264 encoding via GStreamer + VA-API (hardware acceleration, baby)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Binary frames over WebSocket (L7 only, works through any proxy)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;WebCodecs API for hardware decoding in the browser&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;60fps at 40Mbps with sub-100ms latency&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We were so proud. We wrote Rust. We wrote TypeScript. We implemented our own binary protocol. We measured things in microseconds.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Then someone tried to use it from a coffee shop.&lt;/strong&gt;&lt;/p&gt;&lt;h2&gt;Act II: Denial&lt;/h2&gt;&lt;p&gt;“The video is frozen.”&lt;/p&gt;&lt;p&gt;“Your WiFi is bad.”&lt;/p&gt;&lt;p&gt;“No, the video is definitely frozen. And now my keyboard isn’t working.”&lt;/p&gt;&lt;p&gt;&lt;em&gt;checks the video&lt;/em&gt;&lt;/p&gt;&lt;p&gt;It’s showing what the AI was doing 30 seconds ago. And the delay is growing.&lt;/p&gt;&lt;p&gt;Turns out, 40Mbps video streams don’t appreciate 200ms+ network latency. Who knew.&lt;/p&gt;&lt;p&gt;When the network gets congested:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Frames buffer up in the TCP/WebSocket layer&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;They arrive in-order (thanks TCP!) but increasingly delayed&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Video falls further and further behind real-time&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;You’re watching the AI type code from 45 seconds ago&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;By the time you see a bug, the AI has already committed it to main&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Everything is terrible forever&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;“Just lower the bitrate,” you say. Great idea. Now it’s 10Mbps of blocky garbage that’s &lt;em&gt;still&lt;/em&gt; 30 seconds behind.&lt;/p&gt;&lt;h2&gt;Act III: Bargaining&lt;/h2&gt;&lt;p&gt;We tried everything:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;“What if we only send keyframes?”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This was our big brain moment. H.264 keyframes (IDR frames) are self-contained. No dependencies on previous frames. Just drop all the P-frames on the server side, send only keyframes, get ~1fps of corruption-free video. Perfect for low-bandwidth fallback!&lt;/p&gt;&lt;p&gt;We added a &lt;code&gt;keyframes_only&lt;/code&gt; flag. We modified the video decoder to check &lt;code&gt;FrameType::Idr&lt;/code&gt;. We set GOP to 60 (one keyframe per second at 60fps). We tested.&lt;/p&gt;&lt;p&gt;We got exactly ONE frame.&lt;/p&gt;&lt;p&gt;One single, beautiful, 1080p IDR frame. Then silence. Forever.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;code&gt;[WebSocket] Keyframe received (frame 121), sending
[WebSocket] ...
[WebSocket] ...
[WebSocket] It's been 14 seconds why is nothing else coming
[WebSocket] Failed to send audio frame: Closed&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;checks Wolf logs&lt;/em&gt; — encoder still running&lt;/p&gt;&lt;p&gt;&lt;em&gt;checks GStreamer pipeline&lt;/em&gt; — frames being produced&lt;/p&gt;&lt;p&gt;&lt;em&gt;checks Moonlight protocol layer&lt;/em&gt; — &lt;strong&gt;nothing coming through&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We’re using &lt;a href="https://games-on-whales.github.io/wolf/stable/"&gt;Wolf&lt;/a&gt;, an excellent open-source game streaming server (seriously, the documentation is great). But our WebSocket streaming layer sits on top of the Moonlight protocol, which is reverse-engineered from NVIDIA GameStream. Somewhere in that protocol stack, &lt;em&gt;something&lt;/em&gt; decides that if you’re not consuming P-frames, you’re not ready for more frames. Period.&lt;/p&gt;&lt;p&gt;We poked around for an hour or two, but without diving deep into the Moonlight protocol internals, we weren’t going to fix this. The protocol wanted all its frames, or no frames at all.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;“What if we implement proper congestion control?”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;looks at TCP congestion control literature&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;closes tab&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;“What if we just... don’t have bad WiFi?”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;stares at enterprise firewall that’s throttling everything&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;Act IV: Depression&lt;/h2&gt;&lt;p&gt;One late night, while debugging why the stream was frozen again, I opened our screenshot debugging endpoint in a browser tab:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;code&gt;GET /api/v1/external-agents/abc123/screenshot?format=jpeg&amp;amp;quality=70&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The image loaded instantly.&lt;/p&gt;&lt;p&gt;A pristine, 150KB JPEG of the remote desktop. Crystal clear. No artifacts. No waiting for keyframes. No decoder state. Just... pixels.&lt;/p&gt;&lt;p&gt;I refreshed. Another instant image.&lt;/p&gt;&lt;p&gt;I mashed F5 like a degenerate. 5 FPS of perfect screenshots.&lt;/p&gt;&lt;p&gt;I looked at my beautiful WebCodecs pipeline. I looked at the JPEGs. I looked at the WebCodecs pipeline again.&lt;/p&gt;&lt;p&gt;No.&lt;/p&gt;&lt;p&gt;No, we are not doing this.&lt;/p&gt;&lt;p&gt;We are professionals. We implement proper video codecs. We don’t spam HTTP requests for individual frames like it’s 2009.&lt;/p&gt;&lt;h2&gt;Act V: Acceptance&lt;/h2&gt;&lt;p&gt;typescript&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;code&gt;// Poll screenshots as fast as possible (capped at 10 FPS max)
const fetchScreenshot = async () =&amp;gt; {
  const response = await fetch(`/api/v1/external-agents/${sessionId}/screenshot`)
  const blob = await response.blob()
  screenshotImg.src = URL.createObjectURL(blob)
  setTimeout(fetchScreenshot, 100) // yolo
}&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We did it. We’re sending JPEGs.&lt;/p&gt;&lt;p&gt;And you know what? &lt;strong&gt;It works perfectly.&lt;/strong&gt;&lt;/p&gt;&lt;h2&gt;Why JPEGs Actually Slap&lt;/h2&gt;&lt;p&gt;Here’s the thing about our fancy H.264 pipeline:&lt;/p&gt;&lt;p&gt;PropertyH.264 StreamJPEG SpamBandwidth40 Mbps (constant)100-500 Kbps (varies with complexity)StateStateful (corrupt = dead)Stateless (each frame independent)Latency sensitivityVery highDoesn’t careRecovery from packet lossWait for keyframe (seconds)Next frame (100ms)Implementation complexity3 months of Rust&lt;code&gt;fetch()&lt;/code&gt; in a loop&lt;/p&gt;&lt;p&gt;A JPEG screenshot is &lt;strong&gt;self-contained&lt;/strong&gt;. It either arrives complete, or it doesn’t. There’s no “partial decode.” There’s no “waiting for the next keyframe.” There’s no “decoder state corruption.”&lt;/p&gt;&lt;p&gt;When the network is bad, you get... fewer JPEGs. That’s it. The ones that arrive are perfect.&lt;/p&gt;&lt;p&gt;And the size! A 70% quality JPEG of a 1080p desktop is like &lt;strong&gt;100-150KB&lt;/strong&gt;. A single H.264 keyframe is 200-500KB. We’re sending LESS data per frame AND getting better reliability.&lt;/p&gt;&lt;h2&gt;The Hybrid: Have Your Cake and Eat It Too&lt;/h2&gt;&lt;p&gt;We didn’t throw away the H.264 pipeline. We’re not &lt;em&gt;complete&lt;/em&gt; animals.&lt;/p&gt;&lt;p&gt;Instead, we built adaptive switching:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Good connection&lt;/strong&gt; (RTT &amp;lt; 150ms): Full 60fps H.264, hardware decoded, buttery smooth&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Bad connection detected&lt;/strong&gt;: Pause video, switch to screenshot polling&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Connection recovers&lt;/strong&gt;: User clicks to retry video&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The key insight: &lt;strong&gt;we still need the WebSocket for input&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;Keyboard and mouse events are tiny. Like, 10 bytes each. The WebSocket handles those perfectly even on a garbage connection. We just needed to stop sending the massive video frames.&lt;/p&gt;&lt;p&gt;So we added one control message:&lt;/p&gt;&lt;p&gt;json&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;code&gt;{"set_video_enabled": false}&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Server receives this, stops sending video frames. Client polls screenshots instead. Input keeps flowing. Everyone’s happy.&lt;/p&gt;&lt;p&gt;15 lines of Rust. I am not joking.&lt;/p&gt;&lt;p&gt;rust&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;code&gt;if !video_enabled.load(Ordering::Relaxed) {
    continue; // skip frame, it's screenshot time baby
}&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;The Oscillation Problem (Lol)&lt;/h2&gt;&lt;p&gt;We almost shipped a hilarious bug.&lt;/p&gt;&lt;p&gt;When you stop sending video frames, the WebSocket becomes basically empty. Just tiny input events and occasional pings.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The latency drops dramatically.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Our adaptive mode sees low latency and thinks: “Oh nice! Connection recovered! Let’s switch back to video!”&lt;/p&gt;&lt;p&gt;Video resumes. 40Mbps floods the connection. Latency spikes. Mode switches to screenshots.&lt;/p&gt;&lt;p&gt;Latency drops. Mode switches to video.&lt;/p&gt;&lt;p&gt;Latency spikes. Mode switches to screenshots.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Forever. Every 2 seconds.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The fix was embarrassingly simple: once you fall back to screenshots, &lt;strong&gt;stay there until the user explicitly clicks to retry&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;typescript&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;code&gt;setAdaptiveLockedToScreenshots(true) // no oscillation for you
```

We show an amber icon and a message: "Video paused to save bandwidth. Click to retry."

Problem solved. User is in control. No infinite loops.

---

## Ubuntu Doesn't Ship JPEG Support in grim Because Of Course It Doesn't

Oh, you thought we were done? Cute.

`grim` is a Wayland screenshot tool. Perfect for our needs. Supports JPEG output for smaller files.

Except Ubuntu compiles it without libjpeg.
```
$ grim -t jpeg screenshot.jpg
error: jpeg support disabled&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;incredible&lt;/em&gt;&lt;/p&gt;&lt;p&gt;So now our Dockerfile has a build stage that compiles grim from source:&lt;/p&gt;&lt;p&gt;dockerfile&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;code&gt;FROM ubuntu:25.04 AS grim-build
RUN apt-get install -y meson ninja-build libjpeg-turbo8-dev ...
RUN git clone https://git.sr.ht/~emersion/grim &amp;amp;&amp;amp; \
    meson setup build -Djpeg=enabled &amp;amp;&amp;amp; \
    ninja -C build
```

We're building a screenshot tool from source so we can send JPEGs in 2025. This is fine.

---

## The Final Architecture
```
┌─────────────────────────────────────────────────────────────┐
│                     User's Browser                          │
├─────────────────────────────────────────────────────────────┤
│  WebSocket (always connected)                               │
│  ├── Video frames (H.264) ──────────── when RTT &amp;lt; 150ms    │
│  ├── Input events (keyboard/mouse) ── always               │
│  └── Control messages ─────────────── {"set_video_enabled"} │
│                                                              │
│  HTTP (screenshot polling) ──────────── when RTT &amp;gt; 150ms    │
│  └── GET /screenshot?quality=70                             │
└─────────────────────────────────────────────────────────────┘&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Good connection:&lt;/strong&gt; 60fps H.264, hardware accelerated, beautiful &lt;strong&gt;Bad connection:&lt;/strong&gt; 2-10fps JPEGs, perfectly reliable, works everywhere&lt;/p&gt;&lt;p&gt;The screenshot quality adapts too:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Frame took &amp;gt;500ms? Drop quality by 10%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Frame took &amp;lt;300ms? Increase quality by 5%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Target: minimum 2 FPS, always&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;Lessons Learned&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Simple solutions often beat complex ones.&lt;/strong&gt; Three months of H.264 pipeline work. One 2am hacking session the night before production deployment: “what if we just... screenshots?”&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Graceful degradation is a feature.&lt;/strong&gt; Users don’t care about your codec. They care about seeing their screen and typing.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;WebSockets are for input, not necessarily video.&lt;/strong&gt; The input path staying responsive is more important than video frames.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ubuntu packages are missing random features.&lt;/strong&gt; Always check. Or just build from source like it’s 2005.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Measure before optimizing.&lt;/strong&gt; We assumed video streaming was the only option. It wasn’t.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;Try It Yourself&lt;/h2&gt;&lt;p&gt;Helix is open source: &lt;a href="https://github.com/helixml/helix"&gt;github.com/helixml/helix&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The shameful-but-effective screenshot code:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;api/cmd/screenshot-server/main.go&lt;/code&gt; — 200 lines of Go that changed everything&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;MoonlightStreamViewer.tsx&lt;/code&gt; — React component with adaptive logic&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;websocket-stream.ts&lt;/code&gt; — WebSocket client with &lt;code&gt;setVideoEnabled()&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The beautiful H.264 pipeline we’re still proud of:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;moonlight-web-stream/&lt;/code&gt; — Rust WebSocket server&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Still used when your WiFi doesn’t suck&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;em&gt;We’re building Helix, open-source AI infrastructure that works in the real world — even on terrible WiFi. We started by &lt;a href="https://blog.helix.ml/p/LINK-TO-PART-1"&gt;killing WebRTC&lt;/a&gt;, then we killed our replacement. Sometimes the 15-year-old solution is the right one.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Star us on GitHub: &lt;a href="https://github.com/helixml/helix"&gt;github.com/helixml/helix&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Thanks for reading HelixML! Subscribe for free to receive new posts and support my work.&lt;/p&gt;311Share&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://blog.helix.ml/p/we-mass-deployed-15-year-old-screen</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 18:00:31 +0000</pubDate>
    </item>
    <item>
      <title>Towards a secure peer-to-peer app platform for Clan</title>
      <link>https://clan.lol/blog/towards-app-platform-vmtech/</link>
      <description>While most of the existing Clan framework is dedicated to machine and service management, there’s more on the horizon. Our mission is to make sure peer-to-peer, user-controlled, community software can beat Big Tech solutions. That’s why we’re working on platform fundamentals that would open the way for our FOSS stack to match the usability and convenience of proprietary platforms.</description>
      <content:encoded>&lt;article class="article"&gt;


&lt;img src="https://clan.lol/blog/towards-app-platform-vmtech/post-hero-image_hu_300d99c55181014e.webp"/&gt;


&lt;p&gt;While most of the existing Clan framework is dedicated to machine and service management, there’s more on the horizon. Our mission is to make sure peer-to-peer, user-controlled, community software can beat Big Tech solutions. That’s why we’re working on platform fundamentals that would open the way for our FOSS stack to match the usability and convenience of proprietary platforms.&lt;/p&gt;
&lt;p&gt;Unfortunately, the FOSS world is still lagging behind commercial platforms in some important aspects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Web and mobile apps are strongly sandboxed, so while they can get very aggressive in snooping on the data they &lt;em&gt;are&lt;/em&gt; allowed to access, the enforcement of the isolation model is very robust — and there is a model for &lt;em&gt;sharing&lt;/em&gt; data that makes the isolated applications actually useful..
&lt;ul&gt;
&lt;li&gt;Meanwhile in the FOSS world, it’s still extremely common to run software with full access to the user’s account. The only project that has built anything close to a similar platform for local software is Flatpak, which is still not perfect and its main repo has a very lax policy;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Centralized Web services can have “multiple instances” simply by switching accounts; self-hosting Web services is trivially multi-instance; even Android now provides a multi-instance facility..
&lt;ul&gt;
&lt;li&gt;Meanwhile local software often doesn’t have a global database, but when it does, it can be impossible to make it multi-instance without advanced knowledge;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Commercial apps come with their own always-online remote servers. Users don’t have to think about connecting the clients to the servers, it’s all pre-connected!
&lt;ul&gt;
&lt;li&gt;Meanwhile decentralized community software is stuck between various bad options. Supporting multiple commercial backends is tedious and defeats the point anyway. Self-hosting traditional web servers can get complex and unreliable, and exposes attack surface to the public Web. Direct peer-to-peer connections can be hard to set up and unreliable too, and typically don’t provide asynchronous communication.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So… What do we need to make it possible for communities to share apps install and load &lt;strong&gt;quickly&lt;/strong&gt;, already &lt;strong&gt;pre-connected&lt;/strong&gt; to network services; are isolated to a &lt;strong&gt;worry-free&lt;/strong&gt; level of security, and yet allow for enough &lt;strong&gt;sharing&lt;/strong&gt; via explicit permissions to make them useful?&lt;/p&gt;
&lt;p&gt;The first piece of the puzzle is, unsurprisingly, Nix. The entire Clan project is built on Nix, and the future app platform is no exception. Nix makes it possible to quickly fetch and run any software – thanks to caching, as long as we steer everyone towards using very few common versions of the nixpkgs tree, most downloads could be almost as fast as web app loads.&lt;/p&gt;
&lt;p&gt;Then we have to add a &lt;strong&gt;microVM hypervisor&lt;/strong&gt; with &lt;strong&gt;Wayland and GPU virtualization&lt;/strong&gt; and a side of &lt;strong&gt;D-Bus portals&lt;/strong&gt;… and we can finally get a glimpse of the future!&lt;/p&gt;





&lt;h2&gt;microVMs&lt;/h2&gt;
&lt;p&gt;Secure isolation is essential for any modern app platform. Hardware-based virtualization is a lot more confidence-inspiring than shared-kernel isolation mechanisms like Linux namespaces. But it’s not &lt;em&gt;only&lt;/em&gt; a security measure. Running apps in VMs also improves environment consistency/reproducibility by ensuring everyone runs the same kernel — which can also give us portability, since it enables running on completely different host OSes as well.&lt;/p&gt;
&lt;p&gt;If your experience with virtualization on desktop has only been with booting entire Linux distros under something like VirtualBox, you might be very skeptical of the same technology being involved in launching applications all the time. But that’s not at all inherent to the use of KVM!&lt;/p&gt;
&lt;p&gt;Conventional VMs feel “heavy” —slow to launch, big RAM footprint, extra background CPU usage, fixed storage allocation, usually not very well integrated with the host desktop— only because their goal is to simulate a whole another computer within your existing computer. For app isolation, we don’t need that, so the whole stack can be vastly simplified and optimized for high performance and low overhead. The microVM idea was first popularized by AWS’s &lt;a href="https://firecracker-microvm.github.io/"&gt;Firecracker&lt;/a&gt; on the server side, powering instantly-launching event/request handlers in Lambda. A microVM boots directly into the kernel (skipping firmware) and does not emulate any legacy PC hardware, which results in &lt;em&gt;very&lt;/em&gt; fast boot times, on the order of a couple hundred milliseconds.&lt;/p&gt;
&lt;p&gt;Now, has this been used on the client side already? Yes, most prominently by Asahi Linux, motivated by a technical restriction that was preventing Apple machines from playing legacy Windows games. That’s the &lt;a href="https://github.com/AsahiLinux/muvm"&gt;muvm&lt;/a&gt; project, powered by &lt;a href="https://github.com/containers/libkrun"&gt;libkrun&lt;/a&gt; – a Firecracker-like VMM provided as a dynamic library so that different frontends could be built. For our platform development, we have indeed adopted muvm (after submitting &lt;a href="https://github.com/AsahiLinux/muvm/pull/192"&gt;some changes&lt;/a&gt; that make it more useful for us), combining it with namespace-based &lt;a href="https://github.com/containers/bubblewrap"&gt;Bubblewrap&lt;/a&gt; to make a script that &lt;a href="https://git.clan.lol/clan/munix"&gt;runs NixOS system closures&lt;/a&gt; in microVMs.&lt;/p&gt;
&lt;p&gt;…Wait, did someone mention playing games– like, highly GPU-demanding games? In a VM? Without a dedicated GPU?&lt;/p&gt;
&lt;h2&gt;Desktop and GPU support&lt;/h2&gt;





&lt;p&gt;In the Beginning (of virtio-gpu), there was the Framebuffer. An emulated computer monitor, a single rectangle representing the entire graphical output of the VM. Then there was &lt;a href="https://docs.mesa3d.org/drivers/virgl.html"&gt;VirGL&lt;/a&gt;, a way to forward the OpenGL API across the VM boundary to make the host render on its GPU on behalf of the VM, so that 3D graphics could be displayed on the emulated monitor. It wasn’t super fast, it wasn’t compatible with the latest GL extensions, it wasn’t very secure, but it was something. With the advent of Vulkan, &lt;a href="https://docs.mesa3d.org/drivers/venus.html"&gt;Venus&lt;/a&gt; was started as the Vulkan version of the same thing.&lt;/p&gt;
&lt;p&gt;Meanwhile, the Chrome OS team was working on adding support for Linux apps. While it was initially based on namespaces, they quickly started working on switching to hardware virtualization. The virtio-gpu device was extended to support arbitrary “cross-domain” protocols, making it possible —with some wrapping-unwrapping— to forward Unix domain sockets that pass certain types of file descriptors (shared memory and DMA-BUF) to the guest. (Well, initially it was a whole separate virtual device but let’s skip over that.) Google’s crosvm supports &lt;a href="https://crosvm.dev/book/devices/wayland.html"&gt;connecting to the host Wayland socket&lt;/a&gt; to that facility, and the team wrote &lt;a href="https://chromium.googlesource.com/chromiumos/platform2/+/refs/heads/main/vm_tools/sommelier/README.md"&gt;Sommelier&lt;/a&gt; as the guest-side proxy that exposes a normal Wayland socket to guest apps.&lt;/p&gt;
&lt;p&gt;The part of crosvm responsible for handling the virtio-gpu device was written as a reusable library called Rutabaga (now &lt;a href="https://github.com/magma-gpu/rutabaga_gfx"&gt;living outside of the CrOS repos&lt;/a&gt;), and integrated into other VMMs such as good old Qemu. Sommelier was packaged by various Linux distros as well, and one enthusiast wrote &lt;a href="https://roscidus.com/blog/blog/2021/03/07/qubes-lite-with-kvm-and-wayland/"&gt;an entire alternative to Sommelier&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Meanwhile, there was also a lot to improve in terms of accessing the GPU. As we’ve mentioned already, API forwarding solutions like VirGL/Venus leave a lot to be desired. PCIe passthrough requires a dedicated GPU, or SR-IOV support which GPU vendors have mostly restricted to enterprise models. However… of course a better way was possible! Rob Clark presented &lt;a href="https://indico.freedesktop.org/event/2/contributions/53/attachments/76/121/XDC2022_%20virtgpu%20drm%20native%20context.pdf"&gt;DRM native contexts&lt;/a&gt; at XDC 2022: this approach essentially paravirtualizes the kernel-space GPU driver, letting the guest submit hardware-specific commands that the host would run in separate contexts (relying on the same separation as between programs on the host). That’s the approach that was picked up by the Asahi Linux project for gaming because of the amazing performance it allows for, but it’s also intended to be a stronger security boundary due to providing way less attack surface on the host (it’s all I/O management rather than implementing complex APIs).&lt;/p&gt;
&lt;p&gt;So, was it possible to take all of this technology and use it? Well… it required quite a bit of debugging and fixing everywhere – but that’s exactly why I joined! So far I’ve discovered that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the rutabaga_gfx integration in QEMU (which was the thing we tried to use initially) and other C consumers was broken with the latest versions due to &lt;a href="https://issuetracker.google.com/issues/440386997"&gt;an &lt;code&gt;ifdef&lt;/code&gt; mistake&lt;/a&gt; (&lt;a href="https://github.com/magma-gpu/rutabaga_gfx/pull/9"&gt;fixed&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;it’s not documented everywhere that &lt;a href="https://gitlab.com/qemu-project/qemu/-/issues/2574"&gt;kernel &amp;gt;= 6.13 is required&lt;/a&gt; to be able to touch AMD GPU memory from KVM guests in any way&lt;/li&gt;
&lt;li&gt;Sommelier was &lt;a href="https://issuetracker.google.com/u/2/issues/441537635"&gt;assuming Chromium OS kernel patches&lt;/a&gt; and misinterpreting &lt;code&gt;ioctl&lt;/code&gt; responses on regular mainline Linux&lt;/li&gt;
&lt;li&gt;libkrun’s internal version of rutabaga_gfx contained a tiny strange API modification incompatible with Sommelier/proxy-virtwl and didn’t handle memfd seals (&lt;a href="https://github.com/containers/libkrun/pull/407"&gt;fixed&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;RADV (Radeon Vulkan driver in Mesa) only recognized PCI devices including for virtgpu, ignoring the &lt;code&gt;virtio-mmio&lt;/code&gt; setup used by libkrun (&lt;a href="https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/37281"&gt;fixed&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And we’re continuing with more work in this area.&lt;/p&gt;
&lt;h2&gt;D-Bus / XDG Desktop Portals&lt;/h2&gt;





&lt;p&gt;Application isolation is great, but completely isolated applications tend to have limited usefulness. That’s why we’re also integrating &lt;a href="https://flatpak.github.io/xdg-desktop-portal/"&gt;desktop portals&lt;/a&gt; that Flatpak uses —at least the file-opening / document portal— into the microVM-based platform.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://git.clan.lol/clan/sidebus"&gt;sidebus&lt;/a&gt; project is inspired by &lt;a href="https://spectrum-os.org/"&gt;Spectrum&lt;/a&gt;’s setup for using the document portal with virtiofs to dynamically expose chosen files to the guest, using vsock as the D-Bus transport. It is based on the &lt;a href="https://github.com/dbus2/busd"&gt;busd&lt;/a&gt; broker library, and uses the portal frontend on the host for perfect integration with arbitrary desktop environments.&lt;/p&gt;
&lt;p&gt;With the switch to libkrun however, we are looking at the possibility of making the Camera and Screencast portals working, with full hardware acceleration – by switching to virtgpu cross-domain as the transport instead of vsock. Currently libkrun already has added some PipeWire support to its copy of rutabaga_gfx, however that’s fixed to one system-wide socket. How these portals work is that for every request they pass a new restricted PipeWire remote socket over D-Bus. So we’re looking to make rutabaga’s cross-domain sockets more generic, to be able to just pass through that whole chain of file descriptor passing.&lt;/p&gt;
&lt;p&gt;(And yes, lots of people are worried about PipeWire attack surface — it’s definitely possible to mitigate that with a proxy on the host that would only allow a small validated subset of the PipeWire protocol.)&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We’re looking to finally make a peer-to-peer community software platform that’s competitive with commercial ones in terms of security, usability and convenience.
If you want to try it out now, you can! Just follow the installation instructions on our &lt;a href="https://git.clan.lol/clan/munix"&gt;munix project&lt;/a&gt;.
Note that it’s still actively being developed, so if you find any issues, please open up a bug report!&lt;/p&gt;

&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://clan.lol/blog/towards-app-platform-vmtech/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 17:34:22 +0000</pubDate>
    </item>
    <item>
      <title>Fabrice Bellard Releases MicroQuickJS</title>
      <link>https://github.com/bellard/mquickjs/blob/main/README.md</link>
      <description>mquickjs/README.md at main · bellard/mquickjs · GitHub</description>
      <content:encoded>&lt;main id="js-repo-pjax-container"&gt;






&lt;a href="https://github.com/bellard"&gt;
        bellard
&lt;/a&gt; 
/
&lt;strong&gt;
&lt;a href="https://github.com/bellard/mquickjs"&gt;mquickjs&lt;/a&gt;
&lt;/strong&gt;
Public



&lt;ul&gt;
&lt;li&gt;
&lt;a href="https://github.com/login?return_to=%2Fbellard%2Fmquickjs"&gt; Notifications
&lt;/a&gt; You must be signed in to change notification settings
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://github.com/login?return_to=%2Fbellard%2Fmquickjs"&gt; Fork
    11
&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;

&lt;a href="https://github.com/login?return_to=%2Fbellard%2Fmquickjs"&gt; 
          Star
 539
&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;













&lt;/main&gt;</content:encoded>
      <guid isPermaLink="false">https://github.com/bellard/mquickjs/blob/main/README.md</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 17:33:42 +0000</pubDate>
    </item>
    <item>
      <title>Meta is using the Linux scheduler designed for Valve's Steam Deck on its servers</title>
      <link>https://www.phoronix.com/news/Meta-SCX-LAVD-Steam-Deck-Server</link>
      <description>Meta Is Using The Linux Scheduler Designed For Valve's Steam Deck On Its Servers - Phoronix</description>
      <content:encoded>&lt;article class="full"&gt;
&lt;h1&gt;Meta Is Using The Linux Scheduler Designed For Valve's Steam Deck On Its Servers&lt;/h1&gt;
Written by &lt;a href="https://www.michaellarabel.com/"&gt;Michael Larabel&lt;/a&gt; in &lt;a href="https://www.phoronix.com/linux/Linux+Kernel"&gt;Linux Kernel&lt;/a&gt; on 23 December 2025 at 06:10 AM EST. &lt;a href="https://www.phoronix.com/forums/node/1601261"&gt;13 Comments&lt;/a&gt;

&lt;img alt="LINUX KERNEL" src="https://www.phoronix.com/assets/categories/linuxkernel.webp"/&gt;
An interesting anecdote from this month's Linux Plumbers Conference in Tokyo is that Meta (Facebook) is using the Linux scheduler originally designed for the needs of Valve's Steam Deck... On Meta Servers. Meta has found that the scheduler can actually adapt and work very well on the hyperscaler's large servers.&#13;
&lt;br/&gt;
&lt;br/&gt;SCX-LAVD as the Latency-criticality Aware Virtual Deadline scheduler has &lt;a href="https://www.phoronix.com/news/LAVD-Scheduler-Linux-Gaming"&gt;worked out very well for the needs of Valve's Steam Deck&lt;/a&gt; with similar or better performance than &lt;a href="https://www.phoronix.com/search/EEVDF"&gt;EEVDF&lt;/a&gt;. SCX-LAVD has been worked on by Linux consulting firm Igalia under contract for Valve. SCX-LAVD has also seen varying use by the CachyOS Handheld Edition, Bazzite, and other Linux gaming software initiatives.&#13;
&lt;br/&gt;&lt;p&gt;&lt;a href="https://www.phoronix.com/image-viewer.php?id=2025&amp;amp;image=meta_lavd_1_lrg"&gt;&lt;img alt="Steam Deck and Server" src="https://www.phoronix.net/image.php?id=2025&amp;amp;image=meta_lavd_1_med"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;br/&gt;It turns out that besides working well on handhelds, SCX-LAVD can also end up working well on large servers too. The presentation at LPC 2025 by Meta engineers was in fact titled "&lt;em&gt;How do we make a Steam Deck scheduler work on large servers&lt;/em&gt;." At Meta they have explored SCX_LAVD as a "default" fleet scheduler for their servers that works for a range of hardware and use-cases for where they don't need any specialized scheduler.&#13;
&lt;br/&gt;&lt;p&gt;&lt;img alt="Meta SCX LAVD" src="https://www.phoronix.net/image.php?id=2025&amp;amp;image=meta_lavd_2_med"/&gt;&lt;/p&gt;
&lt;br/&gt;They call this scheduler built atop sched_ext as "Meta's New Default Scheduler". LAVD they found to work well across the growing CPU and memory configurations of their servers, nice load balancing between CCX/LLC boundaries, and more. Those wishing to learn more about Meta's use and research into SCX-LAVD can find the Linux Plumbers Conference presentation embedded below along with the &lt;a href="https://lpc.events/event/19/contributions/2099/attachments/1875/4020/lpc-2025-lavd-meta.pdf"&gt;slide deck&lt;/a&gt;.&#13;
&lt;br/&gt;&lt;p&gt;&lt;/p&gt;









&lt;a href="https://www.phoronix.com/forums/node/1601261"&gt;13 Comments&lt;/a&gt; 

&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://www.phoronix.com/news/Meta-SCX-LAVD-Steam-Deck-Server</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 17:08:34 +0000</pubDate>
    </item>
    <item>
      <title>When irate product support customers demand to speak to Bill Gates</title>
      <link>https://devblogs.microsoft.com/oldnewthing/20251223-00/?p=111896</link>
      <description>A colleague of mine who used to work in product support told me that they had a procedure if a customer became irate and demanded to speak with Bill Gates. (This was, of course, back in the days when Bill Gates still ran the company.)</description>
      <content:encoded>&lt;article class="middle-column pe-xl-198" data-clarity-region="article" id="post-111896"&gt;

&lt;p&gt;A colleague of mine who used to work in product support told me that they had a procedure if a customer became irate and demanded to speak with Bill Gates. (This was, of course, back in the days when Bill Gates still ran the company.)&lt;/p&gt;
&lt;p&gt;The product support technician would apologize for not resolving the problem to the customer’s satisfaction, but if the customer continued to demand to speak with The Boss, the technician would indeed transfer the customer.&lt;/p&gt;
&lt;p&gt;The customer was transferred to a special internal phone number, and when the operators saw a call on that line, they took the call and said, “Bill Gates’s office.” They weren’t actually in Bill Gates’s office. They were just pretending to be Bill Gates’s secretary. Their job was to tell the caller that Mr. Gates is currently unavailable, but if the customer leaves a message and their contact information, they will pass the information to Mr. Gates.&lt;/p&gt;
&lt;p&gt;Of course, the information was never actually passed along to Bill. The information went back into the product support channel with a note that the customer was escalated to “Bill Gates’s office.” The technician who returned the call would probably say something like “Bill Gates asked me to contact you to follow up on an issue you had earlier.”&lt;/p&gt;
&lt;!-- .entry-content --&gt;
&lt;!-- AI Disclaimer --&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://devblogs.microsoft.com/oldnewthing/20251223-00/?p=111896</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 16:43:03 +0000</pubDate>
    </item>
    <item>
      <title>Stop Slopware</title>
      <link>https://stopslopware.net/</link>
      <description>Stop Slopware</description>
      <guid isPermaLink="false">https://stopslopware.net/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 15:51:17 +0000</pubDate>
    </item>
    <item>
      <title>Test, don't (just) verify</title>
      <link>https://alperenkeles.com/posts/test-dont-verify/</link>
      <description>AI is making formal verification go mainstream.</description>
      <content:encoded>&lt;article&gt;


        Test, don't (just) verify.


                    
                        Posted on 2025-12-22
                    

                    

                    :: 
  
  
  
  13 min read


                    
                    
                             :: Tags:

&lt;a href="https://alperenkeles.com/tags/software-engineering/"&gt;🏷software engineering&lt;/a&gt;, 
                                
                                    &lt;a href="https://alperenkeles.com/tags/testing/"&gt;🏷testing&lt;/a&gt;, 
                                
                                    &lt;a href="https://alperenkeles.com/tags/formal-methods/"&gt;🏷formal methods&lt;/a&gt;




&lt;p&gt;AI &lt;em&gt;is&lt;/em&gt; making formal verification go mainstream.&lt;/p&gt;
&lt;p&gt;AI-assisted mechnical proving companies are raising funds on billion dollar &lt;a href="https://www.reuters.com/business/robinhood-ceos-math-focused-ai-startup-harmonic-valued-145-billion-latest-2025-11-25/"&gt;valuations&lt;/a&gt;, new people are trying proof assistants,
overwhelmingly Lean, at unprecedented rates. Models achieve fascinating results in competitions previously considered to contain
some of the hardest problems in the world, such as IMO, ICPC, Putnam; as well as open problems in mathematics such as Erdös Problems.
It's not just the hobbyists that are excited about AI-assisted proofs, from &lt;a href="https://terrytao.wordpress.com/wp-content/uploads/2024/03/machine-jan-3.pdf"&gt;Terry Tao&lt;/a&gt;, to &lt;a href="https://martin.kleppmann.com/2025/12/08/ai-formal-verification.html"&gt;Martin Kleppman&lt;/a&gt;, to &lt;a href="https://x.com/ilyasergey/status/1989053674552004749"&gt;Ilya Sergey&lt;/a&gt;, prominent researchers around the world are excited and hopeful about the effects.&lt;/p&gt;
&lt;h2&gt;&lt;a href="#formal-verification-the-goods"&gt;Formal Verification: The Goods&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Let me quickly give you a run down of the argument:&lt;/p&gt;
&lt;p&gt;There are multiple complex challenges in formal verification. The first one, and the one that is very hard to solve technically,
is that most software in the world does not have a formal specification. A formal specification is a simpler mathematical description
of the system we build. Algorithms have formal specifications. Data structures, protocols, data formats, safety-critical systems typically
have formal specifications. The majority of the programs in the world doesn't have a formal specification, hell, most of them don't even have
informal specifications. At the limit, which is where we actually are, the specification of a program is itself, the implementation is the
specification. The lack of a formal specification makes it very hard to formally verify some piece of software, because what would you even
verify?&lt;/p&gt;
&lt;p&gt;The second issue is, proof engineering, the practice of writing proofs for theorems about your systems, is very hard. The proofs have many domain
specific elements to them, a proof of a mathematical theorem will be very different from a proof about a programming language, and a proof about the programming
language will highly depend on the underlying constructs of its theoretical framework. The widest taught proof engineering book
is &lt;a href="https://softwarefoundations.cis.upenn.edu"&gt;Software Foundations&lt;/a&gt;, and every chapter has a different style of proofs. Someone that went through
Volume 2: Programming Language Foundations will not find the problems in Volume 6: Separation Logic Foundations intuitive or obvious. There are other problems
such as the tooling for proof automation, brittleness of proofs, reusability of proofs etc. but I don't find them particularly fundamental to proof engineering
itself but rather problems of the current generation, so we can leave those aside for now.&lt;/p&gt;
&lt;p&gt;The rise of LLMs in programming vastly affects both of these points. It affects point number 1 because AI-assisted programming is a very natural fit
fot specification-driven development. AI-assisted programming pushes the limits of programming from what you can implement to what you can specify and
what you can &lt;a href="https://alperenkeles.com/posts/verifiability-is-the-limit/"&gt;verify&lt;/a&gt;. This is a great incentive for writing executable specifications,
because then you can put the LLM inside a loop until it achieves the said objective, irrespective of the means of the achievement.
&lt;a href="https://alperenkeles.com/posts/verifiable-abstractions/"&gt;I predict that&lt;/a&gt; this will give rise to program optimizers and translators that will
be transformative of our work in those domains. However, tests are, as infamously they are, incomplete. Tests are great at finding bugs (actually
they are not so great most of the time, but that's another discussion), but they &lt;strong&gt;cannot&lt;/strong&gt; prove the absence of bugs. SQLite famously has
millions of tests, but researchers still find bugs in SQLite, similar situations arise in almost all software.&lt;/p&gt;
&lt;p&gt;The only way to prove the absence of bugs is formal verification, and industry has seen great examples of this. Highly cited formal verification
projects include CompCert C Compiler, &lt;a href="https://users.cs.utah.edu/~regehr/papers/pldi11-preprint.pdf"&gt;the random testing of which against GCC and Clang&lt;/a&gt;
has led to finding 79 bugs in GCC and 202 bugs in Clang, and only 2 bugs in CompCert in its &lt;em&gt;unverified&lt;/em&gt; parser, finding no bugs in its verified compilation
pass, a striking win for formal verification. (Thanks to &lt;a href="https://emptysqua.re/blog/"&gt;A. Jesse Jiryu Davis&lt;/a&gt; informing me, I learned that researchers have
&lt;a href="https://www.cs.purdue.edu/homes/pfonseca/papers/eurosys2017-dsbugs.pdf"&gt;studied&lt;/a&gt; the source of failures in formally verified distributed systems, and
found that wrong assumptions about the unverified parts of the system are the likely culprit.)&lt;/p&gt;
&lt;p&gt;This makes formal verification a prime target for AI-assisted programming. Given that we have a formal specification, we can just let the machine wander around
for hours, days, even weeks. If it comes back with a solution, we shall trust not to the probabilistic predictions of the so-called artificial intelligence,
but the symbolic proof checker that verifies the solution. This idea has always been at the core of formal verification, the ability to have unsound proof
generation coupled with sound proof checking has given rise to complex tactics, algorithms that produce proofs by searching them, to enable proof engineers
in great capacity.&lt;/p&gt;
&lt;p&gt;This is not the end of the good news. AI is also very good at writing proofs, at least much better than the average software engineer. Given that we have
a perfect oracle, we can also turn the problem into an RLVR (Reinforcement Learning with Verifiable Rewards), which means we should expect the models to get
even better at it as time goes on as they did in chess, go, and many other similar problems. This is the promise behind the billion dollar valuations,
the companies started with impressive autoformalization techniques and automated provers for tackling competition problems and open conjectures, but
the real industrial value is at the core of automating software engineering by letting the engineer write a verbal description, autoformalized into a Lean
theorem, proven by the automated prover, and voila, we have our program that we can fully trust.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;or do we?&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a href="#formal-verification-the-bads"&gt;Formal Verification: The Bads&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I see the appeal, I like the idea, I trust the researchers building these systems, but I don't love the overarching promises. This blog post
is my attempt at building a reasonable middle ground by laying out the goods (as I already did), and the bads (as I now will), and make my closing
remarks by reiterating the position of testing in this space, and in the future.&lt;/p&gt;
&lt;h3&gt;&lt;a href="#autoformalization-is-a-shaky-ground"&gt;Autoformalization is a shaky ground&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I started this post by stating most programs in the world do not have formal specifications, followed by how AI is incentivizing us to write specifications,
and autoformalization takes the specifications, and makes them &lt;strong&gt;formal&lt;/strong&gt;. In formal verification, there's this concept of a &lt;em&gt;trusted computing base (TCB)&lt;/em&gt;. TCB
is your Achilles' Heel, it's the bottom of the ladder, where layers over layers of verification is built on, trusting a small core without verifying it, because
there must be a bottom of the ladder, we cannot build a circular verification argument, and the system cannot verify itself. (please fact check me on this
if I'm wrong, which is possible)&lt;/p&gt;
&lt;p&gt;Autoformalization is part of the TCB in this AI-assisted verified programming paradigm, because one cannot mechanically verify that the verbally stated
specification indeed corresponds to the formalized one. There are several modes of usage, one issue is soundness, there might be mechanically verified scenarios
that would be rejected by the verbal specification. Another issue is completeness, the formalized model might reject valid scenarios in our descriptions.
Autoformalization part of the process requires and deserves our special attention as it's one of the crucial points of failure in this verification process.&lt;/p&gt;
&lt;h3&gt;&lt;a href="#proof-assistants-are-slow"&gt;Proof assistants are slow&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In a proof assistant, the primary goal is reasoning about programs and ease of verification. For instance, proof assistants, traditionally, don't use our classic
two's complement integers packed into words in our memory, they use Peano numbers, an encoding of natural numbers as inductive data structures:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Inductive Nat : Type :=
| zero : Nat
| succ : Nat -&amp;gt; Nat.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This encoding doesn't possess the concept of an integer overflow. Its inductive structure allows us to build theorems that hold for all natural numbers, not
just the ones that can fit in a word. It is also painfully slow, the computational complexity of &lt;code&gt;a + b&lt;/code&gt;, an operation so fast in CPU that it's literally
an instant, is &lt;code&gt;O(a + b)&lt;/code&gt;, addition is linear in time to the added values instead of a constant operation. However, we would like to run verified code on
real life workloads, so we cannot wait for a million cycles to add 1M + 1M = 2M. There are 2 solutions to this problem, the first one is that you build a
more efficient encoding in the proof assistant, prove equivalence of the efficient but hard to reason encoding to the inefficient but simpler to reason one,
and use the efficient one in your computations. The other is once again, axiomatization, or broadening the TCB. Proof assistants offer a mechanism called
&lt;strong&gt;extraction&lt;/strong&gt; that allows for extracting a piece of code in the language of the proof assistant (e.g Rocq) to a language with a larger ecosystem optimized
for running production workloads (e.g OCaml). Much of the extraction is a one-to-one correspondence between the languages via syntactic transformations,
but we can hijack the extraction to axiomatize our own types. We can turn Nat into unsigned integers, where &lt;code&gt;Nat.add&lt;/code&gt; becomes &lt;code&gt;u64 + u64&lt;/code&gt;. For instance,
in Rocq, Nats are extracted to BigInts, which should have the same semantics, but the "should" in this sentence carries the heavyweight. Without an accompanying
proof of correctness, we just put BigInt into the TCB.&lt;/p&gt;
&lt;p&gt;Without broadening the TCB, the speed of a proof assistant will be limited by the large amount of pointer chasing that arises naturally with the use of
inductive types and their tree-inducing characteristics. There are many domains in which speed isn't that big of a deal, but I think there's also a
concern between speed and the requirement for correctness, as faster code tends to involve more complex language constructs such as concurrency,
bit manipulation, layout awareness, which leads to more bugs than their simpler counterparts. If we cannot reason about programs that are more likely
to have bugs, how much of the overarching problem are we tackling?&lt;/p&gt;
&lt;h3&gt;&lt;a href="#verification-requires-models-and-models-are-very-hard-to-come-by"&gt;Verification requires models, and models are very hard to come by&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In order to verify some property of a system, one needs a model of the system. These models don't grow on trees, they are crafted by domain
experts over a number of years. People have been building models for programs with pointers (separation logic), as well as programs with
concurrency, programs with randomness, programs with algebraic effects, and perhaps many more that I haven't even heard of. In proving a property
of the system, we need a foundation for our reasoning process, which these theories give us for their respective domains. However, there are many
domains we don't have good models for, one example is runtime performance. This has been a contentious issue in the computer science curriculum, the
asymptotic complexity does not translate to program behavior in real hardware. Modern CPUs have cache lines, speculative execution, branch prediction
that makes the plain old abstract machine used in asymptotical analysis obsolete for many scenarios. We do not even have a single hardware to conform
our model to, we have tens of different configurations of hardware, each of which will give different results in our measurements. If we tried to prove
that some piece of code has a better result for a specific memory/processor pair than another one, we would have a massive job on our hands, I personally
don't even know where I would start with.&lt;/p&gt;
&lt;p&gt;Contrast this to testing, where you can just spin up the same algorithm on both machines, run our benchmarks, which will be our final ground truth. Testing
is almost universally considered inferior to formal verification, it is what you do when you don't have the resources to justify verification, because if you
had the opportunity to spend the time, proving absence of bugs, or universal facts about your system, would be much more valuable than codifiying the results
of singular measurements. I am here to tell you that there are tests we can write that could not be formally verified, because while building the underlying
models for verification might be hard, just using the real hardware for our measurements can be much easier.&lt;/p&gt;
&lt;h3&gt;&lt;a href="#verification-doesn-t-tell-you-that-you-are-going-down-the-wrong-path"&gt;Verification doesn't tell you that you are going down the wrong path&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In games, there are clear winners and losers. In verification, you can prove that your theorem is correct, you can prove that your theorem is incorrect, but
absence of a proof for a theorem does not imply that the theorem is incorrect, it is possible that you just haven't found the proof. This means the feedback
you get when writing a proof is limited as you cannot rely on your inability to progress as a signal about your theorem, it is plausible that you are the problem.
This is why QuickChick, a testing tool descending from the famous QuickCheck of Haskell that introduced Property-Based Testing to the world, exists in Rocq
ecosystem as one of the three most popular packages. If verification was strictly superior to testing, QuickChick would have no reason to exist, but it serves
a very crucial role that the verification process needs, &lt;strong&gt;falsification&lt;/strong&gt;. I had said that the absence of a proof does not imply that a theorem is wrong, but the witness of a violation of the theorem definitely does. Random testing is the prime suspect for finding such counterexamples, pulling the verifier out of the
rabbit hole of going through many useless paths in the proof search before giving up, because the theorem is ultimately wrong, there is no proof to be found.&lt;/p&gt;
&lt;h2&gt;&lt;a href="#random-testing-and-formal-verification"&gt;Random Testing and Formal Verification&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I have given examples of tests that formal verification is incapable of modeling, and examples of tests that complement formal verification process by creating
a short that falsifies false theorems instead of trying to prove them in vain. The synergy between testing and formal verification doesn't end here, I am a firm
supporter of &lt;a href="https://aws.amazon.com/blogs/opensource/lean-into-verified-software-development/"&gt;Verification-Guided Development (VGD)&lt;/a&gt;, which in addition to leveraging this synergy, solves the problem of proof
assistants being too slow. In verification
guided development, we implement two versions of the same system, one is the simpler to reason, verified version, the other is the complex, production one. We then
test the property that the production system conforms to the reference implementation that is verified by running them with the same inputs and asserting that
the result is the same every single time. VGD &lt;em&gt;lifts&lt;/em&gt; the proof to the production implementation from the slower one implemented in the proof assistant by leveraging
differential random testing, which allows for building a best-of-both-worlds system that is both correct and fast. Below is an image taken from &lt;a href="https://arxiv.org/abs/2407.01688"&gt;the paper&lt;/a&gt;
that (as far as I know) introduced the notion of VGD, explaining their workflow.&lt;/p&gt;
&lt;p&gt;&lt;img alt="cedar vgd" src="https://alperenkeles.com/posts/test-dont-verify/image.png"/&gt;&lt;/p&gt;
&lt;p&gt;VGD doesn't solve all the issues I mention in the rest of the post, but it removes the slowness out of the mix. As we have a production implementation ready to be tested thoroughly,
we can make all kinds of measurements that fall out of the purview of verification, but into the realm of testing. It levels the playing field between the verified realm of computing
and the unverified one, reducing the downsides of the verification leveraging testing.&lt;/p&gt;
&lt;h2&gt;&lt;a href="#closing-remarks"&gt;Closing Remarks&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I would like to state it once more for everyone:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I see the appeal, I like the idea, I trust the researchers building these systems, but I don't love the overarching promises. This blog post
is my attempt at building a reasonable middle ground by laying out the goods, and the bads, and make my closing
remarks by reiterating the position of testing in this space, and in the future.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I believe random testing plays as important of a role as formal verification in the future of software engineering. We will not have magically formally verified systems
in many domains, but as autoformalization tools get better, we will have many, many more formal specifications. Random testing benefits from these formal specifications
in different ways than formal verification, but both have their places. Proof systems will be incomplete without the accompanying testing tools, and the testing tools
will be incomplete without proofs of correctness, it is only possible via some combination of verification and testing that we can achieve our ideals of the future of
software engineering, live in a world where bugs are considered anomalies instead of the norm, where correctness is a virtue, and the bugs in our systems are as old and
as forgotten as the diseases we learned to cure and put away.&lt;/p&gt;

&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://alperenkeles.com/posts/test-dont-verify/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 12:56:50 +0000</pubDate>
    </item>
    <item>
      <title>Ryanair fined €256M over ‘abusive strategy’ to limit ticket sales by OTAs</title>
      <link>https://www.theguardian.com/business/2025/dec/23/ryanair-fined-limit-online-travel-agencies-ticket-sales-ota</link>
      <description>Italy’s competition authority says Irish airline implemented technical obstacles to force sales through its own website</description>
      <content:encoded>&lt;article class="dcr-1xdhyk6" style="background-color:var(--article-background)"&gt;&lt;img alt="Passengers board a Ryanair Boeing 737 MAX in Cagliari, Italy, through its rear entrance; a wing of the plane with gold lettering reading Ryanair looms over the photographer." src="https://i.guim.co.uk/img/media/0c0d402539470b70559e972804fa15cf214418f2/0_0_3780_3024/master/3780.jpg?width=465&amp;amp;dpr=1&amp;amp;s=none&amp;amp;crop=none"/&gt; Ryanair has reached a record valuation of €31bn (£27bn), making it the world’s second most valuable airline behind Delta. Photograph: John Keeble/Getty Images&lt;a href="#img-1"&gt;View image in fullscreen&lt;/a&gt;Ryanair has reached a record valuation of €31bn (£27bn), making it the world’s second most valuable airline behind Delta. Photograph: John Keeble/Getty Images&lt;h1&gt;Ryanair fined €256m over ‘abusive strategy’ to limit ticket sales by online travel agencies&lt;/h1&gt;&lt;p&gt;Italy’s competition authority says Irish airline implemented technical obstacles to force sales through its own website&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.theguardian.com/business/live/2025/dec/23/tesla-sales-fall-europe-byd-surges-car-industry-stock-market-ftse-business-live-news-updates"&gt;Business live – latest updates&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Ryanair has been fined €256m (£223m) by Italy’s competition authority for abusing its dominant market position to limit sales of tickets by online travel agents.&lt;/p&gt;&lt;p&gt;The authority said Europe’s largest airline had “implemented an abusive strategy to hinder travel agencies” via an “elaborate strategy” of technical obstacles for agents and passengers to make it difficult for online travel agents to sell Ryanair tickets and instead force sales through its own website.&lt;/p&gt;&lt;p&gt;The fine related to Ryanair’s conduct between April 2023 and at least until April 2025, the authority said on Tuesday. It said Ryanair had prevented online travel agents from selling tickets on its flights in combination with other airlines and services, weakening competition.&lt;/p&gt;&lt;p&gt;Ryanair said it would immediately appeal against the “legally flawed” ruling.&lt;/p&gt;&lt;p&gt;The Ryanair chief executive, &lt;a href="https://www.theguardian.com/business/michael-oleary"&gt;Michael O’Leary&lt;/a&gt;, had decided to wage war on what he &lt;a href="https://www.theguardian.com/business/2024/jan/29/ryanair-cuts-profits-forecast-flights-lastminute-opodo"&gt;described as “pirate” travel agents&lt;/a&gt;, such as Booking.com, Kiwi and Kayak. O’Leary accused the travel agent industry of scamming and ripping off unsuspecting consumers by charging extra fees and markups on ticket prices.&lt;/p&gt;&lt;p&gt;O’Leary was prepared to accept lower ticket sales as he tried to prevent travel agents from selling tickets, forcing their passengers to fill out extra forms supposedly as a security measure. The abrupt removal of Ryanair flights from agents’ websites in late 2023 &lt;a href="https://www.theguardian.com/business/2024/jan/03/ryanair-ticket-sales-hit-after-travel-agent-websites-delist-airline"&gt;caused a drop in sales for the airline&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The lower sales &lt;a href="https://www.theguardian.com/business/2024/jan/29/ryanair-cuts-profits-forecast-flights-lastminute-opodo"&gt;dented Ryanair’s profits&lt;/a&gt;, although they have not prevented the Irish airline from rising to a record valuation of €31bn (£27bn). That has made it the world’s second most valuable airline, behind only the US’s Delta Air Lines.&lt;/p&gt;&lt;p&gt;O’Leary – who is known for his combative and often sweary criticisms of airports, rivals and regulators – is planning to hand over control of the business to a successor within the next five to 10 years. He will be &lt;a href="https://www.theguardian.com/business/2023/dec/18/ryanair-boss-michael-oleary-heads-for-100m-bonus"&gt;given shares worth €111m&lt;/a&gt; (£97m) if he stays at the airline until the end of July 2028. He was already a billionaire on paper because of his shareholding.&lt;/p&gt;&lt;p&gt;Responding to the ruling, O’Leary said it was “an affront to consumer protection and competition law”.&lt;/p&gt;&lt;p&gt;He added: “The internet and the ryanair.com website have enabled Ryanair to distribute directly to consumers, and Ryanair has passed on these 20% cost savings in the form of the lowest air fares in &lt;a href="https://www.theguardian.com/world/italy"&gt;Italy&lt;/a&gt; and Europe.&lt;/p&gt;&lt;p&gt;“Ryanair looks forward to successfully overturning this legally flawed ruling and its absurd €256m fine in the courts.”&lt;/p&gt;&lt;p&gt;The vast majority of Ryanair’s sales took place through its website even before the battle against online travel agents. However, the Italian authority said Ryanair had been guilty of “abuse of a dominant position” and using its “significant market power” in trying to stamp out the business.&lt;/p&gt;&lt;p&gt;Ryanair’s tactics included rolling out facial recognition procedures for people who bought tickets via a third party, claiming that was necessary for security. It then “totally or intermittently blocked booking attempts by travel agencies”, including by blocking payment methods and mass-deleting accounts.&lt;/p&gt;&lt;p&gt;The airline then “imposed partnership agreements” on agencies that banned sales of Ryanair flights in combinations with other carriers, and blocked bookings to force them to sign up. Only in April this year did it allow agencies’ websites to link up with its own services, allowing effective competition.&lt;/p&gt;&lt;p&gt;The competition authority said Ryanair’s actions had “blocked, hindered or made such purchases more difficult and/or economically or technically burdensome when combined with flights operated by other carriers and/or other tourism and insurance services”.&lt;/p&gt;Explore more on these topics&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/business/ryanair"&gt;Ryanair&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/business/michael-oleary"&gt;Michael O'Leary&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/business/theairlineindustry"&gt;Airline industry&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/world/ireland"&gt;Ireland&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/world/italy"&gt;Italy&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/world/europe-news"&gt;Europe&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/business/regulators"&gt;Regulators&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.theguardian.com/tone/news"&gt;news&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;a href="mailto:?subject=Ryanair fined €256m over ‘abusive strategy’ to limit ticket sales by online travel agencies&amp;amp;body=https://www.theguardian.com/business/2025/dec/23/ryanair-fined-limit-online-travel-agencies-ticket-sales-ota?CMP=share_btn_url"&gt;Share&lt;/a&gt;&lt;a href="https://syndication.theguardian.com/?url=https%3A%2F%2Fwww.theguardian.com%2Fbusiness%2F2025%2Fdec%2F23%2Fryanair-fined-limit-online-travel-agencies-ticket-sales-ota&amp;amp;type=article&amp;amp;internalpagecode=business/2025/dec/23/ryanair-fined-limit-online-travel-agencies-ticket-sales-ota"&gt;Reuse this content&lt;/a&gt;&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://www.theguardian.com/business/2025/dec/23/ryanair-fined-limit-online-travel-agencies-ticket-sales-ota</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 10:53:07 +0000</pubDate>
    </item>
    <item>
      <title>Font with Built-In Syntax Highlighting (2024)</title>
      <link>https://blog.glyphdrawing.club/font-with-built-in-syntax-highlighting/</link>
      <description>I have been trying to identify practical reasons why hand-coding websites with HTML and CSS is so hard ( by hand-coding, I mean not relying on frameworks, generators or 3rd party scripts that modify the DOM ).</description>
      <content:encoded>&lt;article class="container"&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;a href="https://blog.glyphdrawing.club/tags/Blog"&gt;#Blog&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://blog.glyphdrawing.club/tags/Type Design"&gt;#Type Design&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://blog.glyphdrawing.club/tags/Fonts"&gt;#Fonts&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://blog.glyphdrawing.club/tags/CSS"&gt;#CSS&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://blog.glyphdrawing.club/tags/Typography"&gt;#Typography&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Syntax Highlighting in Hand-Coded Websites&lt;/h2&gt;
&lt;h3&gt;The problem&lt;/h3&gt;
&lt;p&gt;I have been trying to identify practical reasons why hand-coding websites with HTML and CSS is so hard (&lt;em&gt;by hand-coding, I mean not relying on frameworks, generators or 3rd party scripts that modify the DOM&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Let's say, I want to make a blog. What are the &lt;strong&gt;actual&lt;/strong&gt; things that prevent me from making—and maintaining—it by hand? What would it take to clear these roadblocks?&lt;/p&gt;
&lt;p&gt;There are many, of course, but for a hand-coded programming oriented blog one of these roadblocks is &lt;strong&gt;syntax highlighting&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;When I display snippets of code, I want to make the code easy to read and understand by highlighting it with colors. To do that, I would normally need to use a complex syntax highlighter library, like &lt;a href="https://prismjs.com/"&gt;Prism&lt;/a&gt; or &lt;a href="https://highlightjs.org/"&gt;highlight.js&lt;/a&gt;. These scripts work by scanning and chopping up the code into small language-specific patterns, then wrapping each part in tags with special styling that creates the highlighted effect, and then injecting the resulting HTML back into the page.&lt;/p&gt;
&lt;p&gt;But, I want to write code by hand. I don't want any external scripts to inject things I didn't write myself. Syntax highlighters also add to the overall complexity and bloat of each page, which I'm trying to avoid. I want to keep things as simple as possible.&lt;/p&gt;
&lt;h3&gt;Leveraging OpenType features to build a simple syntax highlighter inside the font&lt;/h3&gt;
&lt;p&gt;This lead me to think: &lt;strong&gt;could it be possible to build syntax highlighting directly into a font&lt;/strong&gt;, skipping JavaScript altogether? Could I somehow leverage OpenType features, by creating colored glyphs with the COLR table, and identifying and substituting code syntax with contextual alternates?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;div class="spoilers"&amp;gt;
  &amp;lt;strong&amp;gt;Yes, it's possible!&amp;lt;/strong&amp;gt;
  &amp;lt;small&amp;gt;...to some extent =)&amp;lt;/small&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The colors in the HTML snippet above &lt;strong&gt;comes from within the font itself&lt;/strong&gt;, the code is &lt;strong&gt;plain text&lt;/strong&gt;, and requires &lt;strong&gt;no JavaScript&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To achieve that, I modified an open source font Monaspace Krypton to include colored versions of each character, and then used OpenType contextual alternates to essentially find &amp;amp; replace specific strings of text based on HTML, CSS and JS syntax. The result is a simple syntax highlighter, &lt;strong&gt;built-in&lt;/strong&gt; to the font itself.&lt;/p&gt;
&lt;p&gt;If you want to try it yourself, download the font: &lt;a href="https://blog.glyphdrawing.club/assets/fonts/FontWithASyntaxHighlighter-Regular.woff2"&gt;FontWithASyntaxHighlighter-Regular.woff2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And include the following bits of CSS:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@font-face {
  font-family: 'FontWithASyntaxHighlighter';
  src: 
    url('/FontWithASyntaxHighlighter-Regular.woff2') 
    format('woff2')
  ;
}
code {
  font-family: "FontWithASyntaxHighlighter", monospace;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that's it!&lt;/p&gt;
&lt;h2&gt;What are the Pros and Cons of this method?&lt;/h2&gt;
&lt;p&gt;This method opens up some interesting possibilities...&lt;/p&gt;
&lt;h3&gt;Pros&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Install is as easy as using any custom font.&lt;/li&gt;
&lt;li&gt;Works without JavaScript.&lt;/li&gt;
&lt;li&gt;Works without CSS themes.&lt;/li&gt;
&lt;li&gt;...but can be themed with CSS.&lt;/li&gt;
&lt;li&gt;It's fast.&lt;/li&gt;
&lt;li&gt;Snippets of code can be put into &lt;code&gt;&amp;lt;pre&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;code&amp;gt;&lt;/code&gt;, with no extra classes or &lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt;s.&lt;/li&gt;
&lt;li&gt;Clean HTML source code.&lt;/li&gt;
&lt;li&gt;Works everywhere that supports OpenType features, like InDesign.&lt;/li&gt;
&lt;li&gt;Doesn't require maintenance or updating.&lt;/li&gt;
&lt;li&gt;Works in &lt;code&gt;&amp;lt;textarea&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt;! Syntax highlighting inside &lt;code&gt;&amp;lt;textarea&amp;gt;&lt;/code&gt; has been &lt;a href="https://css-tricks.com/creating-an-editable-textarea-that-supports-syntax-highlighted-code/"&gt;previously impossible&lt;/a&gt;, because textareas and inputs can only contain plain text. This is where the interesting possibilities lie. As a demo, I made this tiny HTML, CSS &amp;amp; JS sandbox, with native undo and redo, in a single, &lt;a href="https://blog.glyphdrawing.club/assets/webcomponents/tinybox.js"&gt;web component&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;




&lt;!-- Edit the content! --&gt;
&lt;p&gt;
          tiny HTML &amp;amp; CSS sandbox =)
        &lt;/p&gt;




&lt;h3&gt;Cons&lt;/h3&gt;
&lt;p&gt;There are, of course, some limitations to this method. It is not a direct replacement to the more robust syntax highligting libraries, but works well enough for simple needs.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Making modifications to the syntax highligher, like adding more language supports or changing the look of the font, requires modifying the font file. This requires some knowledge of font production, which most people don't have.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It only works where OpenType is supported. Fortunately, that's all major browsers and most modern programs. However, something like PowerPoint doesn't support OpenType.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finding patterns in text with OpenType contextual alternates is quite basic, and is no match for scripts that use regular expressions. For example, words within &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; tags that are JS keywords will be always highlighted: &lt;code&gt;&amp;lt;p&amp;gt;if I throw this Object through the window, catch it, for else it’ll continue to Infinity &amp;amp; break&amp;lt;/p&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multiline highlighting with manual line breaks will sadly not work.&lt;/p&gt;
&lt;p&gt;This is common, for example, in comment blocks and template literals:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; &amp;lt;!-- This line gets highlighted...
 but not this, because I made a manual line break...
 --&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;How does it actually work?&lt;/h2&gt;
&lt;p&gt;Here's roughly how it works. There are two features in OpenType that make this possible: OpenType COLR table and contextual alternates.&lt;/p&gt;
&lt;h3&gt;OpenType COLR table&lt;/h3&gt;
&lt;p&gt;OpenType COLR table makes multi-colored fonts possible. &lt;a href="https://glyphsapp.com/learn/creating-a-microsoft-color-font"&gt;There is a good guide on creating a color font using Glyphs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I made a palette with 8 colors.&lt;/p&gt;
&lt;p&gt;I duplicated letters &lt;code&gt;A&lt;/code&gt; &lt;code&gt;–&lt;/code&gt; &lt;code&gt;Z&lt;/code&gt;, numbers &lt;code&gt;0&lt;/code&gt; &lt;code&gt;–&lt;/code&gt; &lt;code&gt;9&lt;/code&gt; and the characters &lt;code&gt;.&lt;/code&gt; &lt;code&gt;#&lt;/code&gt; &lt;code&gt;*&lt;/code&gt; &lt;code&gt;-&lt;/code&gt; and &lt;code&gt;_&lt;/code&gt; four times. Each duplicated character is then suffixed with .alt, .alt2, .alt3 or .alt4, and then assigned a color from the palette. For example, all .alt1 glyphs are &lt;code&gt;this&lt;/code&gt; color.&lt;/p&gt;
&lt;p&gt;I also duplicated all characters twice, and gave them suffices .alt1 and .alt5 and assigned them colors used in &lt;code&gt;&amp;lt;!-- comment blocks --&amp;gt;&lt;/code&gt; and &lt;code&gt;"strings within quotes"&lt;/code&gt;&lt;/p&gt;

&lt;img src="https://blog.glyphdrawing.club/assets/kmFZTjkTcx-320.jpeg"/&gt;
View from Glyps app. Each alternate character has a different color.

&lt;p&gt;The two other colors I used for symbols &lt;code&gt;&amp;amp; | $ + − = ~ [] () {} / ; : " @ %&lt;/code&gt; and &lt;code&gt;'&lt;/code&gt;, and they are always in one color. Numbers &lt;code&gt;0 1 2 3 4 5 6 7 8 9&lt;/code&gt; are also always a certain color, unless overriden by other rules.&lt;/p&gt;
&lt;h3&gt;OpenType contextual alternates&lt;/h3&gt;
&lt;p&gt;The second required feature is OpenType contextual alternates. &lt;a href="https://glyphsapp.com/learn/features-part-3-advanced-contextual-alternates"&gt;Here's a great introductory guide to advanced contextual alternates for Glyphs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Contextual alternates makes characters "aware" of their adjacent characters. An example would be fonts that emulate continuous hand writing, where &lt;em&gt;how&lt;/em&gt; a letter connects depends on which letter it connects to. There is a &lt;a href="https://ilovetypography.com/2011/04/01/engaging-contextuality/"&gt;nice article covering possible uses here&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;JavaScript syntax rules&lt;/h4&gt;
&lt;p&gt;The core feature of contextual alternates is substituting glyphs. Here is a simplified code for finding the JavaScript keyword &lt;code&gt;if&lt;/code&gt; and substituting the letters i and f with their colored variant:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sub i' f by i.alt2;
sub i.alt2 f' by f.alt2;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In English:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;When i is followed by f, substitute the default i with an alternate (i.alt2).&lt;/li&gt;
&lt;li&gt;When i.alt2 is followed by f, substitute the default f with an alternate (f.alt2).&lt;/li&gt;
&lt;li&gt;As a result, every "if" in text gets substituted with &lt;code&gt;if&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;OpenType doesn't support many-to-many substitutions directly, but &lt;a href="https://typo.social/@behdad/112967180363218632"&gt;@behdad&lt;/a&gt; on Mastodon had a great suggestion: keywords could be elegantly colored by &lt;em&gt;chaining&lt;/em&gt; contextual substitutions.&lt;/p&gt;
&lt;p&gt;To do this, I made a lookup which substitutes each letter with its colored variant.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lookup ALT_SUBS {
    sub a by a.alt; 
    sub b by b.alt; 
    sub c by c.alt; 
    [etc.]
    sub Y by Y.alt;
    sub Z by Z.alt;
} ALT_SUBS;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I moved this lookup rule to the &lt;a href="https://handbook.glyphsapp.com/layout/standalone-lookups/"&gt;Prefix&lt;/a&gt; section, which just means it doesn't get applied automatically unlike the other lookups.&lt;/p&gt;
&lt;p&gt;Then, I made a lookup rule for each keyword in the contextual alternates section. Here's one for &lt;code&gt;console&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lookup console {
    ignore sub @AllLetters c' o' n' s' o' l' e';
    ignore sub c' o' n' s' o' l' e' @AllLetters;
    sub c' lookup ALT_SUBS
        o' lookup ALT_SUBS
        n' lookup ALT_SUBS
        s' lookup ALT_SUBS
        o' lookup ALT_SUBS
        l' lookup ALT_SUBS
        e' lookup ALT_SUBS;
} console;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First two lines tells it to ignore strings like &lt;code&gt;Xconsole&lt;/code&gt; or &lt;code&gt;consoles&lt;/code&gt;, but not if there's a period like &lt;code&gt;console.log()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The third line starts by replacing the first letter 'c' with its colored variant &lt;code&gt;c&lt;/code&gt;, by using definitions from the other lookup table "ALT_SUBS". This repeats until each letter is replaced by its color variant, and the result is &lt;code&gt;console&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Identifying JavaScript keywords is fairly straightforward. Logic is the same for each keyword, and I used a python script to generate them.&lt;/p&gt;
&lt;h4&gt;HTML &amp;amp; CSS syntax rules&lt;/h4&gt;
&lt;p&gt;But for HTML and CSS... I had to get a bit more creative. There are simply too many keywords for both HTML and CSS combined. Making a separate rule for each keyword would inflate the file size.&lt;/p&gt;
&lt;p&gt;Instead, I came up with this monstrosity. Here's how I find CSS value functions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lookup CssParamCalt useExtension {
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' parenleft by @CssParamAlt4;
} CssParamCalt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;@CssParam is a custom OpenType glyph class I've set up. It includes the characters &lt;code&gt;A&lt;/code&gt; &lt;code&gt;–&lt;/code&gt; &lt;code&gt;Z&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt; &lt;code&gt;–&lt;/code&gt; &lt;code&gt;z&lt;/code&gt;, and &lt;code&gt;-&lt;/code&gt;, which are all the possible characters used in CSS value function names. Because the longest possible CSS value function name is &lt;code&gt;repeating-linear-gradient()&lt;/code&gt;, with 25 letters, the first line of the lookup starts with @CssParam repeated 25 times, followed by parenleft (&lt;code&gt;(&lt;/code&gt;). This lookup will match any word up to 25 letters long, if it's immediately followed by an opening parenthesis. When a match occurs, it substitutes the matched text with its alternate color form (@CssParamAlt4).&lt;/p&gt;
&lt;p&gt;This lookup works for both CSS and JavaScript. It will colorize standard CSS functions like &lt;code&gt;rgb()&lt;/code&gt; as well as custom JavaScript functions like &lt;code&gt;myFunction()&lt;/code&gt;. The result is a semi-flexible syntax highlighter that doesn't require complex parsing. The downside is that if you have a really long function name, it stops working midway: &lt;code&gt;aReallyLongFunctionNameStopsWorkingMidway()&lt;/code&gt;. I've repeated the same principle for finding HTML tags and attributes, and for CSS selectors and parameters.&lt;/p&gt;
&lt;h4&gt;Unknown length rules&lt;/h4&gt;
&lt;p&gt;Comment blocks and strings between quotes also required extra care, because their length can be anything. OpenType doesn't support loops or anything resembling regular expressions. For example, I can't just tell it to simply substitute everything it finds between two quotes.&lt;/p&gt;
&lt;p&gt;However, I got a great suggestion from @penteract on &lt;a href="https://news.ycombinator.com/item?id=41259124"&gt;&lt;em&gt;Hacker News&lt;/em&gt;&lt;/a&gt; to use a finite state machine for these kinds of situations. Here our aim is to colorize eveything between /* and */ gray:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lookup CSScomment useExtension {
  // stop if we encounter a colored */
  ignore sub asterisk.alt1 slash.alt1 @All';

  // color first letter after /*
  sub slash asterisk @All' by @AllAlt1;
  sub slash asterisk space @All' by @AllAlt1;
  
  // color /* itself
  sub slash' asterisk by slash.alt1;
  sub slash.alt1 asterisk' by asterisk.alt1;
  
  // finite state machine to color rest of the characters
  // or until ignore condition is met
  sub @AllAlt1 @All' by @AllAlt1;
} CSScomment;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last line is the important one. The lookup will just continue replacing characters if the previous character is already colored.&lt;/p&gt;
&lt;h3&gt;End note&lt;/h3&gt;
&lt;p&gt;The full process is a little bit too convoluted to go into step-by-step, but if you're a type designer who wants to do this with their own font, don't hesitate to contact me.&lt;/p&gt;
&lt;p&gt;I'm also not an OpenType expert, so I'm sure the substitution logics could be improved upon. If you're interested in learning more about OpenType, I recommend reading &lt;a href="https://opentypecookbook.com/"&gt;The OpenType Cookbook&lt;/a&gt; and the complete &lt;a href="https://adobe-type-tools.github.io/afdko/OpenTypeFeatureFileSpecification.html"&gt;OpenType™ Feature File Specification&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you have any ideas, suggestions or feedback, let me know. You can reach me at &lt;code&gt;hlotvonen@gmail.com&lt;/code&gt; or leave a comment on &lt;a href="https://typo.social/@gdc/112959308500800771"&gt;Mastodon&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Changing the color theme&lt;/h2&gt;
&lt;p&gt;You can change the color theme with CSS &lt;a href="https://developer.mozilla.org/en-US/docs/Web/CSS/@font-palette-values/override-colors"&gt;&lt;code&gt;override-colors&lt;/code&gt;&lt;/a&gt;! &lt;a href="https://caniuse.com/?search=font-palette"&gt;Browser support&lt;/a&gt; is great at ~94%.&lt;/p&gt;




&lt;!-- Edit the content! --&gt;
&lt;pre&gt;&lt;code&gt;
        var const let for while
        function() linear-gradient()
        .myDiv{ background-color: pink; }
        console.log("hello", true)
        /* comment */
        &amp;amp; | $ + − = ~ [] () {} / ; : " @ % 
        0 1 2 3 4 5 6 7 8 9
      &lt;/code&gt;&lt;/pre&gt;



&lt;h2&gt;Alternative built-in color themes&lt;/h2&gt;
&lt;p&gt;Additionally, two alternative color themes &lt;em&gt;Night Owl&lt;/em&gt;, and &lt;em&gt;Light Owl&lt;/em&gt; were added by &lt;a href="https://typo.social/@niutech@fosstodon.org"&gt;niutech&lt;/a&gt;. You can download them from the &lt;a href="https://github.com/hlotvonen/FontWithASyntaxHighlighter"&gt;FontWithASyntaxHighlighter GitHub page&lt;/a&gt;. &lt;a href="https://github.com/sdras/night-owl-vscode-theme"&gt;Night Owl theme&lt;/a&gt; is made by &lt;a href="https://github.com/sdras"&gt;Sarah Drasner&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In order to modify the built-in color palette, you have to edit the font source file. To do so, you can edit the color palettes values in lines 112-120 of the &lt;a href="https://github.com/hlotvonen/FontWithASyntaxHighlighter/blob/main/FontWithASyntaxHighlighter.glyphs"&gt;FontWithASyntaxHighlighter.glyphs&lt;/a&gt; file and then build the font with &lt;a href="https://github.com/googlefonts/fontmake"&gt;fontmake&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Projects using this font&lt;/h2&gt;
&lt;p&gt;Here's some cool projects that are using or are inspired by this font:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.holograph.so/"&gt;Holograph is a visual coding tool built on tldraw&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/dennishansen/holograph"&gt;its GitHub page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://maxbo.me/celine/"&gt;@celine/celine is library for building reactive HTML notebooks&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/MaxwellBo/celine"&gt;its GitHub page&lt;/a&gt; &amp;amp; &lt;a href="https://maxbo.me/a-html-file-is-all-you-need.html"&gt;blogpost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://chenglou.me/pure-css-shaders-art/"&gt;Shaders art made with pure CSS&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/chenglou/pure-css-shaders-art"&gt;its GitHub Page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mdit.pages.dev/"&gt;Mdit, a simple Markdown previewer&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/roblesdotdev/mdit"&gt;its GitHub Page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/JRJurman/textarea-code-block"&gt;Web Component for making a Textarea element into a syntax highlighted codeblock&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;It might also be used as an example for displaying the potential uses for color fonts in the W3C &lt;a href="https://github.com/w3c/csswg-drafts/tree/main/css-fonts-4"&gt;CSS Fonts Module Level 4 specification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://codepen.io/daviddarnes/pen/poXpaLB?editors=1100"&gt; Web Component with syntax highlighting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tug.org/tug2025/preprints/rajeesh-colorfont-syntax.pdf"&gt;An OpenType font with built-in TEX syntax highlighting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://labelary.com/viewer.html"&gt;Labelary ZPL viewer &amp;amp; editor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://garten.salat.dev/"&gt;garten.salat.dev blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ffoodd.fr/devfest.2024/jeu/"&gt;L’invasion du HTML mutant&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;(Did you make a project using this font, or know a project that uses it? Let me know please!)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Potential future&lt;/h2&gt;
&lt;p&gt;Many people suggested that this concept could be taken one step further with &lt;a href="https://github.com/harfbuzz/harfbuzz-wasm-examples"&gt;harfbuzz-wasm&lt;/a&gt;. With harfbuzz-wasm a real parser could be used instead of my crazy opentype lookup rules. Essentially, all the cons could be eliminated... Any harfbuzz-wasm experts who wants to take this on?&lt;/p&gt;
&lt;h2&gt;Licence&lt;/h2&gt;
&lt;p&gt;The original font (&lt;a href="https://monaspace.githubnext.com/"&gt;MonaSpace&lt;/a&gt;) has &lt;a href="https://github.com/githubnext/monaspace/blob/main/LICENSE"&gt;SIL open font license v1.1&lt;/a&gt;, which carries over to my modified version. So, you're free to use the font in any way that the SIL v1.1 license permits.&lt;/p&gt;
&lt;p&gt;As for the code examples, they are MIT licensed. The tiny sandbox web component can be found here: &lt;a href="https://github.com/hlotvonen/tinybox"&gt;https://github.com/hlotvonen/tinybox&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Source&lt;/h2&gt;
&lt;p&gt;The original source .glyphs file is &lt;a href="https://github.com/hlotvonen/FontWithASyntaxHighlighter"&gt;hosted in this GitHub repository&lt;/a&gt;. UFO files were kindly added by &lt;a href="https://typo.social/@niutech@fosstodon.org"&gt;niutech&lt;/a&gt;. Or, you can modify the font with &lt;a href="https://forum.glyphsapp.com/t/script-outside-glyphapp/22454"&gt;some scripting&lt;/a&gt; &amp;amp; build with &lt;a href="https://github.com/googlefonts/fontmake"&gt;fontmake&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;More examples&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;as, in, of, if, for, while, finally, var, new, function,
do, return, void, else, break, catch, instanceof, with,
throw, case, default, try, switch, continue, typeof, delete,
let, yield, const, class, get, set, debugger, async, await,
static, import, from, export, extends

true, false, null, undefined, NaN, Infinity

Object, Function, Boolean, Symbol, Math, Date, Number, BigInt, 
String, RegExp, Array, Float32Array, Float64Array, Int8Array, 
Uint8Array, Uint8ClampedArray, Int16Array, Int32Array, Uint16Array, 
Uint32Array, BigInt64Array, BigUint64Array, Set, Map, WeakSet,
WeakMap, ArrayBuffer, SharedArrayBuffer, Atomics, DataView, 
JSON, Promise, Generator, GeneratorFunction, AsyncFunction, 
Reflect, Proxy, Intl, WebAssembly, Error, EvalError, InternalError, 
RangeError, ReferenceError, SyntaxError, TypeError, URIError, 
setInterval, setTimeout, clearInterval, clearTimeout, require, 
exports, eval, isFinite, isNaN, parseFloat, parseInt, decodeURI, 
decodeURIComponent, encodeURI, encodeURIComponent, escape, 
unescape, arguments, this, super, console, window, document, 
localStorage, sessionStorage, module, global
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;!-- this is a comment! --&amp;gt;
/* and this */
// and this
&amp;lt;!-- however...
it breaks when your code goes to a newline.
there's no way to keep context line to line...
--&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;!-- can't disable highlighting JS keywords in between tags --&amp;gt;
&amp;lt;p&amp;gt;
  give me a break...
&amp;lt;/p&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang="en"&amp;gt;
&amp;lt;head&amp;gt;
  &amp;lt;meta charset="UTF-8"&amp;gt;
  &amp;lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&amp;gt;
  &amp;lt;title&amp;gt;Syntax Highlighter Example&amp;lt;/title&amp;gt;
  &amp;lt;style&amp;gt;
    body {
      background-color: rgb(255, 0, 0);
      font-family: 'Arial Narrow', sans-serif;
      line-height: 1.44;
      color: #333;
    }
  &amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
  &amp;lt;header&amp;gt;
    &amp;lt;h1&amp;gt;Welcome to the Syntax Highlighter Test&amp;lt;/h1&amp;gt;
  &amp;lt;/header&amp;gt;
  &amp;lt;nav&amp;gt;
    &amp;lt;ul&amp;gt;
      &amp;lt;li&amp;gt;&amp;lt;a href="#section1"&amp;gt;Section 1&amp;lt;/a&amp;gt;
    &amp;lt;/ul&amp;gt;
  &amp;lt;/nav&amp;gt;
  &amp;lt;main&amp;gt;
    &amp;lt;section id="section1"&amp;gt;
      &amp;lt;h2&amp;gt;Section 1&amp;lt;/h2&amp;gt;
      &amp;lt;p&amp;gt;This is a &amp;lt;span class="highlight"&amp;gt;highlighted&amp;lt;/span&amp;gt; paragraph.&amp;lt;/p&amp;gt;
      &amp;lt;img src="/api/placeholder/300/200" alt="Placeholder image"&amp;gt;
    &amp;lt;/section&amp;gt;
  &amp;lt;/main&amp;gt;
  &amp;lt;script&amp;gt;
    console.log("This is a JavaScript comment");
    function greet(name) {
      return `Hello, ${name}!`;
    }
    document.addEventListener('DOMContentLoaded', () =&amp;gt; {
      console.log(greet('Syntax Highlighter'));
    });
  &amp;lt;/script&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;.crazyBackground {
  /* don't try this at home */
  background:
    radial-gradient(
      100% 50% at 50% 50%,
      hsl(90 90% 45%) 0% 5%,
      hsl(250 70% 40%) 50%,
      hsl(50 50% 50%)
    ),
    radial-gradient(
      100% 100% at 50% 25%,
      hsl(90 40% 85%) 30%,
      hsl(40 80% 20%) 60% 90%,
      transparent
    ),
    linear-gradient(
      90deg,
      hsl(150 90% 90%) 0 10%,
      hsl(10 10% 20%),
      hsl(150 90% 90%) 90% 100%
    )
  ;
  background-size:
    5% 10%,
    10% 200%,
    25% 100%
  ;
  background-blend-mode:
    color-dodge,
    difference,
    normal
  ;
  animation: fire2 60s linear infinite;
}

@keyframes fire2 {
  from {
    background-position: 0% 0%, 0 30%, 0 0;
  }

  to {
    background-position: 0% -100%, -100% 30%, 200% 0%;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;// Variables and constants
let variable = 'Hello';
const CONSTANT = 42;

// Template literals
const name = 'World';
console.log(`${variable}, ${name}!`);

// Function declaration
function greet(name) {
  return `Hello, ${name}!`;
}

// Arrow function
const multiply = (a, b) =&amp;gt; a * b;

// Class definition
class Person {
  constructor(name, age) {
    this.name = name;
    this.age = age;
  }
  sayHello() {
    console.log(`Hello, my name is ${this.name}`);
  }
}

// Object literal
const config = {
  apiKey: 'abc123',
  maxRetries: 3,
  timeout: 5000
};

// Array methods
const numbers = [1, 2, 3, 4, 5];
const doubled = numbers.map(num =&amp;gt; num * 2);
const sum = numbers.reduce((acc, curr) =&amp;gt; acc + curr, 0);

// Async/await
async function fetchData(url) {
  try {
    const response = await fetch(url);
    const data = await response.json();
    return data;
  } catch (error) {
    console.error('Error fetching data:', error);
  }
}

// Destructuring
const { apiKey, maxRetries } = config;
const [first, second, ...rest] = numbers;

// Spread operator
const newArray = [...numbers, 6, 7, 8];

// Conditional (ternary) operator
const isAdult = age &amp;gt;= 18 ? 'Adult' : 'Minor';

// Switch statement
function getDayName(dayNumber) {
  switch (dayNumber) {
    case 0: return 'Sunday';
    case 1: return 'Monday';
    // ... other cases
    default: return 'Invalid day';
  }
}

// Regular expression
const emailRegex = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;

// Symbol
const uniqueKey = Symbol('description');

// Set and Map
const uniqueNumbers = new Set(numbers);
const userRoles = new Map([['admin', 'full'], ['user', 'limited']]);

// Promises
const promise = new Promise((resolve, reject) =&amp;gt; {
  setTimeout(() =&amp;gt; resolve('Done!'), 1000);
});

// Export statement
export { greet, Person };
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;I received a lot of great feedback from the discussions at &lt;a href="https://typo.social/@gdc/112959308500800771"&gt;Mastodon&lt;/a&gt; and &lt;a href="https://news.ycombinator.com/item?id=41245159"&gt;Hacker News&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Thanks to jfk13 on hn, and &lt;a href="https://typo.social/@kizu@front-end.social/112960336521542558"&gt;@pixelambacht&lt;/a&gt; on Mastodon for pointing out that 'calt' is turned on by default, and that 'colr' is not an opentype feature that needs to be "turned on".&lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href="https://news.ycombinator.com/item?id=41259124"&gt;penteract&lt;/a&gt; on hn and &lt;a href="https://typo.social/@behdad/112967180363218632"&gt;@behdad&lt;/a&gt; on Mastodon for suggesting better substitution rules.&lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href="https://typo.social/@kizu@front-end.social/112960336521542558"&gt;@kizu&lt;/a&gt; and &lt;a href="https://typo.social/@kizu@front-end.social/112960336521542558"&gt;@pixelambacht&lt;/a&gt; on Mastodon for suggesting color theming with &lt;code&gt;override-colors&lt;/code&gt; CSS rule.&lt;/p&gt;
&lt;p&gt;As said earlier, if you have any ideas, suggestions or feedback, let me know. You can reach me at &lt;code&gt;hlotvonen@gmail.com&lt;/code&gt; or leave a comment on &lt;a href="https://typo.social/@gdc/112959308500800771"&gt;Mastodon&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thanks to all who sent emails, messages and commented!&lt;/p&gt;

&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://blog.glyphdrawing.club/font-with-built-in-syntax-highlighting/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 10:28:09 +0000</pubDate>
    </item>
    <item>
      <title>Ask HN: What are the best engineering blogs with real-world depth?</title>
      <link>https://news.ycombinator.com/item?id=46363921</link>
      <description>Specifically interested in posts that:
1. Explain technical concepts clearly and concisely
2. Show real implementation details, trade-offs, and failures
3. Are well-structured and readable
4. Tie engineering decisions back to business or product outcomes Any standout blogs, posts, or platforms you regularly learn from?</description>
      <content:encoded>&lt;body&gt;&lt;a href="https://news.ycombinator.com"&gt;&lt;img src="https://news.ycombinator.com/y18.svg"/&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/news"&gt;Hacker News&lt;/a&gt;&lt;a href="https://news.ycombinator.com/newest"&gt;new&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/front"&gt;past&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/newcomments"&gt;comments&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/ask"&gt;ask&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/show"&gt;show&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/jobs"&gt;jobs&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/submit"&gt;submit&lt;/a&gt;&lt;a href="https://news.ycombinator.com/login?goto=item%3Fid%3D46363921"&gt;login&lt;/a&gt;&lt;a href="https://news.ycombinator.com/vote?id=46363921&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/item?id=46363921"&gt;Ask HN: What are the best engineering blogs with real-world depth?&lt;/a&gt;280 points by &lt;a href="https://news.ycombinator.com/user?id=nishilpatel"&gt;nishilpatel&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46363921"&gt;8 hours ago&lt;/a&gt;  | &lt;a href="https://news.ycombinator.com/hide?id=46363921&amp;amp;goto=item%3Fid%3D46363921"&gt;hide&lt;/a&gt; | &lt;a href="https://hn.algolia.com/?query=Ask%20HN%3A%20What%20are%20the%20best%20engineering%20blogs%20with%20real-world%20depth%3F&amp;amp;type=story&amp;amp;dateRange=all&amp;amp;sort=byDate&amp;amp;storyText=false&amp;amp;prefix&amp;amp;page=0"&gt;past&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/fave?id=46363921&amp;amp;auth=e95b7fed59a69774119812f886e8bd69edd1f2ec"&gt;favorite&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/item?id=46363921"&gt;87 comments&lt;/a&gt;I’m looking for examples of high-quality engineering blog posts—especially from tech company blogs, that go beyond surface-level explanations.&lt;p&gt;Specifically interested in posts that:
1. Explain technical concepts clearly and concisely
2. Show real implementation details, trade-offs, and failures
3. Are well-structured and readable
4. Tie engineering decisions back to business or product outcomes&lt;p&gt;Any standout blogs, posts, or platforms you regularly learn from?&lt;/p&gt;&lt;/p&gt;&lt;br/&gt;
&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364274&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=pella"&gt;pella&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364274"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
&amp;gt; especially from tech company blogs,&lt;p&gt;&lt;a href="https://engineering.fb.com/"&gt;https://engineering.fb.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://netflixtechblog.com/"&gt;https://netflixtechblog.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://stripe.com/blog/engineering"&gt;https://stripe.com/blog/engineering&lt;/a&gt;&lt;p&gt;&lt;a href="https://eng.uber.com"&gt;https://eng.uber.com&lt;/a&gt;&lt;p&gt;&lt;a href="https://engineering.linkedin.com/"&gt;https://engineering.linkedin.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://engineering.atspotify.com/"&gt;https://engineering.atspotify.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://tailscale.com/blog"&gt;https://tailscale.com/blog&lt;/a&gt;&lt;p&gt;&lt;a href="https://careersatdoordash.com/engineering-blog/"&gt;https://careersatdoordash.com/engineering-blog/&lt;/a&gt;&lt;p&gt;&lt;a href="https://dropbox.tech/"&gt;https://dropbox.tech/&lt;/a&gt;&lt;p&gt;--&lt;p&gt;Aggregators:( &lt;a href="https://engineering.fyi/"&gt;https://engineering.fyi/&lt;/a&gt; ; &lt;a href="https://diff.blog/"&gt;https://diff.blog/&lt;/a&gt; )&lt;p&gt;+ &lt;a href="https://hn.algolia.com/?query=engineering%20blog"&gt;https://hn.algolia.com/?query=engineering%20blog&lt;/a&gt;&lt;p&gt;---&lt;p&gt;create a public engineering-blog SKILL.md.
( ~ collect the writing patterns that work on HN )&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364274&amp;amp;goto=item%3Fid%3D46363921%2346364274"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364336&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=i_k"&gt;i_k&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364336"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;parent&lt;/a&gt; | &lt;a href="#46366609"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
I am quite surprised and a bit disappointed that almost none of them have RSS.&lt;p&gt;But thank you!&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364336&amp;amp;goto=item%3Fid%3D46363921%2346364336"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366119&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=reallydoubtful"&gt;reallydoubtful&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366119"&gt;3 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364336"&gt;parent&lt;/a&gt; | &lt;a href="#46364494"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Most of them have feeds.&lt;p&gt;* &lt;a href="https://engineering.fb.com/feed"&gt;https://engineering.fb.com/feed&lt;/a&gt;&lt;p&gt;* &lt;a href="https://netflixtechblog.com/feed"&gt;https://netflixtechblog.com/feed&lt;/a&gt;&lt;p&gt;* No feed for stripe&lt;p&gt;* &lt;a href="https://www.uber.com/en-GB/blog/london/engineering/rss/"&gt;https://www.uber.com/en-GB/blog/london/engineering/rss/&lt;/a&gt;&lt;p&gt;* No feed for LinkedIn&lt;p&gt;* &lt;a href="https://engineering.atspotify.com/feed"&gt;https://engineering.atspotify.com/feed&lt;/a&gt;&lt;p&gt;* &lt;a href="https://tailscale.com/blog/index.xml"&gt;https://tailscale.com/blog/index.xml&lt;/a&gt;&lt;p&gt;* &lt;a href="https://careersatdoordash.com/engineering-blog/feed"&gt;https://careersatdoordash.com/engineering-blog/feed&lt;/a&gt;&lt;p&gt;* &lt;a href="https://dropbox.tech/feed"&gt;https://dropbox.tech/feed&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366119&amp;amp;goto=item%3Fid%3D46363921%2346366119"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364494&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=petercooper"&gt;petercooper&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364494"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364336"&gt;parent&lt;/a&gt; | &lt;a href="#46366119"&gt;prev&lt;/a&gt; | &lt;a href="#46364511"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Not RSS exactly but this OPML has feeds for several hundred such blogs if you can filter down from there: &lt;a href="https://peterc.org/misc/engblogs.opml"&gt;https://peterc.org/misc/engblogs.opml&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364494&amp;amp;goto=item%3Fid%3D46363921%2346364494"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365958&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=phrotoma"&gt;phrotoma&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365958"&gt;3 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364494"&gt;parent&lt;/a&gt; | &lt;a href="#46364511"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Your website is a work of art. Bravo &amp;lt;3&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365958&amp;amp;goto=item%3Fid%3D46363921%2346365958"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364511&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=onion2k"&gt;onion2k&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364511"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364336"&gt;parent&lt;/a&gt; | &lt;a href="#46364494"&gt;prev&lt;/a&gt; | &lt;a href="#46364514"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Spotify and Tailscale do...&lt;p&gt;&lt;a href="https://engineering.atspotify.com/feed"&gt;https://engineering.atspotify.com/feed&lt;/a&gt;&lt;p&gt;&lt;a href="https://tailscale.com/blog/index.xml"&gt;https://tailscale.com/blog/index.xml&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364511&amp;amp;goto=item%3Fid%3D46363921%2346364511"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364514&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=embedding-shape"&gt;embedding-shape&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364514"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364336"&gt;parent&lt;/a&gt; | &lt;a href="#46364511"&gt;prev&lt;/a&gt; | &lt;a href="#46366609"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
&amp;gt; I am quite surprised and a bit disappointed that almost none of them have RSS.&lt;p&gt;I think it's on purpose. It is to signal that these (those without RSS) aren't really "engineering" blogs at all, they're marketing websites aimed to help with recruiting and making the organization seem "engineering-like".&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364514&amp;amp;goto=item%3Fid%3D46363921%2346364514"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364750&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=zbentley"&gt;zbentley&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364750"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364514"&gt;parent&lt;/a&gt; | &lt;a href="#46366609"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
What? That makes no sense. RSS is beloved and known among engineers. Marketers? Not so much.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364750&amp;amp;goto=item%3Fid%3D46363921%2346364750"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364756&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=embedding-shape"&gt;embedding-shape&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364756"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364750"&gt;parent&lt;/a&gt; | &lt;a href="#46366609"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Exactly, so if the blog doesn't have RSS, you know they're probably made from marketers with no input from engineering, otherwise they'd have RSS on the blogs.&lt;p&gt;Edit: Ah, noticed I made a without/with typo, fixed that, should make about 2% more sense now for the ones who the original meaning was unclear :)&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364756&amp;amp;goto=item%3Fid%3D46363921%2346364756"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364764&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=zbentley"&gt;zbentley&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364764"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364756"&gt;parent&lt;/a&gt; | &lt;a href="#46366609"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Oh, I read your post backwards (thought you said RSS == more likely fluff). My fault, sorry!&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364764&amp;amp;goto=item%3Fid%3D46363921%2346364764"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364858&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=embedding-shape"&gt;embedding-shape&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364858"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;root&lt;/a&gt; | &lt;a href="#46364764"&gt;parent&lt;/a&gt; | &lt;a href="#46366609"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
To be fair to you, my original comment did say:&lt;p&gt;&amp;gt; It is to signal that these (those with RSS) aren't really "engineering" blogs at all&lt;p&gt;So now when I corrected that with/without typo, it looks like your previous comment doesn't make sense, but it kind of did, at the time. Sorry about that and thanks for making me realize the typo!&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364858&amp;amp;goto=item%3Fid%3D46363921%2346364858"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366609&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=iancmceachern"&gt;iancmceachern&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366609"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46364274"&gt;prev&lt;/a&gt; | &lt;a href="#46367852"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
It's so interesting to me as a Mechanical Engineer and Hardware designer/architect how on HN "Engineering" almost always means "Software engineering" here.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366609&amp;amp;goto=item%3Fid%3D46363921%2346366609"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367920&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=sp4nner"&gt;sp4nner&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367920"&gt;3 minutes ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;parent&lt;/a&gt; | &lt;a href="#46367406"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Agreed, though I understand the YC bias. I'm in biotech and mostly follow HN just to see what the software people are interested in these days.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367920&amp;amp;goto=item%3Fid%3D46363921%2346367920"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367406&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=throwaway4PP"&gt;throwaway4PP&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367406"&gt;47 minutes ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;parent&lt;/a&gt; | &lt;a href="#46367920"&gt;prev&lt;/a&gt; | &lt;a href="#46367412"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
It is funny, almost as funny as an entire cadre of people with “engineer” in their title who've never had to draw a free body diagram, learn circuit analysis, understand the basics of thermodynamics, or the mechanics of materials.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367406&amp;amp;goto=item%3Fid%3D46363921%2346367406"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367501&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=p2detar"&gt;p2detar&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367501"&gt;38 minutes ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;root&lt;/a&gt; | &lt;a href="#46367406"&gt;parent&lt;/a&gt; | &lt;a href="#46367412"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
I hold a CS master degree from an Eastern European university and everything you listed was in our Bachelor degree program. It’s pretty funny because while studying material properties back then I always wondered how and when am I gonna use that. It kind of makes sense now that I think about it - some students preferred branching out to hardware.&lt;p&gt;edit: typo&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367501&amp;amp;goto=item%3Fid%3D46363921%2346367501"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367412&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=jupin"&gt;jupin&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367412"&gt;47 minutes ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;parent&lt;/a&gt; | &lt;a href="#46367406"&gt;prev&lt;/a&gt; | &lt;a href="#46366717"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
I thought the same. Check out this mechanical engineering channel - &lt;a href="https://youtu.be/8yUsDnBXo_g?si=CXzWV9D5OvHcCBm3"&gt;https://youtu.be/8yUsDnBXo_g?si=CXzWV9D5OvHcCBm3&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367412&amp;amp;goto=item%3Fid%3D46363921%2346367412"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366717&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=jvanderbot"&gt;jvanderbot&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366717"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;parent&lt;/a&gt; | &lt;a href="#46367412"&gt;prev&lt;/a&gt; | &lt;a href="#46367472"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
I would love more blogs on mechanical, hardware, and especially industrial engineering, but the demographics in those areas skew stereo-typically older and also likely less blog-oriented, right?&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366717&amp;amp;goto=item%3Fid%3D46363921%2346366717"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366933&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=georgeburdell"&gt;georgeburdell&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366933"&gt;1 hour ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;root&lt;/a&gt; | &lt;a href="#46366717"&gt;parent&lt;/a&gt; | &lt;a href="#46367560"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Blogs are almost 30 years old at this point, but yes, I do associate a nearly compulsive need to show off one's work in meticulously-crafted blog posts with younger people.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366933&amp;amp;goto=item%3Fid%3D46363921%2346366933"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367560&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=wheelinsupial"&gt;wheelinsupial&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367560"&gt;33 minutes ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;root&lt;/a&gt; | &lt;a href="#46366717"&gt;parent&lt;/a&gt; | &lt;a href="#46366933"&gt;prev&lt;/a&gt; | &lt;a href="#46367472"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Depending on what you're looking for in industrial engineering, there are a lot of blogs on lean manufacturing and the Toyota Production System. INFORMS, may be paywalled, also publishes a lot of pretty interesting articles on applications of operations research to industry.&lt;p&gt;In general, though, my very limited experience working in manufacturing was that much of the blog equivalents were covered in things like white papers from hardware manufacturers or articles in trade publications. We always had a bunch of magazines delivered each month and there were usually some interesting articles to review.&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367560&amp;amp;goto=item%3Fid%3D46363921%2346367560"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367472&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=tekno45"&gt;tekno45&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367472"&gt;40 minutes ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;parent&lt;/a&gt; | &lt;a href="#46366717"&gt;prev&lt;/a&gt; | &lt;a href="#46367852"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
people building physical things are probably too busy to blog about it lol&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367472&amp;amp;goto=item%3Fid%3D46363921%2346367472"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367852&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=Swizec"&gt;Swizec&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367852"&gt;9 minutes ago&lt;/a&gt;  | &lt;a href="#46366609"&gt;prev&lt;/a&gt; | &lt;a href="#46367782"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
While not exactly a blog, I've collected ~16 years of [startup] engineering lessons into a book and I think it came out fantastic. People are saying super nice things.&lt;p&gt;&lt;a href="https://scalingfastbook.com"&gt;https://scalingfastbook.com&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367852&amp;amp;goto=item%3Fid%3D46363921%2346367852"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367782&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=bodash"&gt;bodash&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367782"&gt;15 minutes ago&lt;/a&gt;  | &lt;a href="#46367852"&gt;prev&lt;/a&gt; | &lt;a href="#46364321"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
&amp;gt; &lt;a href="https://lessnews.dev"&gt;https://lessnews.dev&lt;/a&gt;&lt;p&gt;A while ago I felt this "information fatigue" due to the overwhelming updates from the typical news sources (reddit, twitter, even hn).&lt;p&gt;So I built a _slow_ webdev newsfeed aggregator that doesn't overwhelm you of constant updates, so you focus on reading the actual blog contents and enjoy other things.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367782&amp;amp;goto=item%3Fid%3D46363921%2346367782"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364321&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=xnorswap"&gt;xnorswap&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364321"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46367782"&gt;prev&lt;/a&gt; | &lt;a href="#46365543"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
You might be more interested in books than a blog.&lt;p&gt;For example: The Architecture of Open Source Applications&lt;p&gt;&lt;a href="https://aosabook.org/en/index.html"&gt;https://aosabook.org/en/index.html&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364321&amp;amp;goto=item%3Fid%3D46363921%2346364321"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364479&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=alhirzel"&gt;alhirzel&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364479"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364321"&gt;parent&lt;/a&gt; | &lt;a href="#46365543"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Such a great resource!&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364479&amp;amp;goto=item%3Fid%3D46363921%2346364479"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365543&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=simonw"&gt;simonw&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365543"&gt;4 hours ago&lt;/a&gt;  | &lt;a href="#46364321"&gt;prev&lt;/a&gt; | &lt;a href="#46365655"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
This post by Jay Kreps that introduced Kafka to the world remains one of my favorite pieces of engineering blog content of all time: &lt;a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying"&gt;https://engineering.linkedin.com/distributed-systems/log-wha...&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365543&amp;amp;goto=item%3Fid%3D46363921%2346365543"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365655&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=sateesh"&gt;sateesh&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365655"&gt;3 hours ago&lt;/a&gt;  | &lt;a href="#46365543"&gt;prev&lt;/a&gt; | &lt;a href="#46367429"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://jvns.ca/"&gt;https://jvns.ca/&lt;/a&gt;
Not a tech. company blog. Explains technical concepts clearly and top notch technical posts. Fits 1,2, 3 criteria of what you ask, though not the 4th one.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365655&amp;amp;goto=item%3Fid%3D46363921%2346365655"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367180&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=skywhopper"&gt;skywhopper&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367180"&gt;1 hour ago&lt;/a&gt;  | &lt;a href="#46365655"&gt;parent&lt;/a&gt; | &lt;a href="#46367429"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Yes! Julia is fantastic at explaining concepts, and creating ways to learn about them. She produces a great series of “zines” summarizing a bunch of technical topics, her blog archives are really fascinating, and she’s created really useful tools like Mess With DNS (&lt;a href="https://messwithdns.net"&gt;https://messwithdns.net&lt;/a&gt;) which gives you your own DNS subdomain and the means to update records so you can try things out in an easy, harmless way.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367180&amp;amp;goto=item%3Fid%3D46363921%2346367180"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367429&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=jupin"&gt;jupin&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367429"&gt;45 minutes ago&lt;/a&gt;  | &lt;a href="#46365655"&gt;prev&lt;/a&gt; | &lt;a href="#46367631"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
To balance all of the computer engineering blogs, check out this mechanical engineering channel: &lt;a href="https://youtu.be/8yUsDnBXo_g?si=CXzWV9D5OvHcCBm3"&gt;https://youtu.be/8yUsDnBXo_g?si=CXzWV9D5OvHcCBm3&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367429&amp;amp;goto=item%3Fid%3D46363921%2346367429"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367631&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=vishnuharidas"&gt;vishnuharidas&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367631"&gt;28 minutes ago&lt;/a&gt;  | &lt;a href="#46367429"&gt;prev&lt;/a&gt; | &lt;a href="#46366390"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://engineeringblogs.xyz/"&gt;https://engineeringblogs.xyz/&lt;/a&gt; is a good place listing more than 500 (and adding more) engineering blogs.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367631&amp;amp;goto=item%3Fid%3D46363921%2346367631"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366390&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=aranw"&gt;aranw&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366390"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46367631"&gt;prev&lt;/a&gt; | &lt;a href="#46364152"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://samwho.dev"&gt;https://samwho.dev&lt;/a&gt; has some fantastic blog posts with great visualisations&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366390&amp;amp;goto=item%3Fid%3D46363921%2346366390"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366733&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=samwho"&gt;samwho&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366733"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46366390"&gt;parent&lt;/a&gt; | &lt;a href="#46364152"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Thank you &amp;lt;3&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366733&amp;amp;goto=item%3Fid%3D46363921%2346366733"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364152&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=yrand"&gt;yrand&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364152"&gt;8 hours ago&lt;/a&gt;  | &lt;a href="#46366390"&gt;prev&lt;/a&gt; | &lt;a href="#46364405"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Encountered one specific example about a month ago here on HackerNews - All about automotive lidar.
&lt;a href="https://news.ycombinator.com/item?id=46110395"&gt;https://news.ycombinator.com/item?id=46110395&lt;/a&gt;&lt;p&gt;Blog posts where I find quality really shows are usually about something I know next to nothing about how it works. A badly written article usually either goes really shallow or skips some facts when going into depth and requires catchup elsewhere to actually understand it. The lidar article from Main Street Autonomy goes beyond basics and explained everything from the ground up in such a connected way that it was a real pleasure reading it.&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364152&amp;amp;goto=item%3Fid%3D46363921%2346364152"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364405&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=Okkef"&gt;Okkef&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364405"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364152"&gt;prev&lt;/a&gt; | &lt;a href="#46364159"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Armin Ronacher's blog (of flask/jinja fame) &lt;a href="https://lucumr.pocoo.org/"&gt;https://lucumr.pocoo.org/&lt;/a&gt;&lt;p&gt;Antirez' blog (of Redis fame) &lt;a href="https://antirez.com/"&gt;https://antirez.com/&lt;/a&gt;&lt;p&gt;Simon Willison's blog (about AI) &lt;a href="https://simonwillison.net/"&gt;https://simonwillison.net/&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364405&amp;amp;goto=item%3Fid%3D46363921%2346364405"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364159&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=qznc"&gt;qznc&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364159"&gt;8 hours ago&lt;/a&gt;  | &lt;a href="#46364405"&gt;prev&lt;/a&gt; | &lt;a href="#46366497"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Sounds like you look for an intersection of academic papers (1.), tech blogs (2.), text books (3.), and confidential business strategies (4.)? A very high ambition.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364159&amp;amp;goto=item%3Fid%3D46363921%2346364159"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367699&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=cess11"&gt;cess11&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367699"&gt;23 minutes ago&lt;/a&gt;  | &lt;a href="#46364159"&gt;parent&lt;/a&gt; | &lt;a href="#46364194"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Corporations commonly describe some of their internal processes and achievements because it builds reputation and that can be important for both sales and recruitment.&lt;p&gt;Sometimes they do it in the form of free or open source software releases.&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367699&amp;amp;goto=item%3Fid%3D46363921%2346367699"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364194&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=gchamonlive"&gt;gchamonlive&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364194"&gt;8 hours ago&lt;/a&gt;  | &lt;a href="#46364159"&gt;parent&lt;/a&gt; | &lt;a href="#46367699"&gt;prev&lt;/a&gt; | &lt;a href="#46366497"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
A very high ambition?&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364194&amp;amp;goto=item%3Fid%3D46363921%2346364194"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366497&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=nsm"&gt;nsm&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366497"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46364159"&gt;prev&lt;/a&gt; | &lt;a href="#46364190"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://randomascii.wordpress.com/"&gt;https://randomascii.wordpress.com/&lt;/a&gt; - former Chrome engineer about all things performance engineering and particularly focused on Windows.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366497&amp;amp;goto=item%3Fid%3D46363921%2346366497"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364190&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=nchmy"&gt;nchmy&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364190"&gt;8 hours ago&lt;/a&gt;  | &lt;a href="#46366497"&gt;prev&lt;/a&gt; | &lt;a href="#46364537"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
You're probably looking for something that is more focused on specific software decisions/implementations, but &lt;a href="https://infrequently.org"&gt;https://infrequently.org&lt;/a&gt; is the best web development blog out there.&lt;p&gt;It's not "technical" so much as it just educates you on how to be a good web developer/run a team. There's zero fluff and considerable detail (footnotes are practically blog posts themselves).&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364190&amp;amp;goto=item%3Fid%3D46363921%2346364190"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364537&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=bzGoRust"&gt;bzGoRust&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364537"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364190"&gt;prev&lt;/a&gt; | &lt;a href="#46366909"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://discord.com/blog"&gt;https://discord.com/blog&lt;/a&gt;
&lt;a href="https://blog.cloudflare.com/"&gt;https://blog.cloudflare.com/&lt;/a&gt;
&lt;a href="https://netflixtechblog.com/"&gt;https://netflixtechblog.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364537&amp;amp;goto=item%3Fid%3D46363921%2346364537"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366909&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=mitthrowaway2"&gt;mitthrowaway2&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366909"&gt;1 hour ago&lt;/a&gt;  | &lt;a href="#46364537"&gt;prev&lt;/a&gt; | &lt;a href="#46367369"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
I always enjoyed Jason Sachs' blog at embedded related.&lt;p&gt;&lt;a href="https://www.embeddedrelated.com/showarticle/152.php"&gt;https://www.embeddedrelated.com/showarticle/152.php&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366909&amp;amp;goto=item%3Fid%3D46363921%2346366909"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367369&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=mad44"&gt;mad44&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367369"&gt;50 minutes ago&lt;/a&gt;  | &lt;a href="#46366909"&gt;prev&lt;/a&gt; | &lt;a href="#46364206"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
MongoDB Engineering Blog is shaping up well&lt;p&gt;&lt;a href="https://www.mongodb.com/company/blog/channel/engineering-blog"&gt;https://www.mongodb.com/company/blog/channel/engineering-blo...&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367369&amp;amp;goto=item%3Fid%3D46363921%2346367369"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364206&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=tekichan"&gt;tekichan&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364206"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46367369"&gt;prev&lt;/a&gt; | &lt;a href="#46366325"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
&lt;a href="http://highscalability.squarespace.com/all-time-favorites/"&gt;http://highscalability.squarespace.com/all-time-favorites/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364206&amp;amp;goto=item%3Fid%3D46363921%2346364206"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366325&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=NickJLange"&gt;NickJLange&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366325"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46364206"&gt;prev&lt;/a&gt; | &lt;a href="#46364182"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
A lot of great links here to the firehose (or at least for working parents). Unless someone has built it - anything that aggregates and shows beyond the first click of the by-line. (i.e. a first paragraph, or LLM-summary of the content)?&lt;p&gt;Otherwise... coming soon from a vibe-coding session near you...&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366325&amp;amp;goto=item%3Fid%3D46363921%2346366325"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366840&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=soulofmischief"&gt;soulofmischief&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366840"&gt;1 hour ago&lt;/a&gt;  | &lt;a href="#46366325"&gt;parent&lt;/a&gt; | &lt;a href="#46366721"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
A friend and I worked on a startup together that did this back when only the GPT-3 API was available. Sucked up everything we could think of, including HN and traditionally opaque sources such as Telegram&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366840&amp;amp;goto=item%3Fid%3D46363921%2346366840"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366721&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=SleepySteve_sk"&gt;SleepySteve_sk&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366721"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46366325"&gt;parent&lt;/a&gt; | &lt;a href="#46366840"&gt;prev&lt;/a&gt; | &lt;a href="#46364182"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
We're currently building something to solve this problem.&lt;p&gt;&lt;a href="https://joinheader.com/"&gt;https://joinheader.com/&lt;/a&gt;&lt;p&gt;We'll filter an RSS feed based on the topic and description that you provide. Feel free to reach out to me at s.kufuor@&amp;lt;domain&amp;gt; if you have any questions or feedback.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366721&amp;amp;goto=item%3Fid%3D46363921%2346366721"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364182&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=tester756"&gt;tester756&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364182"&gt;8 hours ago&lt;/a&gt;  | &lt;a href="#46366325"&gt;prev&lt;/a&gt; | &lt;a href="#46366537"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Maybe&lt;p&gt;&lt;a href="https://projectzero.google/archive.html"&gt;https://projectzero.google/archive.html&lt;/a&gt;&lt;p&gt;&lt;a href="https://netflixtechblog.medium.com/"&gt;https://netflixtechblog.medium.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://www.uber.com/en-US/blog/engineering/"&gt;https://www.uber.com/en-US/blog/engineering/&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364182&amp;amp;goto=item%3Fid%3D46363921%2346364182"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366537&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=agumonkey"&gt;agumonkey&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366537"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46364182"&gt;prev&lt;/a&gt; | &lt;a href="#46364444"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Often enjoyed article by chris wellons &lt;a href="https://nullprogram.com/"&gt;https://nullprogram.com/&lt;/a&gt;&lt;p&gt;quite diverse, often challenging, sometimes mind bending&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366537&amp;amp;goto=item%3Fid%3D46363921%2346366537"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364444&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=noam_k"&gt;noam_k&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364444"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46366537"&gt;prev&lt;/a&gt; | &lt;a href="#46364885"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://lcamtuf.substack.com/"&gt;https://lcamtuf.substack.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364444&amp;amp;goto=item%3Fid%3D46363921%2346364444"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364885&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=avisk"&gt;avisk&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364885"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46364444"&gt;prev&lt;/a&gt; | &lt;a href="#46366959"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://technology.riotgames.com/"&gt;https://technology.riotgames.com/&lt;/a&gt;
&lt;a href="https://fabiensanglard.net/"&gt;https://fabiensanglard.net/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364885&amp;amp;goto=item%3Fid%3D46363921%2346364885"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366959&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=jonstewart"&gt;jonstewart&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366959"&gt;1 hour ago&lt;/a&gt;  | &lt;a href="#46364885"&gt;prev&lt;/a&gt; | &lt;a href="#46364625"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Not corporate, but two of the best individual developer blogs are Eli Bendersky's and Rachel by the Bay. They've both been blogging prolifically for a decade+, Eli with a focus on, broadly, compilers and Rachel on SRE/debugging.&lt;p&gt;Raymond Chen's The Old New Thing is also required reading for anyone that works with Windows.&lt;p&gt;&lt;a href="https://eli.thegreenplace.net/"&gt;https://eli.thegreenplace.net/&lt;/a&gt;&lt;p&gt;&lt;a href="https://rachelbythebay.com/w/"&gt;https://rachelbythebay.com/w/&lt;/a&gt;&lt;p&gt;&lt;a href="https://devblogs.microsoft.com/oldnewthing/"&gt;https://devblogs.microsoft.com/oldnewthing/&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366959&amp;amp;goto=item%3Fid%3D46363921%2346366959"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364625&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=thundergolfer"&gt;thundergolfer&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364625"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46366959"&gt;prev&lt;/a&gt; | &lt;a href="#46364681"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
- &lt;a href="https://modal.com/blog/vprox"&gt;https://modal.com/blog/vprox&lt;/a&gt;
- &lt;a href="https://modal.com/blog/host-overhead-inference-efficiency"&gt;https://modal.com/blog/host-overhead-inference-efficiency&lt;/a&gt;
- &lt;a href="https://modal.com/blog/resource-solver"&gt;https://modal.com/blog/resource-solver&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364625&amp;amp;goto=item%3Fid%3D46363921%2346364625"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364681&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=mkosmul"&gt;mkosmul&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364681"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364625"&gt;prev&lt;/a&gt; | &lt;a href="#46364335"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Allegro Tech Blog: &lt;a href="https://blog.allegro.tech/"&gt;https://blog.allegro.tech/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364681&amp;amp;goto=item%3Fid%3D46363921%2346364681"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364335&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=pveierland"&gt;pveierland&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364335"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364681"&gt;prev&lt;/a&gt; | &lt;a href="#46364395"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Tweag has many interesting entries with good technical depth:&lt;p&gt;&lt;a href="https://www.tweag.io/blog"&gt;https://www.tweag.io/blog&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364335&amp;amp;goto=item%3Fid%3D46363921%2346364335"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364395&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=ludicity"&gt;ludicity&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364395"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364335"&gt;prev&lt;/a&gt; | &lt;a href="#46364971"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
I'm a huge fan of &lt;a href="https://eblog.fly.dev/index.html"&gt;https://eblog.fly.dev/index.html&lt;/a&gt;. The author, Efron, very graciously advises me on a lot of little things around my engineering practice, and I've learned a huge amount about weird holes in my practice from industry dysfunction in a very short period of time from him.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364395&amp;amp;goto=item%3Fid%3D46363921%2346364395"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364971&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=alzamos"&gt;alzamos&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364971"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46364395"&gt;prev&lt;/a&gt; | &lt;a href="#46364815"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Francesco Mazzoli’s blog on &lt;a href="https://mazzo.li/archive.html"&gt;https://mazzo.li/archive.html&lt;/a&gt;. His blog has topped HN a few times with various low-level/linux topics, some deep dives into algorithms etc.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364971&amp;amp;goto=item%3Fid%3D46363921%2346364971"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364815&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=sevazhidkov"&gt;sevazhidkov&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364815"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364971"&gt;prev&lt;/a&gt; | &lt;a href="#46365128"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
It’s not a traditional blog, but Oxide’s RFDs cover exactly what you asked — implementation details and trade-offs: &lt;a href="https://rfd.shared.oxide.computer/"&gt;https://rfd.shared.oxide.computer/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364815&amp;amp;goto=item%3Fid%3D46363921%2346364815"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365128&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=sdairs"&gt;sdairs&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365128"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46364815"&gt;prev&lt;/a&gt; | &lt;a href="#46365215"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://clickhouse.com/blog?category=engineering"&gt;https://clickhouse.com/blog?category=engineering&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365128&amp;amp;goto=item%3Fid%3D46363921%2346365128"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365215&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=robofanatic"&gt;robofanatic&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365215"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46365128"&gt;prev&lt;/a&gt; | &lt;a href="#46364526"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://www.makingsoftware.com/"&gt;https://www.makingsoftware.com/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365215&amp;amp;goto=item%3Fid%3D46363921%2346365215"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364526&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=louiechristie"&gt;louiechristie&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364526"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46365215"&gt;prev&lt;/a&gt; | &lt;a href="#46364874"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://dora.dev/research/2025/dora-report/"&gt;https://dora.dev/research/2025/dora-report/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364526&amp;amp;goto=item%3Fid%3D46363921%2346364526"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364874&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=nickmonad"&gt;nickmonad&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364874"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46364526"&gt;prev&lt;/a&gt; | &lt;a href="#46365166"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
TigerBeetle: &lt;a href="https://tigerbeetle.com/blog/"&gt;https://tigerbeetle.com/blog/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364874&amp;amp;goto=item%3Fid%3D46363921%2346364874"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365166&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=GeoAtreides"&gt;GeoAtreides&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365166"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46364874"&gt;prev&lt;/a&gt; | &lt;a href="#46366569"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Seems to me you're describing books.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365166&amp;amp;goto=item%3Fid%3D46363921%2346365166"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366569&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=corbet"&gt;corbet&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366569"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46365166"&gt;prev&lt;/a&gt; | &lt;a href="#46364305"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
I feel obligated to mention LWN - &lt;a href="https://lwn.net/"&gt;https://lwn.net/&lt;/a&gt; - since that is exactly what we aspire to.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366569&amp;amp;goto=item%3Fid%3D46363921%2346366569"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364305&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=vogu66"&gt;vogu66&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364305"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46366569"&gt;prev&lt;/a&gt; | &lt;a href="#46364264"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
not software engineering, but &lt;a href="https://practical.engineering/"&gt;https://practical.engineering/&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364305&amp;amp;goto=item%3Fid%3D46363921%2346364305"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364264&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=Agingcoder"&gt;Agingcoder&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364264"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364305"&gt;prev&lt;/a&gt; | &lt;a href="#46365415"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Cloudflare, google project zero.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364264&amp;amp;goto=item%3Fid%3D46363921%2346364264"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365415&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=rramadass"&gt;rramadass&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365415"&gt;4 hours ago&lt;/a&gt;  | &lt;a href="#46364264"&gt;prev&lt;/a&gt; | &lt;a href="#46364525"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Not a blog, but books detailing real-world experiences from Indian Engineers/Scientists/Researchers; Quite inspiring to see how people strive unceasingly towards a goal in spite of all the limitations and hurdles (viz. Political/Financial/Material etc.) imposed on them.&lt;p&gt;There is much to learn, in these books.&lt;p&gt;The Mind of an Engineer by Purnendu Ghosh et al. - &lt;a href="https://link.springer.com/book/10.1007/978-981-10-0119-2"&gt;https://link.springer.com/book/10.1007/978-981-10-0119-2&lt;/a&gt;&lt;p&gt;The Mind of an Engineer: Volume 2 by Purnendu Ghosh et al. - &lt;a href="https://link.springer.com/book/10.1007/978-981-15-1330-5"&gt;https://link.springer.com/book/10.1007/978-981-15-1330-5&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365415&amp;amp;goto=item%3Fid%3D46363921%2346365415"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364525&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=louiechristie"&gt;louiechristie&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364525"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46365415"&gt;prev&lt;/a&gt; | &lt;a href="#46364307"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://youtube.com/@modernsoftwareengineeringyt"&gt;https://youtube.com/@modernsoftwareengineeringyt&lt;/a&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364525&amp;amp;goto=item%3Fid%3D46363921%2346364525"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364307&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=mitjam"&gt;mitjam&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364307"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364525"&gt;prev&lt;/a&gt; | &lt;a href="#46364917"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Maybe it's just because I'm LLMing a bit too much, recently, but this question sounds to me like a prompt.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364307&amp;amp;goto=item%3Fid%3D46363921%2346364307"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365433&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=x187463"&gt;x187463&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365433"&gt;4 hours ago&lt;/a&gt;  | &lt;a href="#46364307"&gt;parent&lt;/a&gt; | &lt;a href="#46364572"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Some people act like the use of an LLM immediately invalidates or lowers the value of a piece of content. But the case of a question or simple post, especially by somebody for whom English is second language, using an LLM to rephrase or clean-up some text seems like an innocent and practical use case for LLMs.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365433&amp;amp;goto=item%3Fid%3D46363921%2346365433"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364572&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=runlaszlorun"&gt;runlaszlorun&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364572"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364307"&gt;parent&lt;/a&gt; | &lt;a href="#46365433"&gt;prev&lt;/a&gt; | &lt;a href="#46364509"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
I'm not beating up on OP but I chuckled when I read the question. Literally the only place I see the phrase "no fluff" with any frequency is with Deepseek lol.&lt;p&gt;Nothing wrong with the phrase itself of course, other than the fact that it's like literally in every other reply for me lol.&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364572&amp;amp;goto=item%3Fid%3D46363921%2346364572"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364509&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=atoav"&gt;atoav&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364509"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364307"&gt;parent&lt;/a&gt; | &lt;a href="#46364572"&gt;prev&lt;/a&gt; | &lt;a href="#46364917"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Had the same thought. ChatGPT often tells me things like: "This is the hard truth" or "I am telling it to you as it is (no fluff)" or whatever. Just because my initial prompt contains a line about it not making things up and telling me how things are instead of what would please me to hear. I added a line to specifically tell it to not phrase out these things, but it appears to be surprisingly hard to get rid of those phrases.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364509&amp;amp;goto=item%3Fid%3D46363921%2346364509"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364917&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=snvzz"&gt;snvzz&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364917"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46364307"&gt;prev&lt;/a&gt; | &lt;a href="#46364271"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
For deeper understanding of seL4's developments and the historical context in which it appeared, Gernot Heiser's blog[0].&lt;p&gt;0. &lt;a href="https://microkerneldude.org/"&gt;https://microkerneldude.org/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364917&amp;amp;goto=item%3Fid%3D46363921%2346364917"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364271&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=throw_await"&gt;throw_await&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364271"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364917"&gt;prev&lt;/a&gt; | &lt;a href="#46364394"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
oldnewthing&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364271&amp;amp;goto=item%3Fid%3D46363921%2346364271"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364394&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=vibesareoff"&gt;vibesareoff&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364394"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364271"&gt;prev&lt;/a&gt; | &lt;a href="#46364716"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Ask the LLM you wrote this post with!&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364394&amp;amp;goto=item%3Fid%3D46363921%2346364394"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366786&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=voxleone"&gt;voxleone&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366786"&gt;1 hour ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;parent&lt;/a&gt; | &lt;a href="#46364684"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
No judgement here whatsoever, but i think LLM would be "the" tool for this job. I also wonder if there's any point to "Ask" sections in websites after LLM's.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366786&amp;amp;goto=item%3Fid%3D46363921%2346366786"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364684&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=bell-cot"&gt;bell-cot&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364684"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;parent&lt;/a&gt; | &lt;a href="#46366786"&gt;prev&lt;/a&gt; | &lt;a href="#46364467"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
OP is asking a good question.  There's no dishonor if he is not fluent in English, and used an LLM to translate.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364684&amp;amp;goto=item%3Fid%3D46363921%2346364684"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364808&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=vibesareoff"&gt;vibesareoff&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364808"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364684"&gt;parent&lt;/a&gt; | &lt;a href="#46364467"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
"OP" couldn't even be bothered to reformat the numbered list to run on separate fucking lines.&lt;p&gt;But sure, cheer on the homogenization of online spaces into beige slop staccato bullshit!&lt;p&gt;˙ ͜ʟ˙&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364808&amp;amp;goto=item%3Fid%3D46363921%2346364808"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46367743&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=CamperBob2"&gt;CamperBob2&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46367743"&gt;18 minutes ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364808"&gt;parent&lt;/a&gt; | &lt;a href="#46366648"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Other sites beckon.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46367743&amp;amp;goto=item%3Fid%3D46363921%2346367743"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46366648&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=loloquwowndueo"&gt;loloquwowndueo&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46366648"&gt;2 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364808"&gt;parent&lt;/a&gt; | &lt;a href="#46367743"&gt;prev&lt;/a&gt; | &lt;a href="#46365355"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
How do you reformat a list so it runs on separate fucking lines?&lt;p&gt;Always happens to me (and I don’t use fucking LLMs) so I’d really like to know.&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46366648&amp;amp;goto=item%3Fid%3D46363921%2346366648"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365355&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=fnordlord"&gt;fnordlord&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365355"&gt;4 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364808"&gt;parent&lt;/a&gt; | &lt;a href="#46366648"&gt;prev&lt;/a&gt; | &lt;a href="#46364934"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
I will always cheer on anyone who shares their curiosity.&lt;p&gt;It was a great question and now I have a ton of new things on my reading list.&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365355&amp;amp;goto=item%3Fid%3D46363921%2346365355"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364934&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=bell-cot"&gt;bell-cot&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364934"&gt;5 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364808"&gt;parent&lt;/a&gt; | &lt;a href="#46365355"&gt;prev&lt;/a&gt; | &lt;a href="#46364467"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
You seem to be picking metrics for their utility in angrily excluding people who you a priori despise.  :(&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364934&amp;amp;goto=item%3Fid%3D46363921%2346364934"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364467&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=sieste"&gt;sieste&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364467"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;parent&lt;/a&gt; | &lt;a href="#46364684"&gt;prev&lt;/a&gt; | &lt;a href="#46365859"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
The LLM instructed him to gather training data.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364467&amp;amp;goto=item%3Fid%3D46363921%2346364467"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364488&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=ozim"&gt;ozim&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364488"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364467"&gt;parent&lt;/a&gt; | &lt;a href="#46365859"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
So prompt injection on humans&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364488&amp;amp;goto=item%3Fid%3D46363921%2346364488"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364516&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=sieste"&gt;sieste&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364516"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364488"&gt;parent&lt;/a&gt; | &lt;a href="#46365859"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Polluting the internet with meat slop.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364516&amp;amp;goto=item%3Fid%3D46363921%2346364516"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364585&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=themafia"&gt;themafia&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364585"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;root&lt;/a&gt; | &lt;a href="#46364516"&gt;parent&lt;/a&gt; | &lt;a href="#46365859"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
"What if we used more energy and got worse results?"&lt;p&gt;Sort of makes you miss "move fast and break things."&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364585&amp;amp;goto=item%3Fid%3D46363921%2346364585"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46365859&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=asupkay"&gt;asupkay&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46365859"&gt;3 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;parent&lt;/a&gt; | &lt;a href="#46364467"&gt;prev&lt;/a&gt; | &lt;a href="#46364716"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
Maybe the LLM is the one asking&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46365859&amp;amp;goto=item%3Fid%3D46363921%2346365859"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364716&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=Joel_Mckay"&gt;Joel_Mckay&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364716"&gt;6 hours ago&lt;/a&gt;  | &lt;a href="#46364394"&gt;prev&lt;/a&gt; | &lt;a href="#46364243"&gt;next&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
These should be read at least once in your life if interested in building industrial grade electrical, mechanical, and or software.&lt;p&gt;1. &lt;a href="https://nepp.nasa.gov/whisker/"&gt;https://nepp.nasa.gov/whisker/&lt;/a&gt;&lt;p&gt;2. &lt;a href="https://standards.nasa.gov/standard/NASA/NASA-STD-87394"&gt;https://standards.nasa.gov/standard/NASA/NASA-STD-87394&lt;/a&gt;&lt;p&gt;3. &lt;a href="https://standards.nasa.gov/NASA-Technical-Standards"&gt;https://standards.nasa.gov/NASA-Technical-Standards&lt;/a&gt;&lt;p&gt;4. &lt;a href="https://sma.nasa.gov/sma-disciplines/workmanship"&gt;https://sma.nasa.gov/sma-disciplines/workmanship&lt;/a&gt;&lt;p&gt;5. &lt;a href="https://www.stroustrup.com/JSF-AV-rules.pdf"&gt;https://www.stroustrup.com/JSF-AV-rules.pdf&lt;/a&gt;&lt;p&gt;6. &lt;a href="https://en.wikipedia.org/wiki/The_Power_of_10:_Rules_for_Developing_Safety-Critical_Code"&gt;https://en.wikipedia.org/wiki/The_Power_of_10:_Rules_for_Dev...&lt;/a&gt;&lt;p&gt;7. &lt;a href="https://www.nist.gov/pml/owm/laboratory-metrology/metrology-training"&gt;https://www.nist.gov/pml/owm/laboratory-metrology/metrology-...&lt;/a&gt;&lt;p&gt;8. &lt;a href="https://www.mitutoyo.com/training-education/"&gt;https://www.mitutoyo.com/training-education/&lt;/a&gt;&lt;p&gt;9. "Memoirs of extraordinary popular delusions and the madness of crowds" (Charles Mackay, 1852, &lt;a href="https://www.gutenberg.org/files/24518/24518-h/24518-h.htm"&gt;https://www.gutenberg.org/files/24518/24518-h/24518-h.htm&lt;/a&gt; )&lt;p&gt;The artifacts are usually beautiful from good Workmanship Standards, Design For Manufacturability, and systematic Metrology.  Dragging us all into the future one project at a time.&lt;p&gt;Note that training an ML model with such data would be pointless, as statistical saliency forms a paradox with consumer product design compromises. Note, there are _always_ tradeoffs in every problem domain.&lt;p&gt;'What it actually means to be "AI Generated"' ( &lt;a href="https://www.youtube.com/watch?v=ERiXDhLHxmo"&gt;https://www.youtube.com/watch?v=ERiXDhLHxmo&lt;/a&gt; )&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=iXbzktx1KfU"&gt;https://www.youtube.com/watch?v=iXbzktx1KfU&lt;/a&gt;&lt;p&gt;Have a nice day, and note &amp;gt;52% of the web is LLM slop now. YMMV =3&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364716&amp;amp;goto=item%3Fid%3D46363921%2346364716"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;a href="https://news.ycombinator.com/vote?id=46364243&amp;amp;how=up&amp;amp;goto=item%3Fid%3D46363921"&gt;&lt;/a&gt;&lt;a href="https://news.ycombinator.com/user?id=gethly"&gt;gethly&lt;/a&gt; &lt;a href="https://news.ycombinator.com/item?id=46364243"&gt;7 hours ago&lt;/a&gt;  | &lt;a href="#46364716"&gt;prev&lt;/a&gt; &lt;a href="javascript:void(0)"&gt;[–]&lt;/a&gt;&lt;br/&gt;
There are no such blogs. Usually companies, or individuals, will write these after they implement some feature into their products. Which makes them inherently little pieces of information scattered all over the internet and there is no one blog that is just about this.&lt;p&gt;&lt;a href="https://news.ycombinator.com/reply?id=46364243&amp;amp;goto=item%3Fid%3D46363921%2346364243"&gt;reply&lt;/a&gt;&lt;/p&gt;&lt;br/&gt;&lt;br/&gt;
&lt;img src="https://news.ycombinator.com/s.gif"/&gt;&lt;br/&gt;
&lt;a href="https://news.ycombinator.com/newsguidelines.html"&gt;Guidelines&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/newsfaq.html"&gt;FAQ&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/lists"&gt;Lists&lt;/a&gt; | &lt;a href="https://github.com/HackerNews/API"&gt;API&lt;/a&gt; | &lt;a href="https://news.ycombinator.com/security.html"&gt;Security&lt;/a&gt; | &lt;a href="https://www.ycombinator.com/legal/"&gt;Legal&lt;/a&gt; | &lt;a href="https://www.ycombinator.com/apply/"&gt;Apply to YC&lt;/a&gt; | &lt;a href="mailto:hn@ycombinator.com"&gt;Contact&lt;/a&gt;&lt;br/&gt;&lt;br/&gt;
&lt;/body&gt;</content:encoded>
      <guid isPermaLink="false">https://news.ycombinator.com/item?id=46363921</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 09:50:31 +0000</pubDate>
    </item>
    <item>
      <title>Carnap – A formal logic framework for Haskell</title>
      <link>https://carnap.io/</link>
      <description>A formal logic framework for Haskell</description>
      <content:encoded>&lt;body&gt;

&lt;h1&gt;Welcome to Carnap.io&lt;/h1&gt;
&lt;p&gt;A formal logic framework for Haskell&lt;/p&gt;


&lt;img src="https://static.carnap.io/img/russell2.png?etag=OWVOGUWw"/&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt; Carnap is a free and open software framework written in &lt;a href="https://haskell.org"&gt;Haskell&lt;/a&gt;
 for teaching and studying formal logic.  Carnap powers logic courses at &lt;a href="https://carnap.io/about#who"&gt;dozens of colleges and universities&lt;/a&gt;
 around the world.&lt;/p&gt;&lt;p&gt; If you're a student in a course that uses Carnap, please follow the links at the top of the page to log in and to access course materials.&lt;/p&gt;
&lt;p&gt; If you're just curious about Carnap, you can find some general information on our &lt;a href="https://carnap.io/about"&gt;about&lt;/a&gt;
 page.  If you're interested in the project, and would like to use Carnap in a class you're teaching, or get involved in some other way, please feel free to &lt;a href="mailto:gleachkr@ksu.edu"&gt;get in touch!&lt;/a&gt;
&lt;/p&gt;



&lt;/body&gt;</content:encoded>
      <guid isPermaLink="false">https://carnap.io/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 09:17:42 +0000</pubDate>
    </item>
    <item>
      <title>Instant database clones with PostgreSQL 18</title>
      <link>https://boringsql.com/posts/instant-database-clones/</link>
      <description>Have you ever watched long running migration script, wondering if it's about
to wreck your data? Or wish you can "just" spin a fresh copy of database for
each test run? Or wanted to have reproducible snapshots to reset between
runs of your test suite, (and yes, because you are reading boringSQL) needed
to reset the learning environment?</description>
      <content:encoded>&lt;article class="post-single"&gt;




Table of Contents


&lt;ul&gt;
&lt;li&gt;
&lt;a href="#create-database-strategy"&gt;CREATE DATABASE ... STRATEGY&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="#file-copy"&gt;FILE_COPY&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="#the-benchmark"&gt;The benchmark&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="#working-with-cloned-data"&gt;Working with cloned data&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="#xfs-proof"&gt;XFS proof&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="#things-to-be-aware-of"&gt;Things to be aware of&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;




&lt;p&gt;Have you ever watched long running migration script, wondering if it's about
to wreck your data? Or wish you can "just" spin a fresh copy of database for
each test run? Or wanted to have reproducible snapshots to reset between
runs of your test suite, (and yes, because you are reading boringSQL) needed
to reset the learning environment?&lt;/p&gt;
&lt;p&gt;When your database is a few megabytes, &lt;code&gt;pg_dump&lt;/code&gt; and restore works fine. But
what happens when you're dealing with hundreds of megabytes/gigabytes - or more?
Suddenly "just make a copy" becomes a burden.&lt;/p&gt;
&lt;p&gt;You've probably noticed that PostgreSQL connects to &lt;code&gt;template1&lt;/code&gt; by default. What
you might have missed is that there's a whole templating system hiding in plain
sight. Every time you run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE DATABASE dbname;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PostgreSQL quietly clones standard system database &lt;code&gt;template1&lt;/code&gt; behind the
scenes. Making it same as if you would use&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE DATABASE dbname TEMPLATE template1;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The real power comes from the fact that you can replace &lt;code&gt;template1&lt;/code&gt; with any
database. You can find more at &lt;a href="https://www.postgresql.org/docs/current/manage-ag-templatedbs.html"&gt;Template Database
documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this article, we will cover a few tweaks that turn this templating system
into an instant, zero-copy database cloning machine.&lt;/p&gt;
&lt;h2&gt;CREATE DATABASE ... STRATEGY&lt;a href="#create-database-strategy"&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Before PostgreSQL 15, when you created a new database from a template, it
operated strictly on the file level. This was effective, but to make it
reliable, Postgres had to flush all pending operations to disk (using
&lt;code&gt;CHECKPOINT&lt;/code&gt;) before taking a consistent snapshot. This created a massive I/O
spike - a "Checkpoint Storm" - that could stall your production traffic.&lt;/p&gt;
&lt;p&gt;Version 15 of PostgreSQL introduced new parameter &lt;code&gt;CREATE DATABASE ... STRATEGY = [strategy]&lt;/code&gt; and at the same time changed the default behaviour how the new
databases are created from templates. The new default become &lt;code&gt;WAL_LOG&lt;/code&gt; which
copies block-by-block via the Write-Ahead Log (WAL), making I/O sequential (and
much smoother) and support for concurrency without facing latency spike. This
prevented the need to CHECKPOINT but made the database cloning operation
potentially significantly slower. For an empty &lt;code&gt;template1&lt;/code&gt;, you won't notice the
difference. But if you try to clone a 500GB database using WAL_LOG, you are
going to be waiting a long time.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;STRATEGY&lt;/code&gt; parameter allows us to switch back to the original method
&lt;code&gt;FILE_COPY&lt;/code&gt; to keep the behaviour, and speed. And since PostgreSQL 18, this
opens the whole new set of options.&lt;/p&gt;
&lt;h2&gt;FILE_COPY&lt;a href="#file-copy"&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Because the &lt;code&gt;FILE_COPY&lt;/code&gt; strategy is a proxy to operating system file operations,
we can change how the OS handles those files.&lt;/p&gt;
&lt;p&gt;When using standard file system (like &lt;code&gt;ext4&lt;/code&gt;), PostgreSQL reads every byte of
the source file and writes it to a new location. It's a physical copy. However
starting with PostgreSQL 18 - &lt;code&gt;file_copy_method&lt;/code&gt; gives you options to switch
that logic; while default option remains &lt;code&gt;copy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;With modern filesystems (like ZFS, XFS with reflinks, APFS, etc.) you can switch
it to &lt;code&gt;clone&lt;/code&gt; and leverage &lt;code&gt;CLONE&lt;/code&gt; (&lt;code&gt;FICLONE&lt;/code&gt; on Linux) operation for almost
instant operation. And it won't take any additional space.&lt;/p&gt;
&lt;p&gt;All you have to do is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linux with XFS or ZFS support (we will use XFS for the demostration) or similar
operating system. MacOS APFS is also fully supported. FreeBSD with ZFS also
supported (which normally would be my choice, but haven't got time to test so
far)&lt;/li&gt;
&lt;li&gt;PostgreSQL cluster on that file system&lt;/li&gt;
&lt;li&gt;update the configuration &lt;code&gt;file_copy_method = clone&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;and reload the configuration&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The benchmark&lt;a href="#the-benchmark"&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;We need some dummy data to copy. This is the only part of the tutorial where you
have to wait. Let's generate a ~6GB database.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE DATABASE source_db;
\c source_db

CREATE TABLE boring_data (
    id serial PRIMARY KEY,
    payload text
);

-- generate 50m rows
INSERT INTO boring_data (payload)
SELECT md5(random()::text) || md5(random()::text)
FROM generate_series(1, 50000000);

-- force a checkpoint
CHECKPOINT;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can verify the database now has roughly 6GB of data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name              | source_db
Owner             | postgres
Encoding          | UTF8
Locale Provider   | libc
Collate           | en_US.UTF-8
Ctype             | en_US.UTF-8
Locale            |
ICU Rules         |
Access privileges |
Size              | 6289 MB
Tablespace        | pg_default
Description       |
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While enabling &lt;code&gt;\timing&lt;/code&gt; you can test the default (WAL_LOG) strategy. And on my
test volume (relatively slow storage) I get&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE DATABASE slow_copy TEMPLATE source_db;
CREATE DATABASE
Time: 67000.615 ms (01:07.001)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let's verify our configuration is set for speed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;show file_copy_method;
 file_copy_method
------------------
 clone
(1 row)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let's request the semi-instant clone of the same database, without taking
extra disk space at the same time.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE DATABASE fast_clone TEMPLATE source_db STRATEGY=FILE_COPY;
CREATE DATABASE
Time: 212.053 ms
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That's a quite an improvement, isn't it?&lt;/p&gt;
&lt;h2&gt;Working with cloned data&lt;a href="#working-with-cloned-data"&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;That was the simple part. But what is happening behind the scenes?&lt;/p&gt;
&lt;p&gt;When you clone a database with &lt;code&gt;file_copy_method = clone&lt;/code&gt;, PostgreSQL doesn't
duplicate any data. The filesystem creates new metadata entries that point to
the same physical blocks. Both databases share identical storage.&lt;/p&gt;
&lt;p&gt;This can create some initial confusion. If you ask PostgreSQL for the size:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT pg_database_size('source_db') as source,
       pg_database_size('fast_clone') as clone;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PostgreSQL reports both as ~6GB because that's the logical size - how much data
each database "contains" - i.e. logical size.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;-[ RECORD 1 ]------
source | 6594041535
clone  | 6594041535
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The interesting part happens when you start writing. PostgreSQL doesn't update
tuples in place. When you UPDATE a row, it writes a new tuple version somewhere
(often a different page entirely) and marks the old one as dead. The filesystem
doesn't care about PostgreSQL internals - it just sees writes to 8KB pages. Any
write to a shared page triggers a copy of that entire page.&lt;/p&gt;
&lt;p&gt;A single UPDATE will therefore trigger copy-on-write on multiple pages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the page holding the old tuple&lt;/li&gt;
&lt;li&gt;the page receiving the new tuple&lt;/li&gt;
&lt;li&gt;index pages if any indexed columns changed&lt;/li&gt;
&lt;li&gt;FSM and visibility map pages as PostgreSQL tracks free space&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And later, VACUUM touches even more pages while cleaning up dead tuples. In this
case diverging quickly from the linked storage.&lt;/p&gt;
&lt;h2&gt;XFS proof&lt;a href="#xfs-proof"&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Using the database OID and relfilenode we can verify the both databases are now
sharing physical blocks.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16402/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16402/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..    2031:   10471550..  10473581:   2032:             shared
   1:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   2:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   3:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   4:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   5:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   6:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16402/16404: 7 extents found
root@clone-demo:/var/lib/postgresql#
root@clone-demo:/var/lib/postgresql#
root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16418/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16418/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..    2031:   10471550..  10473581:   2032:             shared
   1:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   2:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   3:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   4:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   5:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   6:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16418/16404: 7 extents found
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All it takes is to update some rows using&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;update boring_data set payload = 'new value' || id where id IN (select id from boring_data limit 20);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the situation will start to change.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16402/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16402/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..      39:   10471550..  10471589:     40:
   1:       40..    2031:   10471590..  10473581:   1992:             shared
   2:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   3:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   4:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   5:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   6:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   7:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16402/16404: 7 extents found
root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16418/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16418/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..      39:   10297326..  10297365:     40:
   1:       40..    2031:   10471590..  10473581:   1992:   10297366: shared
   2:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   3:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   4:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   5:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   6:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   7:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16418/16404: 8 extents found
root@clone-demo:/var/lib/postgresql#
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case extent 0 no longer has shared flag, first 40 blocks size (with
default size 4KB) now diverge, making it total of 160KB. Each database now has
its own copy at different physical address. The remaining extents are still
shared.&lt;/p&gt;
&lt;h2&gt;Things to be aware of&lt;a href="#things-to-be-aware-of"&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Cloning is tempting but there's one serious limitation you need to be aware if
you ever attempt to do it in production. The source database can't have any
active connections during cloning. This is a PostgreSQL limitation, not a
filesystem one. For production use, this usually means you create a dedicated
template database rather than cloning your live database directly. Or given the
relatively short time the operation takes you have to schedule the cloning in
times where you can temporary block/terminate all connections.&lt;/p&gt;
&lt;p&gt;Other limitation is that the cloning only works within a single filesystem. If
your databases spans multiple table spaces on different mount points, cloning
will fall back to regular physical copy.&lt;/p&gt;
&lt;p&gt;Finally, in most managed cloud environments (AWS RDS, Google Cloud SQL), you
will not have access to the underlying filesystem to configure this. You are
stuck with their proprietary (and often billed) functionality. But for your own
VMs or bare metal? Go ahead and try it.&lt;/p&gt;


&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://boringsql.com/posts/instant-database-clones/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 07:58:25 +0000</pubDate>
    </item>
    <item>
      <title>10 years bootstrapped: €6.5M revenue with a team of 13</title>
      <link>https://www.datocms.com/blog/a-look-back-at-2025</link>
      <description>Explore all the capabilities of the friendliest Headless CMS in town</description>
      <content:encoded>&lt;article class="_itemBody_11r63_114"&gt;  &lt;h4&gt;Meet DatoCMS!&lt;/h4&gt;  &lt;p&gt;Explore all the capabilities of the friendliest Headless CMS in town&lt;/p&gt; &lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://www.datocms.com/blog/a-look-back-at-2025</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 07:50:03 +0000</pubDate>
    </item>
    <item>
      <title>iOS 26.3 brings AirPods-like pairing to third-party devices in EU under DMA</title>
      <link>https://www.macrumors.com/2025/12/22/ios-26-3-dma-airpods-pairing/</link>
      <description>The European Commission today praised the interoperability changes that Apple is introducing in iOS 26.3, once again crediting the Digital Markets Act (DMA) with bringing "new opportunities" to European users and developers.</description>
      <content:encoded>&lt;article class="article--2pJwZBkO js-article" expanded="true"&gt;&lt;h1&gt;iOS 26.3 Brings AirPods-Like Pairing to Third-Party Devices in EU Under DMA&lt;/h1&gt;Monday December 22, 2025 3:20 pm PST by &lt;a href="https://www.macrumors.com/author/juli-clover/"&gt;Juli Clover&lt;/a&gt;&lt;p&gt;The European Commission today praised the interoperability changes that Apple is introducing in iOS 26.3, once again crediting the Digital Markets Act (DMA) with bringing "new opportunities" to European users and developers.&lt;/p&gt;
&lt;p&gt;&lt;img alt="iOS 26" src="https://images.macrumors.com/t/yhvXefawwRAhQDktpUWYQjx3MLI=/400x0/article-new/2025/12/iOS-26.3-Feature.jpg?lossy"/&gt;&lt;br/&gt;The Digital Markets Act requires Apple to provide third-party accessories with the same capabilities and access to device features that Apple's own products get. In iOS 26.3, EU wearable device makers can now test proximity pairing and improved notifications.&lt;/p&gt;
&lt;p&gt;Here are the new capabilities that Apple is adding:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Proximity pairing&lt;/strong&gt; - Devices like earbuds will be able to pair with an iOS device in an AirPods-like way by bringing the accessory close to an &lt;a href="https://www.macrumors.com/guide/iphone/"&gt;iPhone&lt;/a&gt; or &lt;a href="https://www.macrumors.com/roundup/ipad/"&gt;iPad&lt;/a&gt; to initiate a simple, one-tap pairing process. Pairing third-party devices will no longer require multiple steps.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Notifications&lt;/strong&gt; - Third-party accessories like smart watches will be able to receive notifications from the ‌iPhone‌. Users will be able to view and react to incoming notifications, which is functionality normally limited to the Apple Watch. Notifications can only be forwarded to one connected device at a time, and turning on notifications for a third-party device disables notifications to an Apple Watch.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The European Commission says that developers can test third-party TVs, smart watches, and headphones with the new features in iOS 26.3, with the functionality to be "fully available in Europe" in 2026.&lt;/p&gt;
&lt;p&gt;iOS 26.3 offers "another step towards a more inter-connected digital ecosystem to the benefit of all EU citizens," according to the European Commission. iOS 26.3 is expected to launch at the end of January.&lt;/p&gt;
&lt;p&gt;The changes to proximity pairing and notifications are only available for device makers and ‌iPhone‌ and ‌iPad‌ users in the European Union.&lt;/p&gt;
[ &lt;a href="https://forums.macrumors.com/threads/ios-26-3-brings-airpods-like-pairing-to-third-party-devices-in-eu-under-dma.2475090/"&gt;71 comments&lt;/a&gt; ]&lt;a href="https://twitter.com/share?url=http%3A%2F%2Fwww.macrumors.com%2F2025%2F12%2F22%2Fios-26-3-dma-airpods-pairing%2F&amp;amp;text=iOS+26.3+Brings+AirPods-Like+Pairing+to+Third-Party+Devices+in+EU+Under+DMA&amp;amp;related=macrumors"&gt;&lt;/a&gt;&lt;a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fwww.macrumors.com%2F2025%2F12%2F22%2Fios-26-3-dma-airpods-pairing%2F&amp;amp;amp;src=sdkpreparse"&gt;&lt;/a&gt;&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://www.macrumors.com/2025/12/22/ios-26-3-dma-airpods-pairing/</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 06:22:21 +0000</pubDate>
    </item>
    <item>
      <title>Show HN: CineCLI – Browse and torrent movies directly from your terminal</title>
      <link>https://github.com/eyeblech/cinecli</link>
      <description>GitHub - eyeblech/cinecli: CineCLI is a cross-platform command-line movie browser built with Python.</description>
      <content:encoded>&lt;article class="markdown-body entry-content container-lg" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://camo.githubusercontent.com/d7a2a0757d30060c754de6e7b99cb5b4f0ffbc5ca1c30226c193693c606268c7/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f63696e65636c69"&gt;&lt;img alt="PyPI" src="https://camo.githubusercontent.com/d7a2a0757d30060c754de6e7b99cb5b4f0ffbc5ca1c30226c193693c606268c7/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f63696e65636c69"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/6095792cccbdf1e7ff3823e190b373efb89d3be5920ef7d442bffad7e06a46d4/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f63696e65636c69"&gt;&lt;img alt="Python" src="https://camo.githubusercontent.com/6095792cccbdf1e7ff3823e190b373efb89d3be5920ef7d442bffad7e06a46d4/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f63696e65636c69"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/f8df3091bbe1149f398a5369b2c39e896766f9f6efba3477c63e9b4aa940ef14/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e"&gt;&lt;img alt="License" src="https://camo.githubusercontent.com/f8df3091bbe1149f398a5369b2c39e896766f9f6efba3477c63e9b4aa940ef14/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/8016a83d8d76dafb654d0d0996f171069ef251332c073a69a63ea84549a1c9b6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f63696e65636c69"&gt;&lt;img alt="Downloads" src="https://camo.githubusercontent.com/8016a83d8d76dafb654d0d0996f171069ef251332c073a69a63ea84549a1c9b6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f63696e65636c69"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://camo.githubusercontent.com/5714c1d3e40f657a117aa9c961de32bf3aa29c10f83ea590e736f48c08a628a5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f657965626c6563682f63696e65636c693f7374796c653d666c61742d737175617265"&gt;&lt;img alt="Stars" src="https://camo.githubusercontent.com/5714c1d3e40f657a117aa9c961de32bf3aa29c10f83ea590e736f48c08a628a5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f657965626c6563682f63696e65636c693f7374796c653d666c61742d737175617265"/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;📡 YTS API Status&lt;/h2&gt;&lt;a href="#-yts-api-status"&gt;&lt;/a&gt;
&lt;p&gt;
&lt;a href="https://camo.githubusercontent.com/ee1c288267112cfd617cecbe2f9ec3209df0d2dbe299de8b1cd3fb4df4e3bfb5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f657965626c6563682f63696e65636c692f6170692d6865616c74682e796d6c3f6c6162656c3d595453253230415049267374796c653d666f722d7468652d6261646765"&gt;&lt;img alt="YTS API Status" src="https://camo.githubusercontent.com/ee1c288267112cfd617cecbe2f9ec3209df0d2dbe299de8b1cd3fb4df4e3bfb5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f657965626c6563682f63696e65636c692f6170692d6865616c74682e796d6c3f6c6162656c3d595453253230415049267374796c653d666f722d7468652d6261646765"/&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;strong&gt;Status is automatically monitored every 15 minutes.&lt;/strong&gt;&lt;br/&gt;

    🟢 Green = Operational   •  
    🔴 Red = Outage / API Down
  
&lt;/p&gt;

&lt;h1&gt;🎬 CineCLI&lt;/h1&gt;&lt;a href="#-cinecli"&gt;&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;Browse, inspect, and launch movie torrents directly from your terminal.&lt;br/&gt;
Fast. Cross-platform. Minimal. Beautiful.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/eyeblech/cinecli/blob/master/demo.gif"&gt;&lt;img alt="Demo" src="https://github.com/eyeblech/cinecli/raw/master/demo.gif"/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://camo.githubusercontent.com/6bce8c074ba1847154d5448fa4b909b24bdc5e2dead78e01601b2388cba04190/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6c696e75782532302537432532306d61634f5325323025374325323077696e646f77732d626c7565"&gt;&lt;img alt="Platform" src="https://camo.githubusercontent.com/6bce8c074ba1847154d5448fa4b909b24bdc5e2dead78e01601b2388cba04190/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6c696e75782532302537432532306d61634f5325323025374325323077696e646f77732d626c7565"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/4d05de71d175fd180e6a54008bb655173476617f053e30cce46a2ae0c33139dc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e746572666163652d434c492d6f72616e6765"&gt;&lt;img alt="CLI" src="https://camo.githubusercontent.com/4d05de71d175fd180e6a54008bb655173476617f053e30cce46a2ae0c33139dc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e746572666163652d434c492d6f72616e6765"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/a875f41a2bd54510be00d5743cb13308c385a54e0b945c0c2d49622d786f1ddf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f776f726b73253230696e2d7465726d696e616c2d626c61636b"&gt;&lt;img alt="Terminal" src="https://camo.githubusercontent.com/a875f41a2bd54510be00d5743cb13308c385a54e0b945c0c2d49622d786f1ddf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f776f726b73253230696e2d7465726d696e616c2d626c61636b"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;✨ Features&lt;/h2&gt;&lt;a href="#-features"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;🔍 Search movies from &lt;strong&gt;YTS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;🎥 View detailed movie information&lt;/li&gt;
&lt;li&gt;🧲 Launch magnet links directly into your torrent client&lt;/li&gt;
&lt;li&gt;📦 Download &lt;code&gt;.torrent&lt;/code&gt; files if preferred&lt;/li&gt;
&lt;li&gt;⚡ Auto-select best torrent (highest quality + healthy seeds)&lt;/li&gt;
&lt;li&gt;🖥 Cross-platform (Linux, macOS, Windows)&lt;/li&gt;
&lt;li&gt;🎨 Rich, clean terminal UI (powered by &lt;code&gt;rich&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;🧠 Smart defaults with full user control&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href="https://camo.githubusercontent.com/24291b36ca6bfe1e10b218fe428f13ad6a14a9ad12c9d0f814872d851314e944/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6275696c74253230776974682d54797065722d666636396234"&gt;&lt;img alt="Built with Typer" src="https://camo.githubusercontent.com/24291b36ca6bfe1e10b218fe428f13ad6a14a9ad12c9d0f814872d851314e944/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6275696c74253230776974682d54797065722d666636396234"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/7a65b723320e5684e19ed5a6a8383a2f2722db455ae63eabb40f81d666aa1fdd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6275696c74253230776974682d526963682d626c756576696f6c6574"&gt;&lt;img alt="Built with Rich" src="https://camo.githubusercontent.com/7a65b723320e5684e19ed5a6a8383a2f2722db455ae63eabb40f81d666aa1fdd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6275696c74253230776974682d526963682d626c756576696f6c6574"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;📦 Installation&lt;/h2&gt;&lt;a href="#-installation"&gt;&lt;/a&gt;
&lt;pre&gt;pip install cinecli
&lt;/pre&gt;
&lt;p&gt;Requires &lt;strong&gt;Python 3.9+&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;🚀 Usage&lt;/h2&gt;&lt;a href="#-usage"&gt;&lt;/a&gt;
&lt;h3&gt;🔎 Search for movies&lt;/h3&gt;&lt;a href="#-search-for-movies"&gt;&lt;/a&gt;
&lt;pre&gt;cinecli search matrix
&lt;/pre&gt;
&lt;p&gt;Displays matching movies with IDs:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ID     Title                 Year   Rating
3525   The Matrix            1999   8.7
3526   The Matrix Reloaded   2003   7.2

&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;🎬 Watch a movie&lt;/h3&gt;&lt;a href="#-watch-a-movie"&gt;&lt;/a&gt;
&lt;pre&gt;cinecli watch 3525
&lt;/pre&gt;
&lt;p&gt;What happens:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Shows movie details&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lists available torrents&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Auto-selects the best option (you can override)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Launches magnet or downloads &lt;code&gt;.torrent&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;🧭 Interactive mode (recommended for exploration)&lt;/h3&gt;&lt;a href="#-interactive-mode-recommended-for-exploration"&gt;&lt;/a&gt;
&lt;pre&gt;cinecli interactive
&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Search → select movie → choose torrent&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Manual selection by design (safe &amp;amp; explicit)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;⚙️ How magnet launching works&lt;/h2&gt;&lt;a href="#️-how-magnet-launching-works"&gt;&lt;/a&gt;
&lt;p&gt;CineCLI delegates magnet handling to your OS.&lt;/p&gt;
&lt;p&gt;That means:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Whatever torrent client is registered (&lt;code&gt;qBittorrent&lt;/code&gt;, &lt;code&gt;Transmission&lt;/code&gt;, etc.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CineCLI will launch it directly&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example (Linux):&lt;/p&gt;
&lt;pre&gt;xdg-mime query default x-scheme-handler/magnet
&lt;/pre&gt;

&lt;h2&gt;🎞 Demo Video&lt;/h2&gt;&lt;a href="#-demo-video"&gt;&lt;/a&gt;
&lt;p&gt;Full terminal walkthrough:&lt;/p&gt;



demo.mov






&lt;h2&gt;🛠 Tech Stack&lt;/h2&gt;&lt;a href="#-tech-stack"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Python&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Typer&lt;/strong&gt; — CLI framework&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rich&lt;/strong&gt; — terminal UI&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Requests&lt;/strong&gt; — API communication&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;YTS API&lt;/strong&gt; — movie data source&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;📄 License&lt;/h2&gt;&lt;a href="#-license"&gt;&lt;/a&gt;
&lt;p&gt;MIT—see &lt;a href="https://github.com/eyeblech/cinecli/blob/master/LICENSE"&gt;LICENSE&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Use it. Fork it. Improve it.&lt;/p&gt;

&lt;h2&gt;🙌 Author&lt;/h2&gt;&lt;a href="#-author"&gt;&lt;/a&gt;
&lt;p&gt;Built by &lt;strong&gt;eyeblech&lt;/strong&gt;&lt;br/&gt;
📧 &lt;a href="mailto:0x1123@proton.me"&gt;0x1123@proton.me&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;STAR the repo if you like it! ⭐&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://camo.githubusercontent.com/55adbef48125ea91f8864924c309ff1710b23c84e01b0670b45e467e795ae13b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6f70656e2d2d736f757263652d7965732d627269676874677265656e"&gt;&lt;img alt="Open Source" src="https://camo.githubusercontent.com/55adbef48125ea91f8864924c309ff1710b23c84e01b0670b45e467e795ae13b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6f70656e2d2d736f757263652d7965732d627269676874677265656e"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/da21b16c25c3d396a17fe7ba748e0497a765b934030128e412161493c4ea5bec/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d61696e7461696e65642d7965732d73756363657373"&gt;&lt;img alt="Maintained" src="https://camo.githubusercontent.com/da21b16c25c3d396a17fe7ba748e0497a765b934030128e412161493c4ea5bec/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d61696e7461696e65642d7965732d73756363657373"/&gt;&lt;/a&gt;
&lt;a href="https://camo.githubusercontent.com/03fe1c696836c0a3e46f187fad3b80abadafb5f331737069810e10468871395f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5052732d77656c636f6d652d707572706c65"&gt;&lt;img alt="PRs Welcome" src="https://camo.githubusercontent.com/03fe1c696836c0a3e46f187fad3b80abadafb5f331737069810e10468871395f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5052732d77656c636f6d652d707572706c65"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;⭐ Star History&lt;/h2&gt;&lt;a href="#-star-history"&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href="https://star-history.com/#eyeblech/cinecli&amp;amp;Date"&gt;&lt;img alt="Star History Chart" src="https://camo.githubusercontent.com/b96f697184e70f5a4a70e67126c617988ef3cbcab75c7d140ab123f6695090ab/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d657965626c6563682f63696e65636c6926747970653d44617465"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://github.com/eyeblech/cinecli</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 05:17:50 +0000</pubDate>
    </item>
    <item>
      <title>Snitch – A friendlier ss/netstat</title>
      <link>https://github.com/karol-broda/snitch</link>
      <description>a friendlier ss / netstat for humans. inspect network connections with a clean tui or styled tables.</description>
      <content:encoded>&lt;article class="markdown-body entry-content container-lg" itemprop="text"&gt;&lt;h1&gt;snitch&lt;/h1&gt;&lt;a href="#snitch"&gt;&lt;/a&gt;
&lt;p&gt;a friendlier &lt;code&gt;ss&lt;/code&gt; / &lt;code&gt;netstat&lt;/code&gt; for humans. inspect network connections with a clean tui or styled tables.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/karol-broda/snitch/blob/master/demo/demo.gif"&gt;&lt;img alt="snitch demo" src="https://github.com/karol-broda/snitch/raw/master/demo/demo.gif"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;install&lt;/h2&gt;&lt;a href="#install"&gt;&lt;/a&gt;
&lt;h3&gt;go&lt;/h3&gt;&lt;a href="#go"&gt;&lt;/a&gt;
&lt;pre&gt;go install github.com/karol-broda/snitch@latest&lt;/pre&gt;
&lt;h3&gt;nixos / nix&lt;/h3&gt;&lt;a href="#nixos--nix"&gt;&lt;/a&gt;
&lt;pre&gt;# try it
nix run github:karol-broda/snitch

# install to profile
nix profile install github:karol-broda/snitch

# or add to flake inputs
{
  inputs.snitch.url = "github:karol-broda/snitch";
}
# then use: inputs.snitch.packages.${system}.default&lt;/pre&gt;
&lt;h3&gt;arch linux (aur)&lt;/h3&gt;&lt;a href="#arch-linux-aur"&gt;&lt;/a&gt;
&lt;pre&gt;# with yay
yay -S snitch-bin

# with paru
paru -S snitch-bin&lt;/pre&gt;
&lt;h3&gt;shell script&lt;/h3&gt;&lt;a href="#shell-script"&gt;&lt;/a&gt;
&lt;pre&gt;curl -sSL https://raw.githubusercontent.com/karol-broda/snitch/master/install.sh | sh&lt;/pre&gt;
&lt;p&gt;installs to &lt;code&gt;~/.local/bin&lt;/code&gt; if available, otherwise &lt;code&gt;/usr/local/bin&lt;/code&gt;. override with:&lt;/p&gt;
&lt;pre&gt;curl -sSL https://raw.githubusercontent.com/karol-broda/snitch/master/install.sh | INSTALL_DIR=~/bin sh&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;macos:&lt;/strong&gt; the install script automatically removes the quarantine attribute (&lt;code&gt;com.apple.quarantine&lt;/code&gt;) from the binary to allow it to run without gatekeeper warnings. to disable this, set &lt;code&gt;KEEP_QUARANTINE=1&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;binary&lt;/h3&gt;&lt;a href="#binary"&gt;&lt;/a&gt;
&lt;p&gt;download from &lt;a href="https://github.com/karol-broda/snitch/releases"&gt;releases&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;linux:&lt;/strong&gt; &lt;code&gt;snitch_&amp;lt;version&amp;gt;_linux_&amp;lt;arch&amp;gt;.tar.gz&lt;/code&gt; or &lt;code&gt;.deb&lt;/code&gt;/&lt;code&gt;.rpm&lt;/code&gt;/&lt;code&gt;.apk&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;macos:&lt;/strong&gt; &lt;code&gt;snitch_&amp;lt;version&amp;gt;_darwin_&amp;lt;arch&amp;gt;.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;tar xzf snitch_*.tar.gz
sudo mv snitch /usr/local/bin/&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;macos:&lt;/strong&gt; if blocked with "cannot be opened because the developer cannot be verified", run:&lt;/p&gt;
&lt;pre&gt;xattr -d com.apple.quarantine /usr/local/bin/snitch&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h2&gt;quick start&lt;/h2&gt;&lt;a href="#quick-start"&gt;&lt;/a&gt;
&lt;pre&gt;snitch              # launch interactive tui
snitch -l           # tui showing only listening sockets
snitch ls           # print styled table and exit
snitch ls -l        # listening sockets only
snitch ls -t -e     # tcp established connections
snitch ls -p        # plain output (parsable)&lt;/pre&gt;
&lt;h2&gt;commands&lt;/h2&gt;&lt;a href="#commands"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;code&gt;snitch&lt;/code&gt; / &lt;code&gt;snitch top&lt;/code&gt;&lt;/h3&gt;&lt;a href="#snitch--snitch-top"&gt;&lt;/a&gt;
&lt;p&gt;interactive tui with live-updating connection list.&lt;/p&gt;
&lt;pre&gt;snitch                  # all connections
snitch -l               # listening only
snitch -t               # tcp only
snitch -e               # established only
snitch -i 2s            # 2 second refresh interval&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;keybindings:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;j/k, ↑/↓      navigate
g/G           top/bottom
t/u           toggle tcp/udp
l/e/o         toggle listen/established/other
s/S           cycle sort / reverse
w             watch/monitor process (highlight)
W             clear all watched
K             kill process (with confirmation)
/             search
enter         connection details
?             help
q             quit
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;code&gt;snitch ls&lt;/code&gt;&lt;/h3&gt;&lt;a href="#snitch-ls"&gt;&lt;/a&gt;
&lt;p&gt;one-shot table output. uses a pager automatically if output exceeds terminal height.&lt;/p&gt;
&lt;pre&gt;snitch ls               # styled table (default)
snitch ls -l            # listening only
snitch ls -t -l         # tcp listeners
snitch ls -e            # established only
snitch ls -p            # plain/parsable output
snitch ls -o json       # json output
snitch ls -o csv        # csv output
snitch ls -n            # numeric (no dns resolution)
snitch ls --no-headers  # omit headers&lt;/pre&gt;
&lt;h3&gt;&lt;code&gt;snitch json&lt;/code&gt;&lt;/h3&gt;&lt;a href="#snitch-json"&gt;&lt;/a&gt;
&lt;p&gt;json output for scripting.&lt;/p&gt;
&lt;pre&gt;snitch json
snitch json -l&lt;/pre&gt;
&lt;h3&gt;&lt;code&gt;snitch watch&lt;/code&gt;&lt;/h3&gt;&lt;a href="#snitch-watch"&gt;&lt;/a&gt;
&lt;p&gt;stream json frames at an interval.&lt;/p&gt;
&lt;pre&gt;snitch watch -i 1s | jq '.count'
snitch watch -l -i 500ms&lt;/pre&gt;
&lt;h3&gt;&lt;code&gt;snitch upgrade&lt;/code&gt;&lt;/h3&gt;&lt;a href="#snitch-upgrade"&gt;&lt;/a&gt;
&lt;p&gt;check for updates and upgrade in-place.&lt;/p&gt;
&lt;pre&gt;snitch upgrade              # check for updates
snitch upgrade --yes        # upgrade automatically
snitch upgrade -v 0.1.7     # install specific version&lt;/pre&gt;
&lt;h2&gt;filters&lt;/h2&gt;&lt;a href="#filters"&gt;&lt;/a&gt;
&lt;p&gt;shortcut flags work on all commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;-t, --tcp           tcp only
-u, --udp           udp only
-l, --listen        listening sockets
-e, --established   established connections
-4, --ipv4          ipv4 only
-6, --ipv6          ipv6 only
-n, --numeric       no dns resolution
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;for more specific filtering, use &lt;code&gt;key=value&lt;/code&gt; syntax with &lt;code&gt;ls&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;snitch ls proto=tcp state=listen
snitch ls pid=1234
snitch ls proc=nginx
snitch ls lport=443
snitch ls contains=google&lt;/pre&gt;
&lt;h2&gt;output&lt;/h2&gt;&lt;a href="#output"&gt;&lt;/a&gt;
&lt;p&gt;styled table (default):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  ╭─────────────────┬───────┬───────┬─────────────┬─────────────────┬────────╮
  │ PROCESS         │ PID   │ PROTO │ STATE       │ LADDR           │ LPORT  │
  ├─────────────────┼───────┼───────┼─────────────┼─────────────────┼────────┤
  │ nginx           │ 1234  │ tcp   │ LISTEN      │ *               │ 80     │
  │ postgres        │ 5678  │ tcp   │ LISTEN      │ 127.0.0.1       │ 5432   │
  ╰─────────────────┴───────┴───────┴─────────────┴─────────────────┴────────╯
  2 connections
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;plain output (&lt;code&gt;-p&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PROCESS    PID    PROTO   STATE    LADDR       LPORT
nginx      1234   tcp     LISTEN   *           80
postgres   5678   tcp     LISTEN   127.0.0.1   5432
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;configuration&lt;/h2&gt;&lt;a href="#configuration"&gt;&lt;/a&gt;
&lt;p&gt;optional config file at &lt;code&gt;~/.config/snitch/snitch.toml&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;[defaults]
numeric = false
theme = "auto"&lt;/pre&gt;
&lt;h2&gt;requirements&lt;/h2&gt;&lt;a href="#requirements"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;linux or macos&lt;/li&gt;
&lt;li&gt;linux: reads from &lt;code&gt;/proc/net/*&lt;/code&gt;, root or &lt;code&gt;CAP_NET_ADMIN&lt;/code&gt; for full process info&lt;/li&gt;
&lt;li&gt;macos: uses system APIs, may require sudo for full process info&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://github.com/karol-broda/snitch</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 01:03:57 +0000</pubDate>
    </item>
    <item>
      <title>Inside CECOT – 60 Minutes [video]</title>
      <link>https://archive.org/details/insidececot</link>
      <description>0 Views</description>
      <content:encoded>&lt;main id="maincontent"&gt;

&lt;!--HTML--&gt;
&lt;!--//.container-ia--&gt;






&lt;h1&gt;
    Inside CECOT | 60 Minutes  &lt;/h1&gt;
&lt;h2&gt;
    Video Item Preview
  &lt;/h2&gt;




&lt;a href="#"&gt;&lt;/a&gt;
&lt;a href="#"&gt;

&lt;/a&gt;
&lt;a href="#"&gt;&lt;/a&gt;
&lt;a href="#"&gt;


&lt;/a&gt;
&lt;!--/#theatre-controls--&gt;


&lt;!--/.xs-col-12--&gt;
&lt;!--/.row--&gt;






remove-circle 
&lt;h1&gt;Share or Embed This Item&lt;/h1&gt;
&lt;!--/.modal-header--&gt;




&lt;a href="https://twitter.com/intent/tweet?url=https://archive.org/details/insidececot&amp;amp;via=internetarchive&amp;amp;text=Inside+CECOT+%7C+60+Minutes+%3A+CBS+News+%3A+Free+Download%2C+Borrow%2C+and+Streaming+%3A+Internet+Archive"&gt;

Share to Twitter
&lt;/a&gt;
&lt;a href="https://www.facebook.com/sharer/sharer.php?u=https://archive.org/details/insidececot"&gt;

Share to Facebook
&lt;/a&gt;
&lt;a href="http://www.reddit.com/submit?url=https://archive.org/details/insidececot&amp;amp;title=Inside+CECOT+%7C+60+Minutes+%3A+CBS+News+%3A+Free+Download%2C+Borrow%2C+and+Streaming+%3A+Internet+Archive"&gt;

Share to Reddit
&lt;/a&gt;
&lt;a href="https://www.tumblr.com/widgets/share/tool?posttype=link&amp;amp;title=Inside+CECOT+%7C+60+Minutes+%3A+CBS+News+%3A+Free+Download%2C+Borrow%2C+and+Streaming+%3A+Internet+Archive&amp;amp;caption=Inside+CECOT+%7C+60+Minutes+%3A+CBS+News+%3A+Free+Download%2C+Borrow%2C+and+Streaming+%3A+Internet+Archive&amp;amp;content=https://archive.org/details/insidececot&amp;amp;canonicalUrl=https://archive.org/details/insidececot"&gt;

Share to Tumblr
&lt;/a&gt;
&lt;a href="http://www.pinterest.com/pin/create/button/?url=https://archive.org/details/insidececot&amp;amp;description=Inside+CECOT+%7C+60+Minutes+%3A+CBS+News+%3A+Free+Download%2C+Borrow%2C+and+Streaming+%3A+Internet+Archive"&gt;

Share to Pinterest
&lt;/a&gt;
&lt;a href="mailto:?body=https://archive.org/details/insidececot&amp;amp;subject=Inside CECOT | 60 Minutes : CBS News : Free Download, Borrow, and Streaming : Internet Archive"&gt;

Share via email
&lt;/a&gt;
&lt;a href="#"&gt;

&lt;img src="https://archive.org/images/link.svg"&gt;
&lt;/img&gt;
Copy Link
&lt;/a&gt;

&lt;br&gt;
&lt;/br&gt;



Begin playing at:
&lt;br&gt;
&lt;/br&gt;







                Want more?
                &lt;a href="https://archive.org/help/video.php?identifier=insidececot"&gt;Advanced embedding details, examples, and help&lt;/a&gt;!
              
&lt;!--/#cher-body--&gt;
&lt;!--/.modal-content--&gt;

&lt;!--/.modal-dialog--&gt;
&lt;!--/#cher-modal--&gt;
&lt;!--//#theatre-ia--&gt;


&lt;!--//.container-ia--&gt;

&lt;!--/.container-ia--&gt;








Favorite 




Share 



Flag

&lt;h1&gt;Flag this item for&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href="https://archive.org/login?referer=https%3A%2F%2Farchive.org%2Fdetails%2Finsidececot"&gt;
                  Graphic Violence                &lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://archive.org/login?referer=https%3A%2F%2Farchive.org%2Fdetails%2Finsidececot"&gt;
                  Explicit Sexual Content                &lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://archive.org/login?referer=https%3A%2F%2Farchive.org%2Fdetails%2Finsidececot"&gt;
                  Hate Speech                &lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://archive.org/login?referer=https%3A%2F%2Farchive.org%2Fdetails%2Finsidececot"&gt;
                  Misinformation/Disinformation                &lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://archive.org/login?referer=https%3A%2F%2Farchive.org%2Fdetails%2Finsidececot"&gt;
                  Marketing/Phishing/Advertising                &lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://archive.org/login?referer=https%3A%2F%2Farchive.org%2Fdetails%2Finsidececot"&gt;
                  Misleading/Inaccurate/Missing Metadata                &lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
 &lt;!-- /#flag-popover --&gt;
 &lt;!--/.dropdown --&gt;
 
&lt;!--/.thats-right--&gt;

movies
&lt;h1&gt;
Inside CECOT | 60 Minutes
&lt;/h1&gt;

by

&lt;a href="https://archive.org/search.php?query=creator%3A%22CBS+News%22"&gt;CBS News&lt;/a&gt; 

&lt;br&gt;
&lt;/br&gt;&lt;!--/.thats-left--&gt;











Publication date

&lt;a href="https://archive.org/search.php?query=date:2025-12-22"&gt;
2025-12-22
&lt;/a&gt;

Topics
&lt;a href="https://archive.org/search.php?query=subject%3A%2260+Minutes%22"&gt;60 Minutes&lt;/a&gt;
&lt;!-- contributor (also does usage rights, if specified for the contributor) --&gt;
&lt;!-- display Item Size in this position if theatre_type is not TV --&gt;
Item Size

            1.4G                

 &lt;!-- class="row metadata-list" --&gt;

Sharyn Alfonsi's "Inside CECOT" for 60 Minutes, which was censored by Bari Weiss, as it appeared on Canada's Global TV app.
&lt;!--/#descript--&gt;


Addeddate

2025-12-23 00:05:32 
Identifier

insidececot 
Scanner

            Internet Archive HTML5 Uploader 1.7.0                





&lt;h2&gt;


plus-circle Add Review

&lt;br&gt;
&lt;/br&gt;
comment
        Reviews (1)
&lt;/h2&gt;



&lt;!--/.thats-left--&gt;


&lt;p&gt;

0

        Views      &lt;/p&gt;
&lt;p&gt;
102
Favorites
&lt;/p&gt;
&lt;p&gt;
&lt;a href="#reviews"&gt;
1
            Review          &lt;/a&gt;
&lt;/p&gt;


&lt;h1&gt;
        DOWNLOAD OPTIONS
      &lt;/h1&gt;


&lt;a href="https://archive.org/download/insidececot/60minutesCECOTsegment.ia.mp4"&gt;

download                    1 file                  
&lt;/a&gt;

&lt;a href="https://archive.org/download/insidececot/60minutesCECOTsegment.ia.mp4"&gt;
                  H.264 IA                  download &lt;/a&gt;



&lt;a href="https://archive.org/download/insidececot/__ia_thumb.jpg"&gt;

download                    1 file                  
&lt;/a&gt;

&lt;a href="https://archive.org/download/insidececot/__ia_thumb.jpg"&gt;
                  ITEM TILE                  download &lt;/a&gt;



&lt;a href="https://archive.org/download/insidececot/60minutesCECOTsegment.mp4"&gt;

download                    1 file                  
&lt;/a&gt;

&lt;a href="https://archive.org/download/insidececot/60minutesCECOTsegment.mp4"&gt;
                  MPEG4                  download &lt;/a&gt;



&lt;a href="https://archive.org/download/insidececot/insidececot_archive.torrent"&gt;

download                    1 file                  
&lt;/a&gt;

&lt;a href="https://archive.org/download/insidececot/insidececot_archive.torrent"&gt;
                  TORRENT                  download &lt;/a&gt;



&lt;a href="https://archive.org/compress/insidececot"&gt;
download 24 Files
            &lt;/a&gt;&lt;br/&gt;
&lt;a href="https://archive.org/compress/insidececot/formats=MPEG4,ITEM TILE,ARCHIVE BITTORRENT,METADATA"&gt;download                7 Original&lt;/a&gt;&lt;br/&gt;

&lt;a href="https://archive.org/download/insidececot"&gt;SHOW ALL&lt;/a&gt;
&lt;br/&gt;




&lt;h1&gt;IN COLLECTIONS&lt;/h1&gt;
&lt;a href="https://archive.org/details/opensource_movies"&gt;
Community Video
&lt;img src="https://archive.org/services/img/opensource_movies"/&gt;
&lt;/a&gt;



&lt;p&gt;
        Uploaded by
                  &lt;a href="https://archive.org/details/@colin_haskins"&gt;
            Colin Haskins          &lt;/a&gt;
        
                  on December 23, 2025
&lt;/p&gt;

&lt;!--/.col-md-2--&gt;
&lt;!--/.thats-right--&gt;
&lt;!--/.row--&gt;
&lt;!--//.container-ia--&gt;



&lt;h1&gt;SIMILAR ITEMS (based on metadata)&lt;/h1&gt;



&lt;!--//.container-ia--&gt;

&lt;!--/.container--&gt;

&lt;a href="https://archive.org/about/terms"&gt;Terms of Service (last updated 12/31/2014)&lt;/a&gt;

&lt;/main&gt;</content:encoded>
      <guid isPermaLink="false">https://archive.org/details/insidececot</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 00:36:27 +0000</pubDate>
    </item>
    <item>
      <title>Local AI is driving the biggest change in laptops in decades</title>
      <link>https://spectrum.ieee.org/ai-models-locally</link>
      <description>Local AI is driving the biggest change in laptops in decades</description>
      <content:encoded>&lt;article class="clearfix image-article sm-mb-1 quality-HD post-2674288969" data-category="Computing" data-frozen-sections="[]" elid="2674288969"&gt;&lt;a href="https://spectrum.ieee.org/topic/computing/"&gt;Computing&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/topic/artificial-intelligence/"&gt;AI&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/magazine/"&gt;Magazine&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/type/feature/"&gt;Feature&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/magazine/2025/december/"&gt;December 2025&lt;/a&gt;&lt;h1&gt;
        Your Laptop Isn’t Ready for LLMs. That’s About to Change
    &lt;/h1&gt;&lt;h2&gt;&lt;p&gt;Local AI is driving the biggest change in laptops in decades&lt;/p&gt;&lt;/h2&gt;&lt;a href="https://spectrum.ieee.org/u/matthew-s-smith"&gt;Matthew S. Smith&lt;/a&gt;17 Nov 202510 min read&lt;!-- EMAIL --&gt;&lt;a href="mailto:?subject=Your%20Laptop%20Isn%E2%80%99t%20Ready%20for%20LLMs.%20That%E2%80%99s%20About%20to%20Change&amp;amp;body=https://spectrum.ieee.org/ai-models-locally"&gt;&lt;/a&gt;&lt;!-- COPY LINK --&gt;&lt;!-- TWITTER --&gt;&lt;!-- FACEBOOK --&gt;&lt;!-- LINKEDIN --&gt;Vertical&lt;img alt="Hands typing on laptop with a building scaffold shaped like a face on the screen." src="https://spectrum.ieee.org/media-library/hands-typing-on-laptop-with-a-building-scaffold-shaped-like-a-face-on-the-screen.jpg?id=62173854&amp;amp;width=1200&amp;amp;height=1587"/&gt;
        Dan Page
    Blue&lt;p&gt;&lt;strong&gt;Odds are the PC in &lt;/strong&gt;your office today isn’t ready to run AI &lt;a href="https://spectrum.ieee.org/large-language-model-performance"&gt;large language models&lt;/a&gt; (LLMs).&lt;/p&gt;&lt;p&gt;Today, most users interact with LLMs via an online, browser-based interface. The more technically inclined might use an application &lt;a href="https://spectrum.ieee.org/tag/programming"&gt;programming&lt;/a&gt; interface or command line interface. In either case, the queries are sent to a &lt;a href="https://spectrum.ieee.org/dcflex-data-center-flexibility"&gt;data center&lt;/a&gt;, where the model is hosted and run. It works well, until it doesn’t; a data-center outage can take a model offline for hours. Plus, some users might be unwilling to send &lt;a href="https://spectrum.ieee.org/tag/personal-data"&gt;personal data&lt;/a&gt; to an anonymous entity.&lt;/p&gt;&lt;p&gt;Running a model locally on your computer could offer significant benefits: lower latency, better understanding of your personal needs, and the privacy that comes with keeping your data on your own machine.&lt;/p&gt;&lt;p&gt;However, for the average laptop that’s over a year old, the number of useful &lt;a href="https://spectrum.ieee.org/tag/ai-models"&gt;AI models&lt;/a&gt; you can run locally on your PC is close to zero. This laptop might have a four- to eight-core processor (&lt;a href="https://en.wikipedia.org/wiki/Central_processing_unit"&gt;CPU&lt;/a&gt;), no dedicated &lt;a href="https://spectrum.ieee.org/tag/graphics"&gt;graphics&lt;/a&gt; chip (&lt;a href="https://en.wikipedia.org/wiki/Graphics_processing_unit"&gt;GPU&lt;/a&gt;) or neural-processing unit (&lt;a href="https://en.wikipedia.org/wiki/Neural_processing_unit"&gt;NPU&lt;/a&gt;), and 16 gigabytes of &lt;a href="https://en.wikipedia.org/wiki/Random-access_memory"&gt;RAM&lt;/a&gt;, leaving it underpowered for LLMs.&lt;/p&gt;&lt;p&gt;Even new, high-end PC &lt;a href="https://spectrum.ieee.org/tag/laptops"&gt;laptops&lt;/a&gt;, which often include an NPU and a GPU, can struggle. The largest AI models have over a trillion parameters, which requires memory in &lt;a href="https://snowkylin.github.io/blogs/a-note-on-deepseek-r1.html#:~:text=Note-,Models,Studio%20(%245.6k)!"&gt;the hundreds of gigabytes&lt;/a&gt;. Smaller versions of these models are available, even prolific, but they often lack the intelligence of larger models, which only dedicated AI &lt;a href="https://spectrum.ieee.org/tag/data-centers"&gt;data centers&lt;/a&gt; can handle.&lt;/p&gt;&lt;p&gt;The situation is even worse when other AI features aimed at making the model more capable are considered. &lt;a href="https://huggingface.co/blog/jjokah/small-language-model"&gt;Small language models (SLMs)&lt;/a&gt; that run on local hardware either scale back these features or omit them entirely. Image and video generation are difficult to run locally on laptops, too, and until recently they were reserved for high-end tower desktop PCs.&lt;/p&gt;&lt;p&gt;That’s a problem for AI adoption.&lt;/p&gt;&lt;p&gt;To make running AI models locally possible, the hardware found inside laptops and the software that runs on it will need an upgrade. This is the beginning of a shift in laptop design that will give engineers the opportunity to abandon the last vestiges of the past and reinvent the PC from the ground up.&lt;/p&gt;&lt;h2&gt;NPUs enter the chat&lt;/h2&gt;&lt;p&gt;The most obvious way to boost a PC’s AI performance is to place a powerful NPU alongside the CPU.&lt;/p&gt;&lt;p&gt;An NPU is a specialized chip &lt;a href="https://penntoday.upenn.edu/what-is-an-NPU-in-computing"&gt;designed for the matrix multiplication calculations&lt;/a&gt; that most AI models rely on. These matrix operations are highly parallelized, which is why &lt;a href="https://spectrum.ieee.org/tag/gpus"&gt;GPUs&lt;/a&gt; (which were already better at highly parallelized tasks than CPUs) became the go-to option for AI data centers.&lt;/p&gt;&lt;p&gt;However, because NPUs are designed specifically to handle these matrix operations—and not other tasks, like 3D graphics—&lt;a href="https://www.ibm.com/think/topics/npu-vs-gpu"&gt;they’re more power efficient than GPUs&lt;/a&gt;. That’s important for accelerating AI on portable consumer technology. NPUs also tend to provide better support for low-precision arithmetic than laptop GPUs. AI models often use low-precision arithmetic to reduce computational and memory needs on portable hardware, such as laptops.&lt;/p&gt;&lt;p&gt;&lt;h3&gt;
            
                Laptops Are Being Rebuilt to Run LLMs
            
            
        &lt;/h3&gt;&lt;img alt="Open laptop showing numbered internal components and hands at edges, with a screwdriver nearby." src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%202120%201414'%3E%3C/svg%3E"/&gt;&lt;p&gt;Your laptop today is probably not equipped to run large language models. But future laptops might. Chasing the dream of locally run LLMs, laptop architects are rethinking many aspects of current designs, leading to changes that are only now starting to take hold. &lt;/p&gt;&lt;p&gt;iStockphoto&lt;/p&gt;&lt;p&gt;1. &lt;strong&gt;Addition of NPUs.&lt;/strong&gt; Neural processing units (NPUs)—specialized accelerator chips that can run large language models (LLMs) and other AI models faster than CPUs and GPUs can—are being incorporated into laptops.&lt;/p&gt;&lt;p&gt;2. &lt;strong&gt;Addition of more—and faster—memory.&lt;/strong&gt; The largest language models take up hundreds of gigabytes of memory. To host these models, and serve them quickly to the number-crunching processing units, laptops are increasing their memory capacity and speed.&lt;/p&gt;&lt;p&gt;3. &lt;strong&gt;Consolidation of memory.&lt;/strong&gt; Most laptops today have a divided memory architecture, with a separate pool of memory to serve the GPUs. This made sense when the design first came out: GPUs needed faster memory access than could be supplied by the common bus. Now, to feed AI’s data appetite, laptop architects are rethinking this decision, and now they’re pooling memory together with faster &lt;a href="https://spectrum.ieee.org/tag/interconnects"&gt;interconnects&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;4. &lt;strong&gt;Combination of chips on the same silicon.&lt;/strong&gt; To help shorten the path to pooled memory, all the processing units—CPUs, GPUs, and NPUs—are now being integrated into the same silicon chip. This helps them connect to one another and to memory, but it will make maintenance more challenging.&lt;/p&gt;&lt;p&gt;5. &lt;strong&gt;&lt;a href="https://spectrum.ieee.org/tag/power-management"&gt;Power management&lt;/a&gt;.&lt;/strong&gt; AI models can see heavy use when they power always-on features like Microsoft’s Windows Recall, or the AI-powered Windows Search. Power-sipping NPUs help laptops run these models without excessive battery drain.&lt;/p&gt;&lt;/p&gt;&lt;p&gt;“With the NPU, the entire structure is really designed around the data type of tensors [a multidimensional array of numbers],” said &lt;a href="https://www.microsoft.com/applied-sciences/people/steven-bathiche"&gt;Steven Bathiche&lt;/a&gt;, technical fellow at &lt;a href="https://spectrum.ieee.org/tag/microsoft"&gt;Microsoft&lt;/a&gt;. “NPUs are much more specialized for that workload. And so we go from a CPU that can handle three [trillion] operations per second (TOPS), to an NPU” in &lt;a href="https://www.qualcomm.com"&gt;Qualcomm’s&lt;/a&gt; &lt;a href="https://spectrum.ieee.org/tag/snapdragon"&gt;Snapdragon&lt;/a&gt; X chip, which can power &lt;a href="https://www.microsoft.com/en-us/"&gt;Microsoft’s&lt;/a&gt; &lt;a href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/"&gt;Copilot+&lt;/a&gt; features. This includes &lt;a href="https://support.microsoft.com/en-us/windows/retrace-your-steps-with-recall-aa03f8a0-a78b-4b3e-b0a1-2eb8ac48701c"&gt;Windows Recall&lt;/a&gt;, which uses AI to create a searchable timeline of a user’s usage history by analyzing screenshots, and &lt;a href="https://blogs.windows.com/windows-insider/2024/02/22/windows-photos-gets-generative-erase-and-recent-ai-editing-features-now-available-on-arm64-devices-and-windows-10/"&gt;Windows Photos’ Generative erase&lt;/a&gt;, which can remove the background or specific objects from an image.&lt;/p&gt;&lt;p&gt;While &lt;a href="https://qualcomm.com"&gt;Qualcomm&lt;/a&gt; was arguably the first to provide an NPU for Windows laptops, it kickstarted an NPU TOPS arms race that also includes &lt;a href="https://www.amd.com/en.html"&gt;AMD&lt;/a&gt; and &lt;a href="https://www.intel.com/content/www/us/en/homepage.html"&gt;Intel&lt;/a&gt;, and the competition is already pushing NPU performance upward.&lt;/p&gt;&lt;p&gt;In 2023, prior to Qualcomm’s Snapdragon X, &lt;a href="https://spectrum.ieee.org/tag/amd"&gt;AMD&lt;/a&gt; chips with NPUs were uncommon, and those that existed delivered about 10 TOPS. Today, AMD and &lt;a href="https://spectrum.ieee.org/tag/intel"&gt;Intel&lt;/a&gt; have NPUs that are competitive with Snapdragon, &lt;a href="https://www.pcworld.com/article/2806864/intel-vs-amd-vs-qualcomm-which-copilot-pc-cpu-is-best-for-you.html"&gt;providing 40 to 50 TOPS&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.lifewire.com/dell-pro-max-plus-ai-laptop-11739880"&gt;Dell’s upcoming Pro Max Plus AI PC&lt;/a&gt; will up the ante with a &lt;a href="https://spectrum.ieee.org/tag/qualcomm"&gt;Qualcomm&lt;/a&gt; AI 100 NPU that promises up to 350 TOPS, improving performance by a staggering 35 times compared with that of the best available NPUs just a few years ago. Drawing that line up and to the right implies that NPUs capable of thousands of TOPS are just a couple of years away.&lt;/p&gt;&lt;p&gt;How many TOPS do you need to run state-of-the-art models with hundreds of millions of parameters? No one knows exactly. It’s not possible to run these models on today’s consumer hardware, so real-world tests just can’t be done. But it stands to reason that we’re within throwing distance of those capabilities. It’s also worth noting that LLMs are not the only use case for NPUs. &lt;a href="https://www.qualcomm.com/news/onq/2024/05/from-olympic-table-tennis-to-ai-product-manager-meet-dr-vinesh-sukumar"&gt;Vinesh Sukumar&lt;/a&gt;, Qualcomm’s head of AI and &lt;a href="https://spectrum.ieee.org/tag/machine-learning"&gt;machine learning&lt;/a&gt; product management, says &lt;a href="https://spectrum.ieee.org/tag/ai-image-generation"&gt;AI image generation&lt;/a&gt; and manipulation is an example of a task that’s difficult without an NPU or high-end GPU.&lt;/p&gt;&lt;h2&gt;Building balanced chips for better AI&lt;/h2&gt;&lt;p&gt;Faster NPUs will handle more tokens per second, which in turn will deliver a faster, more fluid experience when using AI models. Yet there’s more to running AI on local hardware than throwing a bigger, better NPU at the problem.&lt;/p&gt;&lt;p&gt;&lt;a href="https://ieeexplore.ieee.org/author/37089001134"&gt;Mike Clark&lt;/a&gt;, corporate fellow design engineer at AMD, says that companies that design chips to accelerate AI on the PC can’t put all their bets on the NPU. That’s in part because AI isn’t a replacement for, but rather an addition to, the tasks a PC is expected to handle.&lt;/p&gt;&lt;p&gt;“We must be good at low latency, at handling smaller data types, at branching code—traditional workloads. We can’t give that up, but we still want to be good at AI,” says Clark. He also noted that “the CPU is used to prepare data” for AI workloads, which means an inadequate CPU could become a bottleneck.&lt;/p&gt;&lt;p&gt;NPUs must also compete or cooperate with GPUs. On the PC, that often means a high-end AMD or &lt;a href="https://spectrum.ieee.org/tag/nvidia"&gt;Nvidia&lt;/a&gt; GPU with large amounts of built-in memory. The &lt;a href="https://signal65.com/research/ai/nvidia-geforce-rtx-5090-founders-edition-performance-analysis/"&gt;Nvidia GeForce RTX 5090&lt;/a&gt;’s specifications quote an AI performance up to 3,352 TOPS, which leaves even the Qualcomm AI 100 in the dust.&lt;/p&gt;&lt;p&gt;That comes with a big caveat, however: power. Though extremely capable, the RTX 5090 is designed to draw up to &lt;a href="https://signal65.com/research/ai/nvidia-geforce-rtx-5090-founders-edition-performance-analysis/"&gt;575 watts&lt;/a&gt; on its own. Mobile versions for laptops are more miserly but still draw up to &lt;a href="https://www.notebookcheck.net/Nvidia-GeForce-RTX-5090-Laptop-Benchmarks-and-Specs.934947.0.html"&gt;175 W&lt;/a&gt;, which can quickly drain a laptop battery.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.linkedin.com/in/simonng39/?originalSubdomain=ca"&gt;Simon Ng&lt;/a&gt;, client AI product manager at Intel, says the company is “seeing that the NPU will just do things much more efficiently at lower power.”&lt;a href="https://www.linkedin.com/in/rakesh-s-anigundi/"&gt; Rakesh Anigundi&lt;/a&gt;, AMD’s director of product management for Ryzen AI, agrees. He adds that &lt;a href="https://spectrum.ieee.org/tag/low-power"&gt;low-power&lt;/a&gt; operation is particularly important because AI workloads tend to take longer to run than other demanding tasks, like encoding a video or rendering graphics. “You’ll want to be running this for a longer period of time, such as an AI personal assistant, which could be always active and listening for your command,” he says.&lt;/p&gt;&lt;p&gt;These competing priorities mean chip architects and system designers will need to make tough calls about how to allocate silicon and power in AI PCs, especially those that often rely on battery power, such as laptops.&lt;/p&gt;&lt;p&gt;“We have to be very deliberate in how we design our &lt;a href="https://spectrum.ieee.org/tag/system-on-a-chip"&gt;system-on-a-chip&lt;/a&gt; to ensure that a larger &lt;a href="https://spectrum.ieee.org/tag/soc"&gt;SoC&lt;/a&gt; can perform to our requirements in a thin and light form factor,” said&lt;a href="https://www.linkedin.com/in/mahesh-subramony-a0ba60/"&gt; Mahesh Subramony&lt;/a&gt;, senior fellow design engineer at AMD.&lt;/p&gt;&lt;h2&gt;When it comes to AI, memory matters&lt;/h2&gt;&lt;p&gt;Squeezing an NPU alongside a CPU and GPU will improve the average PC’s performance in AI tasks, but it’s not the only revolutionary change AI will force on PC architecture. There’s another that’s perhaps even more fundamental: memory.&lt;/p&gt;&lt;p&gt;Most modern PCs have a divided memory architecture &lt;a href="https://www.electronicdesign.com/technologies/embedded/article/55300900/jon-peddie-research-what-came-before-pcie-the-evolution-of-pc-graphics-buses"&gt;rooted in decisions made over 25 years ago&lt;/a&gt;. Limitations in bus bandwidth led GPUs (and other add-in cards that might require high-bandwidth memory) to move away from accessing a PC’s system memory and instead rely on the GPU’s own dedicated memory. As a result, powerful PCs typically have two pools of memory, system memory and graphics memory, which operate independently.&lt;/p&gt;&lt;p&gt;That’s a problem for AI. Models require large amounts of memory, and the entire model must load into memory at once. The legacy PC architecture, which splits memory between the system and the GPU, is at odds with that requirement.&lt;/p&gt;&lt;p&gt;“When I have a discrete GPU, I have a separate memory subsystem hanging off it,” explained &lt;a href="https://www.linkedin.com/in/joseph-macri-9a288a55/"&gt;Joe Macri, &lt;/a&gt; vice president and chief technology officer at AMD. “When I want to share data between our [CPU] and GPU, I’ve got to take the data out of my memory, slide it across the PCI Express bus, put it in the GPU memory, do my processing, then move it all back.” Macri said this increases power draw and leads to a sluggish &lt;a href="https://spectrum.ieee.org/tag/user-experience"&gt;user experience&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The solution is a unified memory architecture that provides all system resources access to the same pool of memory over a fast, interconnected memory bus. Apple’s in-house silicon is perhaps the most well-known recent example of a chip with a unified memory architecture. However, unified memory is otherwise rare in modern PCs.&lt;/p&gt;&lt;p&gt;AMD is following suit in the laptop space. The company announced a new line of APUs targeted at high-end laptops, &lt;a href="https://www.amd.com/en/products/processors/laptop/ryzen/ai-300-series/amd-ryzen-ai-max-plus-395.html"&gt;Ryzen AI Max&lt;/a&gt;, at &lt;a href="https://spectrum.ieee.org/tag/ces"&gt;CES&lt;/a&gt; (&lt;a href="https://spectrum.ieee.org/topic/consumer-electronics/"&gt;Consumer Electronics&lt;/a&gt; Show) 2025.&lt;/p&gt;&lt;p&gt;Ryzen AI Max places the company’s Ryzen CPU cores on the same silicon as Radeon-branded GPU cores, plus an NPU rated at 50 TOPS, on a single piece of silicon with a unified memory architecture. Because of this, the CPU, GPU, and NPU can all access up to a maximum of &lt;a href="https://www.amd.com/en/developer/resources/technical-articles/2025/amd-ryzen-ai-max-395--a-leap-forward-in-generative-ai-performanc.html"&gt;128 GB of system memory&lt;/a&gt;, which is shared among all three. AMD believes this strategy is ideal for memory and performance management in consumer PCs. “By bringing it all under a single thermal head, the entire power envelope becomes something that we can manage,” said Subramony.&lt;/p&gt;&lt;p&gt;The Ryzen AI Max is already available in several laptops, including&lt;a href="https://www.pcworld.com/article/2650073/hands-on-the-hp-zbook-ultra-g1a-smashes-the-work-laptop-paradigm.html"&gt; the HP Zbook Ultra G1a&lt;/a&gt; and the &lt;a href="https://rog.asus.com/laptops/rog-flow/rog-flow-z13-2025/"&gt;Asus ROG Flow Z13&lt;/a&gt;. It also powers the&lt;a href="https://frame.work/marketplace/desktops"&gt; Framework Desktop&lt;/a&gt; and several mini desktops from less well-known brands, such as the &lt;a href="https://www.gmktec.com/products/amd-ryzen%E2%84%A2-ai-max-395-evo-x2-ai-mini-pc?srsltid=AfmBOooME4uCrsnIh5mOf98eGHteIzsi-DAPl6E5xhNrTzG94qr3Tjf6"&gt;GMKtec EVO-X2 AI mini PC&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Intel and Nvidia will also join this party, though in an unexpected way. In September, the former rivals announced an alliance to sell chips that pair Intel CPU cores with Nvidia GPU cores. While the details are still under wraps, the chip architecture will likely include unified memory and an Intel NPU.&lt;/p&gt;&lt;p&gt;Chips like these stand to drastically change PC architecture if they catch on. They’ll offer access to much larger pools of memory than before and integrate the CPU, GPU, and NPU into one piece of silicon that can be closely monitored and controlled. These factors should make it easier to shuffle an AI workload to the hardware best suited to execute it at a given moment.&lt;/p&gt;&lt;p&gt;Unfortunately, they’ll also make PC upgrades and repairs more difficult, as chips with a unified memory architecture typically bundle the CPU, GPU, NPU, and memory into a single, physically inseparable package on a PC mainboard. That’s in contrast with traditional PCs, where the CPU, GPU, and memory can be replaced individually.&lt;/p&gt;&lt;h2&gt;Microsoft’s bullish take on AI is rewriting Windows&lt;/h2&gt;&lt;p&gt;MacOS is well regarded for its attractive, intuitive &lt;a href="https://spectrum.ieee.org/tag/user-interface"&gt;user interface&lt;/a&gt;, and Apple Silicon chips have a unified memory architecture that can prove useful for AI. However, Apple’s GPUs aren’t as capable as the best ones used in PCs, and its AI tools for developers are less widely adopted.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.linkedin.com/in/chrissiecremers/?originalSubdomain=nl"&gt;Chrissie Cremers&lt;/a&gt;, cofounder of the AI-focused marketing firm Aigency Amsterdam, told me earlier this year that although she prefers macOS, her agency doesn’t use Mac computers for AI work. “The GPU in my Mac desktop can hardly manage [our AI workflow], and it’s not an old computer,” she said. “I’d love for them to catch up here, because they used to be the creative tool.”&lt;/p&gt;&lt;p&gt; &lt;img alt="Laptop beneath glass dome shaped like human head on striped orange and blue background." src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201200%201200'%3E%3C/svg%3E"/&gt;  Dan Page&lt;/p&gt;&lt;p&gt;That leaves an opening for competitors to become the go-to choice for AI on the PC—and Microsoft knows it.&lt;/p&gt;&lt;p&gt;Microsoft launched &lt;a href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/"&gt;Copilot+ PCs&lt;/a&gt; at the company’s 2024 Build developer conference. The launch had problems, most notably the &lt;a href="https://www.theverge.com/2024/6/13/24178144/microsoft-windows-ai-recall-feature-delay"&gt;botched&lt;/a&gt; release of its key feature,&lt;a href="https://spectrum.ieee.org/microsoft-copilot"&gt; Windows Recall&lt;/a&gt;, which uses AI to help users search through anything they’ve seen or heard on their PC. Still, the launch was successful in pushing the PC industry toward NPUs, as AMD and Intel both introduced new laptop chips with upgraded NPUs in late 2024.&lt;/p&gt;&lt;p&gt;At Build 2025, Microsoft also revealed &lt;a href="https://devblogs.microsoft.com/foundry/foundry-local-a-new-era-of-edge-ai/"&gt;Windows’ AI Foundry Local&lt;/a&gt;, a “runtime stack” that includes a catalog of popular open-source &lt;a href="https://spectrum.ieee.org/tag/large-language-models"&gt;large language models&lt;/a&gt;. While Microsoft’s own models are available,&lt;a href="https://azure.microsoft.com/en-us/products/ai-model-catalog#tabs-pill-bar-oc92d8_tab0"&gt; the catalog includes thousands of open-source models&lt;/a&gt; from &lt;a href="https://spectrum.ieee.org/tag/alibaba"&gt;Alibaba&lt;/a&gt;, DeepSeek, &lt;a href="https://spectrum.ieee.org/tag/meta"&gt;Meta&lt;/a&gt;, Mistral AI, Nvidia, &lt;a href="https://spectrum.ieee.org/tag/openai"&gt;OpenAI&lt;/a&gt;, Stability AI, xAI, and more.&lt;/p&gt;&lt;p&gt;Once a model is selected and implemented into an app, Windows executes AI tasks on local hardware through the Windows ML runtime, which automatically directs AI tasks to the CPU, GPU, or NPU hardware best suited for the job.&lt;/p&gt;&lt;p&gt;AI &lt;a href="https://spectrum.ieee.org/tag/foundry"&gt;Foundry&lt;/a&gt; also provides APIs for local knowledge retrieval and low-rank adaptation (LoRA), advanced features that let developers customize the data an AI model can reference and how it responds. Microsoft also announced support for on-device semantic search and retrieval-augmented generation, features that help developers build AI tools that reference specific on-device information.&lt;/p&gt;&lt;p&gt;“[AI Foundry] is about being smart. It’s about using all the &lt;a href="https://spectrum.ieee.org/tag/processors"&gt;processors&lt;/a&gt; at hand, being efficient, and prioritizing workloads across the CPU, the NPU, and so on. There’s a lot of opportunity and runway to improve,” said Bathiche.&lt;/p&gt;&lt;h3&gt;Toward &lt;a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence"&gt;AGI&lt;/a&gt; on PCs&lt;/h3&gt;&lt;p&gt;The rapid evolution of AI-capable PC hardware represents more than just an incremental upgrade. It signals a coming shift in the PC industry that’s likely to wipe away the last vestiges of the PC architectures designed in the ’80s, ’90s, and early 2000s.&lt;/p&gt;&lt;p&gt;The combination of increasingly powerful NPUs, unified memory architectures, and sophisticated software-optimization techniques is closing the performance gap between local and cloud-based AI at a pace that has surprised even industry insiders, such as Bathiche.&lt;/p&gt;&lt;p&gt;It will also nudge chip designers toward ever-more-integrated chips that have a unified memory subsystem and to bring the CPU, GPU, and NPU onto a single piece of silicon—even in high-end laptops and desktops. AMD’s Subramony said the goal is to have users “carrying a mini workstation in your hand, whether it’s for AI workloads, or for high compute. You won’t have to go to the cloud.”&lt;/p&gt;&lt;p&gt;A change that massive won’t happen overnight. Still, it’s clear that many in the PC industry are committed to reinventing the computers we use every day in a way that optimizes for AI. Qualcomm’s Vinesh Sukumar even believes affordable consumer laptops, much like data centers, should aim for &lt;a href="https://spectrum.ieee.org/tag/agi"&gt;AGI&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;“I want a complete &lt;a href="https://spectrum.ieee.org/tag/artificial-general-intelligence"&gt;artificial general intelligence&lt;/a&gt; running on Qualcomm devices,” he said. “That’s what we’re trying to push for.” &lt;/p&gt;&lt;p&gt;&lt;em&gt;This article appears in the December 2025 print issue.&lt;/em&gt;&lt;/p&gt;From Your Site Articles&lt;ul&gt;&lt;li&gt;&lt;a href="https://spectrum.ieee.org/personal-ai-assistant"&gt;When AI Unplugs, All Bets Are Off ›&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://spectrum.ieee.org/agentic-ai-opera-mini"&gt;Opera Includes AI Agents in Latest Web Browser ›&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;Related Articles Around the Web&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/16y95hk/a_starter_guide_for_playing_with_your_own_local_ai/"&gt;A Starter Guide for Playing with Your Own Local AI! : r/LocalLLaMA ›&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://medium.com/@springrod/why-you-should-use-local-models-a3fce1124c94"&gt;Why You Should Use Local Models. When building Gen AI ... ›&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;a href="https://spectrum.ieee.org/tag/large-language-models"&gt;large language models&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/laptops"&gt;laptops&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/amd"&gt;amd&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/apple"&gt;apple&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/microsoft"&gt;microsoft&lt;/a&gt;&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://spectrum.ieee.org/ai-models-locally</guid>
      <category>Hacker News</category>
      <pubDate>Tue, 23 Dec 2025 00:12:16 +0000</pubDate>
    </item>
    <item>
      <title>It's Always TCP_NODELAY</title>
      <link>https://brooker.co.za/blog/2024/05/09/nagle.html</link>
      <description>It's not the 1980s anymore, thankfully.</description>
      <content:encoded>&lt;body&gt;


&lt;h1&gt;&lt;a href="https://brooker.co.za/blog/"&gt;Marc's Blog&lt;/a&gt;&lt;/h1&gt;


&lt;h1&gt;About Me&lt;/h1&gt;
    My name is Marc Brooker. I've been writing code, reading code, and living vicariously through computers for as long as I can remember. I like to build things that work. I also dabble in machining, welding, cooking and skiing.&lt;br/&gt;&lt;br/&gt;

    I'm currently an engineer at Amazon Web Services (AWS) in Seattle, where I work on databases, serverless, and serverless databases. Before that, I worked on EC2 and EBS.&lt;br/&gt;

All opinions are my own.
    &lt;h1&gt;Links&lt;/h1&gt;
&lt;a href="https://brooker.co.za/blog/publications.html"&gt;My Publications and Videos&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://fediscience.org/@marcbrooker"&gt;@marcbrooker on Mastodon&lt;/a&gt;
&lt;a href="https://twitter.com/MarcJBrooker"&gt;@MarcJBrooker on Twitter&lt;/a&gt;


&lt;h1&gt;Itâs always TCP_NODELAY. Every damn time.&lt;/h1&gt;
&lt;p&gt;It's not the 1980s anymore, thankfully.&lt;/p&gt;
&lt;p&gt;The first thing I check when debugging latency issues in distributed systems is whether &lt;a href="https://linux.die.net/man/7/tcp"&gt;TCP_NODELAY&lt;/a&gt; is enabled. And itâs not just me. Every distributed system builder I know has lost hours to latency issues quickly fixed by enabling this simple socket option, suggesting that the default behavior is wrong, and perhaps that the whole concept is outmoded.&lt;/p&gt;
&lt;p&gt;First, letâs be clear about what weâre talking about. Thereâs no better source than John Nagleâs &lt;a href="https://datatracker.ietf.org/doc/html/rfc896"&gt;RFC896&lt;/a&gt; from 1984&lt;a href="#foot1"&gt;1&lt;/a&gt;. First, the problem statement:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There is a special problem associated with small  packets.   When TCP  is  used  for  the transmission of single-character messages originating at a keyboard, the typical result  is  that  41  byte packets (one  byte  of data, 40 bytes of header) are transmitted for each byte of useful data.  This 4000%  overhead  is  annoying but tolerable on lightly loaded networks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In short, Nagle was interested in better amortizing the cost of TCP headers, to get better throughput out of the network. Up to 40x better throughput! These tiny packets had two main causes: human-interactive applications like shells, where folks were typing a byte at a time, and poorly implemented programs that dribbled messages out to the kernel through many &lt;code&gt;write&lt;/code&gt; calls. Nagleâs proposal for fixing this was simple and smart:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A  simple and elegant solution has been discovered.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The solution is to inhibit the sending of new TCP  segments  when new  outgoing  data  arrives  from  the  user  if  any previously transmitted data on the connection remains unacknowledged.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When many people talk about Nagleâs algorithm, they talk about timers, but RFC896 doesnât use any kind of timer other than the round-trip time on the network.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Nagleâs Algorithm and Delayed Acks&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Nagleâs nice, clean, proposal interacted poorly with another TCP feature: delayed &lt;code&gt;ACK&lt;/code&gt;. The idea behind delayed &lt;code&gt;ACK&lt;/code&gt; is to delay sending the acknowledgement of a packet at least until thereâs some data to send back (e.g. a &lt;code&gt;telnet&lt;/code&gt; session echoing back the userâs typing), or until a timer expires. &lt;a href="https://datatracker.ietf.org/doc/html/rfc813"&gt;RFC813&lt;/a&gt; from 1982 is that first that seems to propose delaying &lt;code&gt;ACKs&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The receiver of data will   refrain   from   sending   an   acknowledgement   under   certain circumstances, in which case it must set a timer which  will  cause  the acknowledgement  to be sent later.  However, the receiver should do this only where it is a reasonable guess that some other event will intervene and prevent the necessity of the timer  interrupt.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;which is then formalized further in &lt;a href="https://datatracker.ietf.org/doc/html/rfc1122"&gt;RFC1122&lt;/a&gt; from 1989. The interaction between these two features causes a problem: Nagleâs algorithm is blocking sending more data until an &lt;code&gt;ACK&lt;/code&gt; is received, but delayed ack is delaying that &lt;code&gt;ack&lt;/code&gt; until a response is ready. Great for keeping packets full, not so great for latency-sensitive pipelined applications.&lt;/p&gt;
&lt;p&gt;This is a point Nagle has made himself several times. For example in this &lt;a href="https://news.ycombinator.com/item?id=10608356"&gt;Hacker News comment&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;That still irks me. The real problem is not tinygram prevention. Itâs ACK delays, and that stupid fixed timer. They both went into TCP around the same time, but independently. I did tinygram prevention (the Nagle algorithm) and Berkeley did delayed ACKs, both in the early 1980s. The combination of the two is awful.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As systems builders this is should be a familiar situation: two reasonable features of the system that interact to create an undesirable behavior. This kind of interaction is one of the things that makes protocol design so hard.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Is Nagle blameless?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, itâs not just delayed ACK&lt;a href="#foot2"&gt;2&lt;/a&gt;. Even without delayed ack and that &lt;em&gt;stupid fixed timer&lt;/em&gt;, the behavior of Nagleâs algorithm probably isnât what we want in distributed systems. A single in-datacenter RTT is typically around 500Î¼s, then a couple of milliseconds between datacenters in the same region, and up to hundreds of milliseconds going around the globe. Given the vast amount of work a modern server can do in even a few hundred microseconds, delaying sending data for even one RTT isnât clearly a win.&lt;/p&gt;
&lt;p&gt;To make a clearer case, letâs turn back to the justification behind Nagleâs algorithm: amortizing the cost of headers and avoiding that 40x overhead on single-byte packets. But does anybody send single byte packets anymore? Most distributed databases and systems donât. Partially thatâs because they simply have more to say, partially its because of additional overhead of protocols like TLS, and partially its because of encoding and serialization overhead. But mostly, they have more to say.&lt;/p&gt;
&lt;p&gt;The core concern of not sending tiny messages is still a very real one, but weâve very effectively pushed that into the application layer. Sending a byte at a time wrapped in JSON isnât going to be very efficient, no matter what Nagleâs algorithm does.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Is Nagle needed?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;First, the uncontroversial take: if youâre building a latency-sensitive distributed system running on modern datacenter-class hardware, enable &lt;code&gt;TCP_NODELAY&lt;/code&gt; (disable Nagleâs algorithm) without worries. You donât need to feel bad. Itâs not a sin. Itâs OK. Just go ahead.&lt;/p&gt;
&lt;p&gt;More controversially, I suspect that Nagleâs algorithm just isnât needed on modern systems, given the traffic and application mix, and the capabilities of the hardware we have today. In other words, &lt;code&gt;TCP_NODELAY&lt;/code&gt; should be the default. Thatâs going to make some â&lt;code&gt;write&lt;/code&gt; every byteâ code slower than it would otherwise be, but those applications should be fixed anyway if we care about efficiency.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Footnotes&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a&gt;&lt;/a&gt; I wonât got into it here, but RFC896 is also one of the earliest statements I can find of metastable behavior in computer networks. In it, Nagle says: âThis condition is stable. Once the  saturation point has been reached, if the algorithm for selecting packets to be dropped is fair, the network will continue to operate in a degraded condition.â&lt;/li&gt;
&lt;li&gt;&lt;a&gt;&lt;/a&gt; As this has gone around the internet, a number of folks have asked about &lt;code&gt;TCP_QUICKACK&lt;/code&gt;. I donât tend to reach for it for a few reasons, including lack of portability, and weird semantics (seriously, read &lt;a href="https://linux.die.net/man/7/tcp"&gt;the man page&lt;/a&gt;). The bigger problem is that &lt;code&gt;TCP_QUICKACK&lt;/code&gt; doesnât fix the fundamental problem of the kernel hanging on to data longer than my program wants it to. When I say &lt;code&gt;write()&lt;/code&gt;, I mean &lt;code&gt;write()&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;


  « &lt;a href="https://brooker.co.za/blog"&gt;Back to the blog index&lt;/a&gt;&lt;br/&gt;
&lt;br/&gt;
&lt;!-- Similar Posts --&gt;
&lt;h4&gt;Similar Posts&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;21 Oct 2022 » &lt;a href="https://brooker.co.za/blog/2022/10/21/nudge.html"&gt;Give Your Tail a Nudge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;10 May 2014 » &lt;a href="https://brooker.co.za/blog/2014/05/10/lynch-pub.html"&gt;The Essential Nancy Lynch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;20 May 2025 » &lt;a href="https://brooker.co.za/blog/2025/05/20/icpe.html"&gt;Good Performance for Bad Days&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Dissimilar Posts --&gt;
&lt;h4&gt;Something Completely Different&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;11 Apr 2015 » &lt;a href="https://brooker.co.za/blog/2015/04/11/zero-one.html"&gt;The Zero, One, Infinity Disease&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;
        Marc Brooker&lt;br&gt;
        The opinions on this site are my own. They do not necessarily represent those of my employer.&lt;br&gt;
        marcbrooker@gmail.com
      &lt;/br&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href="https://brooker.co.za/blog/rss.xml"&gt;&lt;img src="https://brooker.co.za/blog/images/feed-icon-14x14.png"/&gt; RSS&lt;/a&gt;
&lt;a href="https://brooker.co.za/blog/atom.xml"&gt;&lt;img src="https://brooker.co.za/blog/images/feed-icon-14x14.png"/&gt; Atom&lt;/a&gt;
&lt;/p&gt;


&lt;!-- &lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;&lt;img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /&gt;&lt;/a&gt;&lt;br /&gt; --&gt;
      This work is licensed under a &lt;a href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.
    


&lt;/body&gt;</content:encoded>
      <guid isPermaLink="false">https://brooker.co.za/blog/2024/05/09/nagle.html</guid>
      <category>Hacker News</category>
      <pubDate>Mon, 22 Dec 2025 21:09:59 +0000</pubDate>
    </item>
    <item>
      <title>Ultrasound Cancer Treatment: Sound Waves Fight Tumors</title>
      <link>https://spectrum.ieee.org/ultrasound-cancer-treatment</link>
      <description>HistoSonics turns its tumor-liquifying tech against pancreatic cancer</description>
      <content:encoded>&lt;article class="clearfix image-article sm-mb-1 quality-HD post-2674366292" data-category="Biomedical" data-frozen-sections="[]" elid="2674366292"&gt;&lt;a href="https://spectrum.ieee.org/topic/biomedical/"&gt;Biomedical&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/magazine/"&gt;Magazine&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/type/feature/"&gt;Feature&lt;/a&gt;&lt;h1&gt;
        Ultrasound Treatment Takes on Cancer’s Toughest Tumors
    &lt;/h1&gt;&lt;h2&gt;&lt;p&gt;HistoSonics turns its tumor-liquifying tech against pancreatic cancer&lt;/p&gt;&lt;/h2&gt;&lt;a href="https://spectrum.ieee.org/u/greg-uyeno"&gt;Greg Uyeno&lt;/a&gt;22 Dec 20253 min read&lt;!-- EMAIL --&gt;&lt;a href="mailto:?subject=Ultrasound%20Treatment%20Takes%20on%20Cancer%E2%80%99s%20Toughest%20Tumors&amp;amp;body=https://spectrum.ieee.org/ultrasound-cancer-treatment"&gt;&lt;/a&gt;&lt;!-- COPY LINK --&gt;&lt;!-- TWITTER --&gt;&lt;!-- FACEBOOK --&gt;&lt;!-- LINKEDIN --&gt;&lt;img alt="Illustration of the HistoSonics device over a patient’s abdomen, sending ultrasound through a water-filled membrane into the body. Three insets show a tumor as bubbles form, expand, and break the tumor apart into liquefied tissue." src="https://spectrum.ieee.org/media-library/illustration-of-the-histosonics-device-over-a-patients-abdomen-sending-ultrasound-through-a-water-filled-membrane-into-the-bod.png?id=62599195&amp;amp;width=1199&amp;amp;height=1280"/&gt;&lt;p&gt;HistoSonics’ Edison system uses a water-filled membrane to transmit focused ultrasound into the body. The resulting bubbles expand and collapse within the tumor, producing mechanical stress that destroys cancer cells and liquefies the tumor.&lt;/p&gt;Gyginfographics.com&lt;p&gt;&lt;strong&gt;For many years,&lt;/strong&gt; doctors and technicians who performed medical &lt;a href="https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes"&gt;ultrasound&lt;/a&gt; procedures viewed bubbles with wary concern. The phenomenon of cavitation—the formation and collapse of tiny gas bubbles due to changes in pressure—was considered an undesirable and largely uncontrollable side effect. But in 2001, researchers at the University of Michigan began exploring ways to harness the phenomenon for the destruction of cancerous tumors and other problematic tissue.&lt;/p&gt;&lt;p&gt;The trouble was, creating and controlling cavitation generated heat, which harmed healthy tissue beyond the target area. &lt;a href="https://bme.umich.edu/people/xu-zhen/"&gt;Zhen Xu&lt;/a&gt;, who was working on a Ph.D. in &lt;a href="https://spectrum.ieee.org/tag/biomedical-engineering"&gt;biomedical engineering&lt;/a&gt; at the time, was bombarding pig heart tissue in a tank of water with &lt;a href="https://spectrum.ieee.org/tag/ultrasound"&gt;ultrasound&lt;/a&gt; when she made a breakthrough.&lt;/p&gt;&lt;p&gt;The key was using extremely powerful ultrasound to produce negative pressure of more than 20 megapascals, delivered in short bursts measured in microseconds—but separated by relatively long gaps, between a millisecond and a full second long. These parameters created bubbles that quickly formed and collapsed, tearing apart nearby cells and turning the tissue into a kind of slurry, while avoiding heat buildup. The result was a form of incisionless &lt;a href="https://spectrum.ieee.org/tag/surgery"&gt;surgery&lt;/a&gt;, a way to wipe out tumors without scalpels, &lt;a href="https://spectrum.ieee.org/tag/radiation"&gt;radiation&lt;/a&gt;, or heat.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“The experiments worked,” says Xu, now a professor at Michigan, “but I also destroyed the ultrasound equipment that I used,” which was the most powerful available at the time. In 2009, she cofounded a company, &lt;a href="https://histosonics.com/"&gt;HistoSonics&lt;/a&gt;, to commercialize more powerful ultrasound machines, test treatment of a variety of diseases, and make the procedure, called histotripsy, widely available.&lt;/p&gt;&lt;p&gt;So far, the killer app is fighting &lt;a href="https://spectrum.ieee.org/tag/cancer"&gt;cancer&lt;/a&gt;. In 2023, HistoSonics’ Edison system received &lt;a href="https://spectrum.ieee.org/tag/fda"&gt;FDA&lt;/a&gt; approval for &lt;a href="https://histosonics.com/news/fda-awards-histosonics-clearance-of-its-first-of-a-kind-edison-histotripsy-system-2/"&gt;treatment of liver tumors&lt;/a&gt;. In 2026, clinicians will conclude a &lt;a href="https://histosonics.com/news/worlds-first-kidney-tumor-treated-using-the-histosonics-edison-histotripsy-system/"&gt;pivotal kidney cancer study&lt;/a&gt; and apply for regulatory approval. They’ll also launch a large-scale pivotal trial for pancreatic &lt;a href="https://spectrum.ieee.org/tag/cancer"&gt;cancer&lt;/a&gt;, considered one of the deadliest forms of the disease with a five-year survival rate of just &lt;a href="https://pancan.org/press-releases/pancreatic-cancer-diagnoses-and-mortality-rates-climb-five-year-survival-rate-for-pancreatic-cancer-stalls-at-13/"&gt;13 percent&lt;/a&gt;. An effective treatment for pancreatic cancer would represent a major advance against one of the most lethal malignancies.&lt;/p&gt;&lt;h2&gt;Histotripsy’s Benefits for Cancer Treatment&lt;/h2&gt;&lt;p&gt;HistoSonics is not the only developer of histotripsy devices or techniques, but it is first to market with a purpose-built device. “What HistoSonics has developed is a symphony of technologies, which combines physics, biology, and biomedical engineering,” says &lt;a href="https://www.cc.nih.gov/meet-our-doctors/bwood"&gt;Bradford Wood&lt;/a&gt;, an interventional radiologist at the National Institutes of Health, who is not affiliated with the company. Its engineering effort has spanned multiple disciplines to produce robotic, computer-guided systems that turn physical forces into therapeutic effects.&lt;/p&gt;&lt;p&gt;Over the past decade, research has confirmed or found other benefits of histotripsy. With precise calibration, fibrous tissue—such as blood vessels—can be spared from damage even in the target zone. And while other noninvasive techniques may leave scar tissue, the liquefied debris created by histotripsy is cleared away by the body’s natural processes.&lt;/p&gt;&lt;p&gt;In HistoSonics’ early trials for pancreatic cancer, doctors used &lt;a href="https://spectrum.ieee.org/tag/focused-ultrasound"&gt;focused ultrasound&lt;/a&gt; pulses to ablate, or destroy, tumors deep within the pancreas. “It’s a great achievement for the entire field to show that it is possible to ablate pancreatic tumors and that it’s well tolerated,” says &lt;a href="https://gastro.uw.edu/people/faculty/khokhlova-t"&gt;Tatiana Khokhlova&lt;/a&gt;, a medical ultrasound researcher at the University of Washington, in Seattle, who has worked on alternative histotripsy techniques.&lt;/p&gt;&lt;p&gt;Khokhlova says the key to harnessing histotripsy’s benefits “will be combining ablation of the primary tumor in the pancreas with some other therapy.” Combination treatment could fight recurrent cancer and tiny tumors that ultrasound might miss, while also tapping into a surprising benefit.&lt;/p&gt;&lt;p&gt;Histotripsy generally seems to &lt;a href="https://pubmed.ncbi.nlm.nih.gov/31940590/"&gt;stimulate an immune response&lt;/a&gt;, helping the body attack cancer cells that weren’t targeted directly by ultrasound. The mechanical destruction of tumors likely leaves behind recognizable traces of cancer proteins that help the immune system learn to identify and destroy similar cells elsewhere in the body, explains Wood. Researchers are now exploring ways to pair histotripsy with immunotherapy to amplify that effect.&lt;/p&gt;&lt;p&gt;The company’s capacity to explore the treatment‘s potential for different conditions will only improve with time, says HistoSonics CEO &lt;a href="https://www.linkedin.com/in/mike-blue-860b9522/"&gt;Mike Blue&lt;/a&gt;. The company has fresh resources to accelerate R&amp;amp;D: A new ownership group, which includes billionaire &lt;a href="https://spectrum.ieee.org/tag/jeff-bezos"&gt;Jeff Bezos&lt;/a&gt;, &lt;a href="https://www.businesswire.com/news/home/20250807749442/en/HistoSonics-Announces-%242.25B-Acquisition-by-Consortium-of-Top-Tier-Investors"&gt;acquired&lt;/a&gt; HistoSonics in August 2025 at a valuation of US $2.25 billion. &lt;/p&gt;&lt;p&gt;Engineers are already testing a new guidance system that uses a form of &lt;a href="https://spectrum.ieee.org/tag/x-rays"&gt;X-rays&lt;/a&gt; rather than &lt;a href="https://spectrum.ieee.org/tag/ultrasound-imaging"&gt;ultrasound imaging&lt;/a&gt;, which should expand use cases. The R&amp;amp;D team is also developing a feedback system that analyzes echoes from the therapeutic ultrasound to detect tissue destruction and integrates that information into the live display, says Blue.&lt;/p&gt;&lt;p&gt;If those advances pan out, histotripsy could move well beyond the liver, kidney, and pancreas in the fight against cancer. What started as a curiosity about bubbles might soon become a new pillar of noninvasive medicine—a future in which surgeons wield not scalpels, but sound waves.&lt;/p&gt;From Your Site Articles&lt;ul&gt;&lt;li&gt;&lt;a href="https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes"&gt;Doctors Could Hack the Nervous System With Ultrasound ›&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://spectrum.ieee.org/cancer-ultrasound-blood-brain-barrier"&gt;Ultrasound Opens New Front Against Lethal Brain Cancer in Kids ›&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;Related Articles Around the Web&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.uchicagomedicine.org/cancer/types-treatments/histotripsy"&gt;Histotripsy - Liver Cancer Ultrasound - UChicago Medicine ›&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://news.umich.edu/tumor-destroying-sound-waves-receive-fda-approval-for-liver-treatment-in-humans/"&gt;Tumor-destroying sound waves receive FDA approval for liver ... ›&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;a href="https://spectrum.ieee.org/tag/cancer"&gt;cancer&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/cancer-therapy"&gt;cancer therapy&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/focused-ultrasound"&gt;focused ultrasound&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/immune-system"&gt;immune system&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/pancreas"&gt;pancreas&lt;/a&gt;&lt;a href="https://spectrum.ieee.org/tag/ultrasound"&gt;ultrasound&lt;/a&gt;&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://spectrum.ieee.org/ultrasound-cancer-treatment</guid>
      <category>Hacker News</category>
      <pubDate>Mon, 22 Dec 2025 19:37:34 +0000</pubDate>
    </item>
    <item>
      <title>The Illustrated Transformer</title>
      <link>https://jalammar.github.io/illustrated-transformer/</link>
      <description>Discussions: Hacker News (65 points, 4 comments) , Reddit r/MachineLearning (29 points, 3 comments) Translations: Arabic , Chinese (Simplified) 1 , Chinese (Simplified) 2 , French 1 , French 2 , Italian , Japanese , Korean , Persian , Russian , Spanish 1 , Spanish 2 , Vietnamese Watch: MIT’s Deep Learning State of the Art lecture referencing this post Featured in courses at Stanford , Harvard , MIT , Princeton , CMU and others</description>
      <content:encoded>&lt;article class="post"&gt;
&lt;h1&gt;The Illustrated Transformer&lt;/h1&gt;

&lt;p&gt;Discussions:
&lt;a href="https://news.ycombinator.com/item?id=18351674"&gt;Hacker News (65 points, 4 comments)&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/MachineLearning/comments/8uh2yz/p_the_illustrated_transformer_a_visual_look_at/"&gt;Reddit r/MachineLearning (29 points, 3 comments)&lt;/a&gt;

&lt;br/&gt;
Translations: &lt;a href="https://www.mundhor.site/post/post14"&gt;Arabic&lt;/a&gt;, &lt;a href="https://blog.csdn.net/yujianmin1990/article/details/85221271"&gt;Chinese (Simplified) 1&lt;/a&gt;, &lt;a href="https://blog.csdn.net/qq_36667170/article/details/124359818"&gt;Chinese (Simplified) 2&lt;/a&gt;, &lt;a href="https://a-coles.github.io/2020/11/15/transformer-illustre.html"&gt;French 1&lt;/a&gt;, &lt;a href="https://lbourdois.github.io/blog/nlp/Transformer/"&gt;French 2&lt;/a&gt;, &lt;a href="https://medium.com/@val.mannucci/il-transformer-illustrato-it-37a78e3e2348"&gt;Italian&lt;/a&gt;, &lt;a href="https://tips-memo.com/translation-jayalmmar-transformer"&gt;Japanese&lt;/a&gt;, &lt;a href="https://nlpinkorean.github.io/illustrated-transformer/"&gt;Korean&lt;/a&gt;, &lt;a href="http://dml.qom.ac.ir/2022/05/17/illustrated-transformer/"&gt;Persian&lt;/a&gt;, &lt;a href="https://habr.com/ru/post/486358/"&gt;Russian&lt;/a&gt;, &lt;a href="https://www.ibidemgroup.com/edu/transformer-ilustrado-jay-alammar/"&gt;Spanish 1&lt;/a&gt;, &lt;a href="https://hackernoon.com/el-transformador-ilustrado-una-traduccion-al-espanol-0y73wwp"&gt;Spanish 2&lt;/a&gt;, &lt;a href="https://trituenhantao.io/tin-tuc/minh-hoa-transformer/"&gt;Vietnamese&lt;/a&gt;
&lt;br/&gt;
Watch: MIT’s &lt;a href="https://youtu.be/53YvP6gdD7U?t=432"&gt;Deep Learning State of the Art&lt;/a&gt; lecture referencing this post
&lt;br/&gt;
Featured in courses at &lt;a href="https://web.stanford.edu/class/cs224n/"&gt;Stanford&lt;/a&gt;, &lt;a href="https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/transformers"&gt;Harvard&lt;/a&gt;, &lt;a href="https://ocw.mit.edu/courses/6-s897-machine-learning-for-healthcare-spring-2019/d39a6eed387ee90b1f72a01949c35c7b_MIT6_S897S19_lec8.pdf"&gt;MIT&lt;/a&gt;, &lt;a href="https://www.cs.princeton.edu/courses/archive/fall22/cos597G/"&gt;Princeton&lt;/a&gt;, &lt;a href="https://www.cs.cmu.edu/~leili/course/mldl22w/14-Transformer.pdf"&gt;CMU&lt;/a&gt; and others&lt;/p&gt;



&lt;a href="https://www.llm-book.com"&gt;&lt;img src="https://github.com/user-attachments/assets/a471dfff-00cc-4cb4-8df5-123e195bcc71"/&gt;&lt;/a&gt;

Update: This post has now become a book! Check out &lt;a href="https://llm-book.com"&gt;LLM-book.com&lt;/a&gt; which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).
  


&lt;p&gt;In the &lt;a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"&gt;previous post, we looked at Attention&lt;/a&gt; – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at &lt;strong&gt;The Transformer&lt;/strong&gt; – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their &lt;a href="https://cloud.google.com/tpu/"&gt;Cloud TPU&lt;/a&gt; offering. So let’s try to break the model apart and look at how it functions.&lt;/p&gt;
&lt;p&gt;The Transformer was proposed in the paper &lt;a href="https://arxiv.org/abs/1706.03762"&gt;Attention is All You Need&lt;/a&gt;. A TensorFlow implementation of it is available as a part of the &lt;a href="https://github.com/tensorflow/tensor2tensor"&gt;Tensor2Tensor&lt;/a&gt; package. Harvard’s NLP group created a &lt;a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html"&gt;guide annotating the paper with PyTorch implementation&lt;/a&gt;. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2025 Update&lt;/strong&gt;: We’ve built a &lt;a href="https://bit.ly/4aRnn7Z"&gt;free short course&lt;/a&gt; that brings the contents of this post up-to-date with animations:&lt;/p&gt;



&lt;h2&gt;A High-Level Look&lt;/h2&gt;
&lt;p&gt;Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/the_transformer_3.png"/&gt;

&lt;!--more--&gt;
&lt;p&gt;Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png"/&gt;

&lt;p&gt;The encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png"/&gt;

&lt;p&gt;The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/Transformer_encoder.png"/&gt;

&lt;p&gt;The encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.&lt;/p&gt;
&lt;p&gt;The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.&lt;/p&gt;
&lt;p&gt;The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in &lt;a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"&gt;seq2seq models&lt;/a&gt;).&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/Transformer_decoder.png"/&gt;

&lt;h2&gt;Bringing The Tensors Into The Picture&lt;/h2&gt;
&lt;p&gt;Now that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.&lt;/p&gt;
&lt;p&gt;As is the case in NLP applications in general, we begin by turning each input word into a vector using an &lt;a href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca"&gt;embedding algorithm&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/embeddings.png"/&gt;
&lt;br/&gt;
  Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes.

&lt;p&gt;The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.&lt;/p&gt;
&lt;p&gt;After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/encoder_with_tensors.png"/&gt;
&lt;br/&gt;

&lt;p&gt;Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.&lt;/p&gt;
&lt;p&gt;Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.&lt;/p&gt;
&lt;h2&gt;Now We’re Encoding!&lt;/h2&gt;
&lt;p&gt;As we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/encoder_with_tensors_2.png"/&gt;
&lt;br/&gt;
  The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.

&lt;h2&gt;Self-Attention at a High Level&lt;/h2&gt;
&lt;p&gt;Don’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.&lt;/p&gt;
&lt;p&gt;Say the following sentence is an input sentence we want to translate:&lt;/p&gt;
&lt;p&gt;”&lt;code&gt;The animal didn't cross the street because it was too tired&lt;/code&gt;”&lt;/p&gt;
&lt;p&gt;What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.&lt;/p&gt;
&lt;p&gt;When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.&lt;/p&gt;
&lt;p&gt;As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.&lt;/p&gt;
&lt;p&gt;If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization.png"/&gt;
&lt;br/&gt;
  As we are encoding the word "it" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on "The Animal", and baked a part of its representation into the encoding of "it".

&lt;p&gt;Be sure to check out the &lt;a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb"&gt;Tensor2Tensor notebook&lt;/a&gt; where you can load a Transformer model, and examine it using this interactive visualization.&lt;/p&gt;
&lt;h2&gt;Self-Attention in Detail&lt;/h2&gt;
&lt;p&gt;Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented – using matrices.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;first step&lt;/strong&gt; in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.&lt;/p&gt;
&lt;p&gt;Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_self_attention_vectors.png"/&gt;
&lt;br/&gt;
  Multiplying x1 by the WQ weight matrix produces q1, the "query" vector associated with that word. We end up creating a "query", a "key", and a "value" projection of each word in the input sentence.

&lt;p&gt;&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;What are the “query”, “key”, and “value” vectors?
&lt;br/&gt;
&lt;br/&gt;
They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;second step&lt;/strong&gt; in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.&lt;/p&gt;
&lt;p&gt;The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_self_attention_score.png"/&gt;
&lt;br/&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;third and fourth steps&lt;/strong&gt; are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/self-attention_softmax.png"/&gt;
&lt;br/&gt;

&lt;p&gt;This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;fifth step&lt;/strong&gt; is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;sixth step&lt;/strong&gt; is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/self-attention-output.png"/&gt;
&lt;br/&gt;

&lt;p&gt;That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.&lt;/p&gt;
&lt;h2&gt;Matrix Calculation of Self-Attention&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The first step&lt;/strong&gt; is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we’ve trained (WQ, WK, WV).&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png"/&gt;
&lt;br/&gt;
  Every row in the X matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finally&lt;/strong&gt;, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png"/&gt;
&lt;br/&gt;
  The self-attention calculation in matrix form

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2&gt;The Beast With Many Heads&lt;/h2&gt;
&lt;p&gt;The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png"/&gt;
&lt;br/&gt;
   With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.
 
&lt;p&gt;&lt;br/&gt;
If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_attention_heads_z.png"/&gt;
&lt;br/&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.&lt;/p&gt;
&lt;p&gt;How do we do that? We concat the matrices then multiply them by an additional weights matrix WO.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png"/&gt;
&lt;br/&gt;

&lt;p&gt;That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png"/&gt;
&lt;br/&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Now that we have touched upon attention heads, let’s revisit our example from before to see where the different attention heads are focusing as we encode the word “it” in our example sentence:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png"/&gt;
&lt;br/&gt;
  As we encode the word "it", one attention head is focusing most on "the animal", while another is focusing on "tired" -- in a sense, the model's representation of the word "it" bakes in some of the representation of both "animal" and "tired".

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;If we add all the attention heads to the picture, however, things can be harder to interpret:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png"/&gt;
&lt;br/&gt;

&lt;h2&gt;Representing The Order of The Sequence Using Positional Encoding&lt;/h2&gt;
&lt;p&gt;One thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.&lt;/p&gt;
&lt;p&gt;To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png"/&gt;
&lt;br/&gt;
  To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_positional_encoding_example.png"/&gt;
&lt;br/&gt;
  A real example of positional encoding with a toy embedding size of 4

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;What might this pattern look like?&lt;/p&gt;
&lt;p&gt;In the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png"/&gt;
&lt;br/&gt;
  A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.

&lt;p&gt;The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in &lt;a href="https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py"&gt;&lt;code&gt;get_timing_signal_1d()&lt;/code&gt;&lt;/a&gt;. This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;July 2020 Update:&lt;/strong&gt; 
The positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. &lt;a href="https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb"&gt;Here’s the code to generate it&lt;/a&gt;:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png"/&gt;
&lt;br/&gt;

&lt;h2&gt;The Residuals&lt;/h2&gt;
&lt;p&gt;One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a &lt;a href="https://arxiv.org/abs/1607.06450"&gt;layer-normalization&lt;/a&gt; step.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png"/&gt;
&lt;br/&gt;

&lt;p&gt;If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png"/&gt;
&lt;br/&gt;

&lt;p&gt;This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png"/&gt;
&lt;br/&gt;

&lt;h2&gt;The Decoder Side&lt;/h2&gt;
&lt;p&gt;Now that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.&lt;/p&gt;
&lt;p&gt;The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_decoding_1.gif"/&gt;
&lt;br/&gt;
  After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).

&lt;p&gt;The following steps repeat the process until a special  symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif"/&gt;
&lt;br/&gt;

&lt;p&gt;The self attention layers in the decoder operate in a slightly different way than the one in the encoder:&lt;/p&gt;
&lt;p&gt;In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to &lt;code&gt;-inf&lt;/code&gt;) before the softmax step in the self-attention calculation.&lt;/p&gt;
&lt;p&gt;The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.&lt;/p&gt;
&lt;h2&gt;The Final Linear and Softmax Layer&lt;/h2&gt;
&lt;p&gt;The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.&lt;/p&gt;
&lt;p&gt;The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.&lt;/p&gt;
&lt;p&gt;Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.&lt;/p&gt;
&lt;p&gt;The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png"/&gt;
&lt;br/&gt;
  This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2&gt;Recap Of Training&lt;/h2&gt;
&lt;p&gt;Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.&lt;/p&gt;
&lt;p&gt;During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.&lt;/p&gt;
&lt;p&gt;To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “&amp;lt;eos&amp;gt;” (short for ‘end of sentence’)).&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/vocabulary.png"/&gt;
&lt;br/&gt;
   The output vocabulary of our model is created in the preprocessing phase before we even begin training.
 
&lt;p&gt;Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/one-hot-vocabulary-example.png"/&gt;
&lt;br/&gt;
  Example: one-hot encoding of our output vocabulary

&lt;p&gt;Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.&lt;/p&gt;
&lt;h2&gt;The Loss Function&lt;/h2&gt;
&lt;p&gt;Say we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.&lt;/p&gt;
&lt;p&gt;What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/transformer_logits_output_and_label.png"/&gt;
&lt;br/&gt;
  Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output.

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;How do you compare two probability distributions? We simply subtract one from the other. For more details, look at  &lt;a href="https://colah.github.io/posts/2015-09-Visual-Information/"&gt;cross-entropy&lt;/a&gt; and &lt;a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained"&gt;Kullback–Leibler divergence&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)&lt;/li&gt;
&lt;li&gt;The first probability distribution has the highest probability at the cell associated with the word “i”&lt;/li&gt;
&lt;li&gt;The second probability distribution has the highest probability at the cell associated with the word “am”&lt;/li&gt;
&lt;li&gt;And so on, until the fifth output distribution indicates ‘&lt;code&gt;&amp;lt;end of sentence&amp;gt;&lt;/code&gt;’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.&lt;/li&gt;
&lt;/ul&gt;

&lt;img src="https://jalammar.github.io/images/t/output_target_probability_distributions.png"/&gt;
&lt;br/&gt;
   The targeted probability distributions we'll train our model against in the training example for one sample sentence.
 
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:&lt;/p&gt;

&lt;img src="https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png"/&gt;
&lt;br/&gt;
    Hopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: &lt;a href="https://www.youtube.com/watch?v=TIgfjmp-4BA"&gt;cross validation&lt;/a&gt;). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process.

&lt;p&gt;Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with.&lt;/p&gt;
&lt;h2&gt;Go Forth And Transform&lt;/h2&gt;
&lt;p&gt;I hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read the &lt;a href="https://arxiv.org/abs/1706.03762"&gt;Attention Is All You Need&lt;/a&gt; paper, the Transformer blog post (&lt;a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"&gt;Transformer: A Novel Neural Network Architecture for Language Understanding&lt;/a&gt;), and the &lt;a href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html"&gt;Tensor2Tensor announcement&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Watch &lt;a href="https://www.youtube.com/watch?v=rBCqOTEfxvg"&gt;Łukasz Kaiser’s talk&lt;/a&gt; walking through the model and its details&lt;/li&gt;
&lt;li&gt;Play with the &lt;a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb"&gt;Jupyter Notebook provided as part of the Tensor2Tensor repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Explore the &lt;a href="https://github.com/tensorflow/tensor2tensor"&gt;Tensor2Tensor repo&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Follow-up works:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1706.03059"&gt;Depthwise Separable Convolutions for Neural Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1706.05137"&gt;One Model To Learn Them All&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1801.09797"&gt;Discrete Autoencoders for Sequence Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1801.10198"&gt;Generating Wikipedia by Summarizing Long Sequences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1802.05751"&gt;Image Transformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1804.00247"&gt;Training Tips for the Transformer Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1803.02155"&gt;Self-Attention with Relative Position Representations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1803.03382"&gt;Fast Decoding in Sequence Models using Discrete Latent Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1804.04235"&gt;Adafactor: Adaptive Learning Rates with Sublinear Memory Cost&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Thanks to &lt;a href="https://twitter.com/ilblackdragon"&gt;Illia Polosukhin&lt;/a&gt;, &lt;a href="http://jakob.uszkoreit.net/"&gt;Jakob Uszkoreit&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/llion-jones-9ab3064b"&gt;Llion Jones &lt;/a&gt;, &lt;a href="https://ai.google/research/people/LukaszKaiser"&gt;Lukasz Kaiser&lt;/a&gt;, &lt;a href="https://twitter.com/nikiparmar09"&gt;Niki Parmar&lt;/a&gt;, and &lt;a href="https://dblp.org/pers/hd/s/Shazeer:Noam"&gt;Noam Shazeer&lt;/a&gt; for providing feedback on earlier versions of this post.&lt;/p&gt;
&lt;p&gt;Please hit me up on &lt;a href="https://twitter.com/JayAlammar"&gt;Twitter&lt;/a&gt; for any corrections or feedback.&lt;/p&gt;


    Written on June 27, 2018
  
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://jalammar.github.io/illustrated-transformer/</guid>
      <category>Hacker News</category>
      <pubDate>Mon, 22 Dec 2025 19:15:56 +0000</pubDate>
    </item>
    <item>
      <title>GLM-4.7: Advancing the Coding Capability</title>
      <link>https://z.ai/blog/glm-4.7</link>
      <description>GLM-4.7: Advancing the Coding Capability</description>
      <content:encoded>&lt;div class="example-content"&gt;Live StyleCyber DomainArtistic PortfolioPromptbuild a html website, High-contrast dark mode + bold condensed headings + animated ticker + chunky category chips + magnetic CTA.&lt;!-- --&gt; &lt;a href="https://chat.z.ai/s/23c15468-405d-4993-9110-cffa99f79acb"&gt;View full trajectory at Z.ai&lt;img src="https://z-cdn.chatglm.cn/z-blog/z-icon.svg"/&gt;&lt;/a&gt;GLM-4.7GLM-4.6Scroll down to see more&lt;/div&gt;</content:encoded>
      <guid isPermaLink="false">https://z.ai/blog/glm-4.7</guid>
      <category>Hacker News</category>
      <pubDate>Mon, 22 Dec 2025 18:46:32 +0000</pubDate>
    </item>
    <item>
      <title>Toad is a unified experience for AI in the terminal</title>
      <link>https://willmcgugan.github.io/toad-released/</link>
      <description>Will McGugan  ·  December 18, 2025</description>
      <content:encoded>&lt;article class="post detailed"&gt;
&lt;h1&gt;Toad is a unified experience for AI in the terminal&lt;/h1&gt;

&lt;p&gt;Will McGugan  ·  December 18, 2025&lt;/p&gt;

&lt;a href="https://willmcgugan.github.io/categories/#text"&gt;text&lt;/a&gt;
         
      
        &lt;a href="https://willmcgugan.github.io/categories/#toad"&gt;toad&lt;/a&gt;
         
      
        &lt;a href="https://willmcgugan.github.io/categories/#ai"&gt;ai&lt;/a&gt;



&lt;p&gt;My startup for terminals wrapped up mid-2025 when the funding ran dry.
So I don’t have money, but what I do have are a very particular set of skills.
Skills I have acquired over a very long career convincing terminals they are actually GUIs.&lt;/p&gt;
&lt;p&gt;Skills which I have used to create a terminal app that offers a more pleasant experience for agentic coding.
Toad (a play on &lt;em&gt;Textual Code&lt;/em&gt;) is a front-end for AI tools such as &lt;a href="https://openhands.dev/"&gt;OpenHands&lt;/a&gt;, &lt;a href="https://www.claude.com/product/claude-code"&gt;Claude Code&lt;/a&gt;, &lt;a href="https://geminicli.com/"&gt;Gemini CLI&lt;/a&gt;, and many more. 
All of which run seamlessly under a single terminal UI, thanks to the &lt;a href="https://agentclientprotocol.com/protocol/initialization"&gt;ACP&lt;/a&gt; protocol.&lt;/p&gt;
&lt;p&gt;At the time of writing, Toad supports 12 agent CLIs, and I expect many more to come online soon.&lt;/p&gt;
&lt;p&gt;Here’s a screenshot:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Toad UI" src="https://willmcgugan.github.io/images/toad-released/toad-1.png"/&gt;&lt;/p&gt;
&lt;p&gt;So what does Toad offer over the CLI apps from big tech?&lt;/p&gt;
&lt;p&gt;It has most of the UI interactions users have come to expect from agentic coding, but hopefully more refined.
For instance the “@” character to bring in files into the context. Here’s Toad’s implementation:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Toad fuzzy files" src="https://willmcgugan.github.io/images/toad-released/fuzzy-file.gif"/&gt;&lt;/p&gt;
&lt;p&gt;A snappy &lt;em&gt;fuzzy&lt;/em&gt; search which filters patterns from the project’s &lt;code&gt;.gitignore&lt;/code&gt; (if there is one).&lt;/p&gt;
&lt;p&gt;The prompt editor offers an experience which you might be surprised to find in a terminal.
You can navigate and select with the keyboard and mouse, select, cut, copy, paste, etc.
The prompt will highlight Markdown as you type (even syntax highlighting code fences &lt;em&gt;before&lt;/em&gt; you close them).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Toad prompt" src="https://willmcgugan.github.io/images/toad-released/prompt.gif"/&gt;&lt;/p&gt;
&lt;p&gt;Toad has really nice Markdown streaming, based on the techniques I described &lt;a href="https://willmcgugan.github.io/streaming-markdown/"&gt;here&lt;/a&gt;.
It remains fast with large documents, and renders everything from tables to syntax highlighted code fences.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Toad Markdown streaming" src="https://willmcgugan.github.io/images/toad-released/stream.gif"/&gt;&lt;/p&gt;
&lt;p&gt;Many other tools either don’t bother to render the Markdown, or they do a fairly half-hearted job.&lt;/p&gt;
&lt;p&gt;Another goal I had for Toad was to integrate a shell.
I wanted the conversation with AI to feel like a natural extension of a traditional terminal based workflow.&lt;/p&gt;
&lt;p&gt;Most tools stop at displaying monochrome output from commands.
Some will break if you run something interactive, like a TUI.
Toad doesn’t have this limitation, and will let you run all your CLI apps with full color, interactivity, and mouse support.&lt;/p&gt;
&lt;p&gt;At the time of writing the only terminal based agentic coding tool I know of that runs dynamic commands inline is Gemini.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Toad running htop" src="https://willmcgugan.github.io/images/toad-released/htop.gif"/&gt;&lt;/p&gt;
&lt;p&gt;Toad adopts the convention of using a &lt;code&gt;!&lt;/code&gt; character to introduce a shell command.
There is also a list of commands in settings which will automatically trigger shell mode.
In practice, this means that you rarely need to explicitly introduce shell commands—just type what’s on your mind.&lt;/p&gt;
&lt;p&gt;Toad borrows tab completion from the shell.
You’ll appreciate this if you have worked in the terminal long enough to commit this interaction to muscle memory.
Hit tab to complete the command or path.
If there is more than one possibility you can hit tab again to cycle through them, and enter to accept.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Toad tab complete" src="https://willmcgugan.github.io/images/toad-released/tabcomplete.gif"/&gt;&lt;/p&gt;
&lt;p&gt;In addition to the shell, Toad implements a few concepts from Jupyter notebooks.
You can cursor through previous conversation, moving a logical block at a time, and interact with it again.
At the moment that feature is used as a convenience to copy content to the clipboard or prompt, and a few other niceties like exporting a SVG.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Cursor block" src="https://willmcgugan.github.io/images/toad-released/cursor-block.png"/&gt;&lt;/p&gt;
&lt;p&gt;Toad will lean more heavily in to this kind of interaction in the future.&lt;/p&gt;
&lt;h2&gt;Friends of Toad&lt;/h2&gt;
&lt;p&gt;I was very fortunate to collaborate with &lt;a href="https://openhands.dev/"&gt;OpenHands&lt;/a&gt;, who are doing some amazing work in this space. Check out their &lt;a href="https://www.openhands.dev/blog/20251218-openhands-toad-collaboration"&gt;blog post&lt;/a&gt; on Toad!&lt;/p&gt;
&lt;p&gt;I also collaborated with &lt;a href="https://huggingface.co"&gt;Hugging Face&lt;/a&gt; on this release. Check out their blog post on their &lt;a href="https://huggingface.co/toad-hf-inference-explorers"&gt;inference explorers&lt;/a&gt;!&lt;/p&gt;
&lt;h2&gt;Try Toad&lt;/h2&gt;
&lt;p&gt;When this post is live you will be able to install Toad yourself.&lt;/p&gt;
&lt;p&gt;The work is ongoing: a few missing features and interface improvements to be done, but Toad is solid enough to use as your daily driver for AI.
I used it to create &lt;a href="https://www.batrachian.ai"&gt;batrachian.ai&lt;/a&gt;, where you will find install instructions.&lt;/p&gt;
&lt;p&gt;For more details, see the &lt;a href="https://github.com/batrachianai/toad"&gt;Toad&lt;/a&gt; repository.&lt;/p&gt;
&lt;p&gt;I need a break (sabbaticals are tiring), but I’ll be picking things up in 2026.
I’m hoping that by the time my year off ends, Toad could become my full-time gig.
If you want to help make that happen, consider &lt;a href="https://github.com/sponsors/willmcgugan"&gt;sponsoring my work&lt;/a&gt;.&lt;/p&gt;


&lt;p&gt;Share: &lt;a href="http://twitter.com/share?text=Toad is a unified experience for AI in the terminal&amp;amp;url=http://willmcgugan.github.io/toad-released/"&gt;Twitter&lt;/a&gt;, &lt;a href="https://www.facebook.com/sharer.php?u=http://willmcgugan.github.io/toad-released/"&gt;Facebook&lt;/a&gt;&lt;/p&gt;

&lt;!--&lt;div class="date"&gt;
    Written on December 18, 2025
  &lt;/div&gt;--&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://willmcgugan.github.io/toad-released/</guid>
      <category>Hacker News</category>
      <pubDate>Mon, 22 Dec 2025 15:12:40 +0000</pubDate>
    </item>
    <item>
      <title>Lua 5.5</title>
      <link>https://lua.org/versions.html#5.5</link>
      <description>Here is a chronology of the versions of Lua. The evolution of Lua is documented in a paper presented at HOPL III,
the Third ACM SIGPLAN History of Programming Languages Conference ,
in 2007.
See also its continuation published in 2025 in a journal .
The source code and documentation for all releases of Lua are available in the download area . Numbering scheme The releases of Lua are numbered x.y.z ,
where x is the series, x.y is the version, and z is the release. Different releases of the same version correspond to bug fixes .
Different releases of the same version have the same reference manual ,
the same virtual machine, and are binary compatible (ABI compatible). Different versions are really different.
The API is likely to be a little different
(but with compatibility switches),
and there is no ABI compatibility:
applications that embed Lua and C libraries for Lua must be recompiled.
The virtual machine is also very likely to be different in a new version:
Lua programs that have been precompiled for one version will not load in a different version. Lua 5.5 Lua 5.5.0 was released on 22 Dec 2025.
Its main new features are
declarations for global variables,
more compact arrays,
new generational mode for garbage collection,
and
major garbage collections done incrementally. Lua 5.4 Lua 5.4
was released on 29 Jun 2020.
Its main new features are
a new generational mode for garbage collection and
const and to-be-closed variables. The current release is Lua 5.4.8 ,
released on 4 Jun 2025. Lua 5.3 Lua 5.3
was released on 12 Jan 2015.
Its main new features were
integers,
bitwise operators,
a basic utf-8 library,
and support for both 64-bit and 32-bit platforms. The last release was Lua 5.3.6 ,
released on 25 Sep 2020.
There will be no further releases of Lua 5.3. Lua 5.2 Lua 5.2
was released on 16 Dec 2011.
Its main new features were
yieldable pcall and metamethods,
new lexical scheme for globals,
ephemeron tables,
new library for bitwise operations,
light C functions,
emergency garbage collector,
goto statement,
and
finalizers for tables. The last release was Lua 5.2.4 ,
released on 7 Mar 2015.
There will be no further releases of Lua 5.2. Lua 5.1 Lua 5.1
was released on 21 Feb 2006.
Its main new features were
a new module system,
incremental garbage collection,
new mechanism for varargs,
new syntax for long strings and comments,
mod and length operators,
metatables for all types,
new configuration scheme via luaconf.h,
and a
fully reentrant parser. The last release was Lua 5.1.5 ,
released on 17 Feb 2012.
There will be no further releases of Lua 5.1. Lua 5.0 Lua 5.0
was released on 11 Apr 2003. Its main new features were
collaborative multithreading via Lua coroutines,
full lexical scoping instead of upvalues,
and
metatables instead of tags and tag methods.
Lua 5.0 also introduced
booleans,
proper tail calls,
and weak tables.
Other features were
better support for packages,
new API for loading Lua chunks,
new error handling protocol,
better error messages,
and much more.
Lua 5.0 was the first version to be released under the new license . The last release was Lua 5.0.3 ,
released on 26 Jun 2006.
There will be no further releases of Lua 5.0. Lua 4.0 Lua 4.0
was released on 6 Nov 2000.
Its main new features were
multiples states,
a new API,
"for" statements,
and full speed execution with full debug information.
Also,
Lua 4.0 no longer had built-in functions:
all functions in the standard library are written using the official API. The last release was Lua 4.0.1 ,
released on 4 Jul 2002.
There will be no further releases of Lua 4.0. Lua 3.2 Lua 3.2
was released on 8 Jul 1999.
Its main new features were
a debug library
and
new table functions. The last release was Lua 3.2.2 ,
released on 22 Feb 2000.
There will be no further releases of Lua 3.2. Lua 3.1 Lua 3.1 was released on 11 Jul 1998.
Its main new features were
anonymous functions and
function closures via "upvalues".
( Lua 5.0 brought full lexical scoping and dropped upvalues.)
This brought a flavor of functional programming to Lua.
There was also support for multiple global contexts;
however, the API was not fully reentrant
(this had to wait until Lua 4.0 ).
Lua 3.1 also saw a
major code re-organization and clean-up,
with much reduced module interdependencies.
Lua 3.1 also adopted double precision for the internal representation of numbers. Lua 3.0 Lua 3.0 was released on 1 Jul 1997.
Its main new feature was
tag methods as a powerful replacement for fallbacks.
Lua 3.0 also introduced auxlib,
a library for helping writing Lua libraries in C,
and support for conditional compilation
(dropped in Lua 4.0 ). Lua 2.5 Lua 2.5 was released on 19 Nov 1996.
Its main new features were
pattern matching and vararg functions. Lua 2.4 Lua 2.4 was released on 14 May 1996.
Its main new features were
the external compiler luac ,
an extended debug interface with hooks,
and the "getglobal" fallback. Lua 2.3 Lua 2.3 was never released publicly; it only existed as a beta version. Lua 2.2 Lua 2.2 was released on 28 Nov 1995.
Its main new features were
long strings,
the debug interface,
better stack tracebacks,
extended syntax for function definition,
garbage collection of functions,
and support for pipes. Lua 2.1 Lua 2.1 was released on 7 Feb 1995.
Its main new features were
extensible semantics via fallbacks
and support for object-oriented programming.
It was described in a journal paper which was awarded the
II Compaq Award for Research and Development in Computer Science in 1997.
Starting with Lua 2.1,
Lua became freely available for all purposes,
including commercial uses. Lua 1.1 Lua 1.1 was released on 8 Jul 1994.
It was the first public release of Lua and was described in a conference paper .
Lua 1.1 already featured
powerful data description constructs,
simple syntax,
and a bytecode virtual machine.
Lua 1.1 was freely available for academic purposes;
commercial uses had to be negotiated, but none ever were. Lua 1.0 Lua 1.0 was never released publicly.
It was up and running on 28 Jul 1993 and most probably a couple of months before that. Last update:
Mon Dec 22 14:21:22 UTC 2025</description>
      <content:encoded>&lt;body&gt;
&lt;h1&gt;
&lt;a href="https://lua.org/home.html"&gt;&lt;img alt="Lua" src="https://lua.org/images/logo.png"/&gt;&lt;/a&gt;
Version history
&lt;/h1&gt;

&lt;a href="#5.5"&gt;5.5&lt;/a&gt;
·
&lt;a href="#5.4"&gt;5.4&lt;/a&gt;
·
&lt;a href="#5.3"&gt;5.3&lt;/a&gt;
·
&lt;a href="#5.2"&gt;5.2&lt;/a&gt;
·
&lt;a href="#5.1"&gt;5.1&lt;/a&gt;
·
&lt;a href="#5.0"&gt;5.0&lt;/a&gt;
·
&lt;a href="#4.0"&gt;4.0&lt;/a&gt;
·
&lt;a href="#3.2"&gt;3.2&lt;/a&gt;
·
&lt;a href="#3.1"&gt;3.1&lt;/a&gt;
·
&lt;a href="#3.0"&gt;3.0&lt;/a&gt;
·
&lt;a href="#2.5"&gt;2.5&lt;/a&gt;
·
&lt;a href="#2.4"&gt;2.4&lt;/a&gt;
·
&lt;a href="#2.2"&gt;2.2&lt;/a&gt;
·
&lt;a href="#2.1"&gt;2.1&lt;/a&gt;
·
&lt;a href="#1.1"&gt;1.1&lt;/a&gt;
·
&lt;a href="#1.0"&gt;1.0&lt;/a&gt;

&lt;p&gt;
Here is a chronology of the versions of Lua.
&lt;a href="https://lua.org/doc/hopl.pdf"&gt;The evolution of Lua&lt;/a&gt;
is documented in a
&lt;a href="https://lua.org/docs.html#papers"&gt;paper&lt;/a&gt;
presented at
&lt;a href="https://en.wikipedia.org/wiki/History_of_Programming_Languages#HOPL_III"&gt;HOPL III,
the Third ACM SIGPLAN History of Programming Languages Conference&lt;/a&gt;,
in 2007.
See also its
&lt;a href="https://lua.org/doc/cola.pdf"&gt;continuation&lt;/a&gt; published in 2025 in a
&lt;a href="https://doi.org/10.1016/j.cola.2025.101326"&gt;journal&lt;/a&gt;.
The source code and documentation for all releases of Lua are available in the
&lt;a href="https://lua.org/ftp/"&gt;download area&lt;/a&gt;.


&lt;img src="https://lua.org/images/timeline.png"/&gt;

&lt;h2&gt;&lt;a&gt;Numbering scheme&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
The releases of Lua are numbered &lt;em&gt;x.y.z&lt;/em&gt;,
where
&lt;em&gt;x&lt;/em&gt; is the series, &lt;em&gt;x.y&lt;/em&gt; is the version, and &lt;em&gt;z&lt;/em&gt; is the release.

&lt;p&gt;
Different releases of the same version correspond to
&lt;a href="https://lua.org/bugs.html"&gt;bug fixes&lt;/a&gt;.
Different releases of the same version have the same
&lt;a href="https://lua.org/manual/"&gt;reference manual&lt;/a&gt;,
the same virtual machine, and are binary compatible (ABI compatible).

&lt;p&gt;
Different versions are really different.
The API is likely to be a little different
(but with compatibility switches),
and there is no ABI compatibility:
applications that embed Lua and C libraries for Lua must be recompiled.
The virtual machine is also very likely to be different in a new version:
Lua programs that have been precompiled for one version will not load in a different version.

&lt;!--
We are getting ready to release
&lt;A HREF="work/"&gt;Lua 5.5&lt;/A&gt;,
the next version of Lua.
Try the
&lt;A HREF="work/"&gt;latest release candidate&lt;/A&gt;.
--&gt;
&lt;h2&gt;&lt;a&gt;Lua 5.5&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
Lua 5.5.0 was released on 22 Dec 2025.
Its
&lt;a href="https://lua.org/manual/5.5/readme.html#changes"&gt;main new features&lt;/a&gt;
are
declarations for global variables,
more compact arrays,
new generational mode for garbage collection,
and
major garbage collections done incrementally.

&lt;h2&gt;&lt;a&gt;Lua 5.4&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
Lua 5.4
was released on 29 Jun 2020.
Its
&lt;a href="https://lua.org/manual/5.4/readme.html#changes"&gt;main new features&lt;/a&gt;
are
a new generational mode for garbage collection and
const and to-be-closed variables.

&lt;p&gt;
The current release is
&lt;a href="https://lua.org/ftp/lua-5.4.8.tar.gz"&gt;Lua 5.4.8&lt;/a&gt;,
released on 4 Jun 2025.

&lt;!--
We are getting ready to release
&lt;A HREF="work/"&gt;Lua 5.4.8&lt;/A&gt;.
--&gt;
&lt;h2&gt;&lt;a&gt;Lua 5.3&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
Lua 5.3
was released on 12 Jan 2015.
Its
&lt;a href="https://lua.org/manual/5.3/readme.html#changes"&gt;main new features&lt;/a&gt;
were
integers,
bitwise operators,
a basic utf-8 library,
and support for both 64-bit and 32-bit platforms.

&lt;p&gt;
The last release was
&lt;a href="https://lua.org/ftp/lua-5.3.6.tar.gz"&gt;Lua 5.3.6&lt;/a&gt;,
released on 25 Sep 2020.
There will be no further releases of Lua 5.3.

&lt;h2&gt;&lt;a&gt;Lua 5.2&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
Lua 5.2
was released on 16 Dec 2011.
Its
&lt;a href="https://lua.org/manual/5.2/readme.html#changes"&gt;main new features&lt;/a&gt;
were
yieldable pcall and metamethods,
new lexical scheme for globals,
ephemeron tables,
new library for bitwise operations,
light C functions,
emergency garbage collector,
goto statement,
and
finalizers for tables.

&lt;p&gt;
The last release was
&lt;a href="https://lua.org/ftp/lua-5.2.4.tar.gz"&gt;Lua 5.2.4&lt;/a&gt;,
released on 7 Mar 2015.
There will be no further releases of Lua 5.2.

&lt;h2&gt;&lt;a&gt;Lua 5.1&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
Lua 5.1
was released on 21 Feb 2006.
Its main new features were
a new module system,
incremental garbage collection,
new mechanism for varargs,
new syntax for long strings and comments,
mod and length operators,
metatables for all types,
new configuration scheme via luaconf.h,
and a
fully reentrant parser.

&lt;p&gt;
The last release was
&lt;a href="https://lua.org/ftp/lua-5.1.5.tar.gz"&gt;Lua 5.1.5&lt;/a&gt;,
released on 17 Feb 2012.
There will be no further releases of Lua 5.1.

&lt;h2&gt;&lt;a&gt;Lua 5.0&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
Lua 5.0
was released on 11 Apr 2003.
&lt;!--
&lt;A HREF="http://compilers.iecc.com/comparch/article/03-04-041"&gt;More...&lt;/A&gt;
--&gt;
Its main new features were
collaborative multithreading via Lua coroutines,
full lexical scoping instead of upvalues,
and
metatables instead of tags and tag methods.
Lua 5.0 also introduced
booleans,
proper tail calls,
and weak tables.
Other features were
better support for packages,
new API for loading Lua chunks,
new error handling protocol,
better error messages,
and much more.
Lua 5.0 was the first version to be released under the
&lt;a href="https://lua.org/license.html"&gt;new license&lt;/a&gt;.

&lt;p&gt;
The last release was
&lt;a href="https://lua.org/ftp/lua-5.0.3.tar.gz"&gt;Lua 5.0.3&lt;/a&gt;,
released on 26 Jun 2006.
There will be no further releases of Lua 5.0.

&lt;h2&gt;&lt;a&gt;Lua 4.0&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
Lua 4.0
was released on 6 Nov 2000.
Its main new features were
multiples states,
a new API,
"for" statements,
and full speed execution with full debug information.
Also,
Lua 4.0 no longer had built-in functions:
all functions in the standard library are written using the official API.

&lt;p&gt;
The last release was
&lt;a href="https://lua.org/ftp/lua-4.0.1.tar.gz"&gt;Lua 4.0.1&lt;/a&gt;,
released on 4 Jul 2002.
There will be no further releases of Lua 4.0.
&lt;!--
&lt;A HREF="http://compilers.iecc.com/comparch/article/00-11-147"&gt;More...&lt;/A&gt;
--&gt;
&lt;h2&gt;&lt;a&gt;Lua 3.2&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
Lua 3.2
was released on 8 Jul 1999.
Its main new features were
a debug library
and
new table functions.

&lt;p&gt;
The last release was
&lt;a href="https://lua.org/ftp/lua-3.2.2.tar.gz"&gt;Lua 3.2.2&lt;/a&gt;,
released on 22 Feb 2000.
There will be no further releases of Lua 3.2.
&lt;!--
&lt;A HREF="http://compilers.iecc.com/comparch/article/99-07-033"&gt;More...&lt;/A&gt;
--&gt;
&lt;h2&gt;&lt;a&gt;Lua 3.1&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
&lt;a href="https://lua.org/ftp/lua-3.1.tar.gz"&gt;Lua 3.1&lt;/a&gt;
was released on 11 Jul 1998.
Its main new features were
anonymous functions and
function closures via "upvalues".
(&lt;a href="#5.0"&gt;Lua 5.0&lt;/a&gt; brought full lexical scoping and dropped upvalues.)
This brought a flavor of functional programming to Lua.
There was also support for multiple global contexts;
however, the API was not fully reentrant
(this had to wait until &lt;a href="#4.0"&gt;Lua 4.0&lt;/a&gt;).
Lua 3.1 also saw a
major code re-organization and clean-up,
with much reduced module interdependencies.
Lua 3.1 also adopted double precision for the internal representation of numbers.
&lt;!--
&lt;A HREF="http://compilers.iecc.com/comparch/article/98-07-106"&gt;More...&lt;/A&gt;
--&gt;
&lt;h2&gt;&lt;a&gt;Lua 3.0&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
&lt;a href="https://lua.org/ftp/lua-3.0.tar.gz"&gt;Lua 3.0&lt;/a&gt;
was released on 1 Jul 1997.
Its main new feature was
tag methods as a powerful replacement for fallbacks.
Lua 3.0 also introduced auxlib,
a library for helping writing Lua libraries in C,
and support for conditional compilation
(dropped in &lt;a href="#4.0"&gt;Lua 4.0&lt;/a&gt;).
&lt;!--
&lt;A HREF="http://compilers.iecc.com/comparch/article/97-07-021"&gt;More...&lt;/A&gt;
--&gt;
&lt;h2&gt;&lt;a&gt;Lua 2.5&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
&lt;a href="https://lua.org/ftp/lua-2.5.tar.gz"&gt;Lua 2.5&lt;/a&gt;
was released on 19 Nov 1996.
Its main new features were
pattern matching and vararg functions.
&lt;!--
&lt;A HREF="http://compilers.iecc.com/comparch/article/96-11-120"&gt;More...&lt;/A&gt;
--&gt;
&lt;h2&gt;&lt;a&gt;Lua 2.4&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
&lt;a href="https://lua.org/ftp/lua-2.4.tar.gz"&gt;Lua 2.4&lt;/a&gt;
was released on 14 May 1996.
Its main new features were
the external compiler &lt;em&gt;luac&lt;/em&gt;,
an extended debug interface with hooks,
and the "getglobal" fallback.
&lt;!--
&lt;A HREF="http://compilers.iecc.com/comparch/article/96-05-101"&gt;More...&lt;/A&gt;
--&gt;
&lt;h2&gt;&lt;a&gt;Lua 2.3&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
Lua 2.3 was never released publicly; it only existed as a beta version.

&lt;h2&gt;&lt;a&gt;Lua 2.2&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
&lt;a href="https://lua.org/ftp/lua-2.2.tar.gz"&gt;Lua 2.2&lt;/a&gt;
was released on 28 Nov 1995.
Its main new features were
long strings,
the debug interface,
better stack tracebacks,
extended syntax for function definition,
garbage collection of functions,
and support for pipes.

&lt;h2&gt;&lt;a&gt;Lua 2.1&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
&lt;a href="https://lua.org/ftp/lua-2.1.tar.gz"&gt;Lua 2.1&lt;/a&gt;
was released on 7 Feb 1995.
Its main new features were
extensible semantics via fallbacks
and support for object-oriented programming.
It was described in a
&lt;a href="https://lua.org/spe.html"&gt;journal paper&lt;/a&gt;
which was awarded the
II Compaq Award for Research and Development in Computer Science in 1997.
Starting with Lua 2.1,
Lua became freely available for all purposes,
including commercial uses.
&lt;!--
&lt;A HREF="http://compilers.iecc.com/comparch/article/95-02-083"&gt;More...&lt;/A&gt;
--&gt;
&lt;h2&gt;&lt;a&gt;Lua 1.1&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
&lt;a href="https://lua.org/ftp/lua-1.1.tar.gz"&gt;Lua 1.1&lt;/a&gt;
was released on 8 Jul 1994.
It was the
&lt;a href="https://compilers.iecc.com/comparch/article/94-07-051"&gt;first public release of Lua&lt;/a&gt; and was described in a
&lt;a href="https://lua.org/semish94.html"&gt;conference paper&lt;/a&gt;.
Lua 1.1 already featured
powerful data description constructs,
simple syntax,
and a bytecode virtual machine.
Lua 1.1 was freely available for academic purposes;
commercial uses had to be negotiated, but none ever were.

&lt;h2&gt;&lt;a&gt;Lua 1.0&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
&lt;a href="https://lua.org/ftp/lua-1.0.tar.gz"&gt;Lua 1.0&lt;/a&gt;
was never released publicly.
It was up and running on
&lt;a href="http://lua-users.org/lists/lua-l/2023-07/msg00130.html"&gt;28 Jul 1993&lt;/a&gt;
and most probably a couple of months before that.

&lt;p&gt;
Last update:
Mon Dec 22 14:21:22 UTC 2025
&lt;/p&gt;
&lt;!--
Last change: Lua 5.5.0 released
--&gt;
&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/body&gt;</content:encoded>
      <guid isPermaLink="false">https://lua.org/versions.html#5.5</guid>
      <category>Hacker News</category>
      <pubDate>Mon, 22 Dec 2025 15:06:13 +0000</pubDate>
    </item>
    <item>
      <title>Show HN: Yapi – FOSS terminal API client for power users</title>
      <link>https://yapi.run/blog/what-is-yapi</link>
      <description>Yapi is Postman, Insomnia or Bruno for the power user.</description>
      <content:encoded>&lt;article class="max-w-3xl w-full"&gt;&lt;a href="https://yapi.run/blog"&gt;Back to Blog&lt;/a&gt;&lt;blockquote&gt;
&lt;p&gt;Yapi is Postman, Insomnia or Bruno for the power user.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yapi is an OSS command line tool that makes it easy to test APIs from your
terminal. Yapi speaks HTTP, gRPC, TCP, GraphQL (and more coming soon).&lt;/p&gt;
&lt;p&gt;&lt;img alt="yapi in action" src="https://github.com/jamierpond/madea.blog/blob/main/yapi/yapi-example.gif?raw=true"/&gt;&lt;/p&gt;
&lt;h3&gt;Heads up! &lt;em&gt;Yapi is early, early alpha software&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;I use yapi daily in my development workflow. However, yapi is a &lt;em&gt;SUPER&lt;/em&gt;
young project and will have bugs, missing features and rough edges.&lt;/p&gt;
&lt;p&gt;If you &lt;a href="https://github.com/jamierpond/yapi"&gt;download yapi&lt;/a&gt;, I would &lt;em&gt;LOVE&lt;/em&gt; your feedback on how to make it better. Please &lt;a href="https://github.com/jamierpond/yapi/issues"&gt;open an issue&lt;/a&gt;
if you have any suggestions or find any bugs!&lt;/p&gt;
&lt;h3&gt;Show me some examples!&lt;/h3&gt;
&lt;h4&gt;POST&lt;/h4&gt;
&lt;p&gt;This request:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# create-issue.yapi.yml
yapi: v1 # specify yapi version
method: POST # or GET, PUT, DELETE, PATCH, etc.
url: https://api.github.com/repos/jamierpond/yapi/issues

headers:
  Accept: application/vnd.github+json
  Authorization: Bearer ${GITHUB_PAT} # supports environment variables

body: # defaults to JSON body, converted automatically
  title: Help! yapi made me too productive.
  body: Now I can't stop YAPPIN' about yapi!

expect: # supports expected response tests
  status: 201
  assert: # assert using jq syntax
    - .body == "Now I can't stop YAPPIN' about yapi!"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Gives you this response:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yapi run create-issue.yapi.yml
{
  "active_lock_reason": null,
  "assignee": null,
  "assignees": [],
  "author_association": "OWNER",
  "body": "Now I can't stop YAPPIN' about yapi!\n",
  // ...blah blah blah
}

URL: https://api.github.com/repos/jamierpond/yapi/issues
Time: 579.408625ms
Size: 2.3 kB (1 lines, 2288 chars)

[PASS] status check
[PASS] .body == "Now I can't stop YAPPIN' about yapi!"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;(Only the JSON goes to stdout, the rest goes to stderr, so is pipeable!)&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Yapi supports chaining requests between protocols&lt;/h3&gt;
&lt;h4&gt;Multi-protocol chaining&lt;/h4&gt;
&lt;p&gt;Yapi makes it easy to chain requests and share data between them, even if they are different protocols.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# multi-protocol-chain.yapi.yml
yapi: v1
chain:
  - name: get_todo # HTTP request
    url: https://jsonplaceholder.typicode.com/todos/1
    method: GET

  - name: grpc_hello # gRPC request
    url: grpc://grpcb.in:9000
    service: hello.HelloService
    rpc: SayHello # Supports reflection if server has it enabled
    plaintext: true

    body:
      greeting: $get_todo.title # use data from previous request

    expect:
      assert:
        - .reply == "hello delectus aut autem" # assert on gRPC response
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://yapi.run/c/Df.8OgAzqUG-ZpArHJoiQovBOB1XLQM8BNSbz655jxnoC_d1kKWwWp3qeqyLUO_M73lKIwrUWJ6prOLMl5R2.qS8r1QzMvktH~6BGCJjH3E6CO6.iI_fuN4z.kAT3W70t~of01maCruwdBfYgS_nxmtesmpiqKbjalxSWIKMGIRbdL_u8WwRc89gKXPB0kW5wy_CGDmIF0Kef.vuRpV7Hm3BjZie2~yI6DHtvb-RgOqEzldl0W~Y.m4RTNw_Oy.OlksIrAkhwiIy6gdyVqPk0tgTzOePDzuo.trOigUj1AtfBNDpowrEdIU3MmT53pjaobcZV73FYwwpjWOfdNCEJqKTTUVrCc8ZgVB5riViWnJxqo5yz759FWyiBFbi~_2nLHnu67V7RLQPUYnFknAovF2SegQYEmmI~vrbJjjmdNShJNaVW"&gt;Run this example in the yapi playground&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Integration Testing with yapi&lt;/h3&gt;
&lt;p&gt;Yapi has built-in support for writing integration tests with expectations and assertions.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yapi: v1
method: GET
url: https://api.github.com/repos/jamierpond/yapi/issues/5
headers:
    Accept: application/vnd.github+json
expect:
    status: 200
    assert:
      - .state == "closed"
      - .closed_by.login == "jamierpond"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://yapi.run/c/TccVrmpVtt_daI~NZovhqJO6LNVxr-qTEAL9aQp.nAvs9GoUK8PdGgJ6o34wC-THPf64WS.-iAWCfXohQ8~eo~7iOhp6Wop-a_lwPT.Y624K-1S97CVJgXwMRxtwUzZz6on7xftwVegIkX~6hO81OIvTsA6ASUgWdnf8va0QlaGHLI5w1d3LZ5WQ4xzYDg8~5lqPB5s3FE~~EL8aBeCKk-FtOPJ1AIPDMNH4X55A8QLYUF3NWY"&gt;Run this example in the yapi playground&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Yapi has an LSP Server for IDE Integration&lt;/h3&gt;
&lt;p&gt;Yapi comes with a Language Server Protocol (LSP) server that provides
syntax highlighting, autocompletion and validation for yapi request files in
editors that support LSP (Neovim, etc), you can also use the &lt;a href="https://github.com/jamierpond/yapi/blob/main/lua/yapi_nvim/init.lua"&gt;yapi Neovim plugin&lt;/a&gt; (still early days).&lt;/p&gt;
&lt;p&gt;At some point I'll write the VSCode extension too, please make an issue if you think this is important!&lt;/p&gt;
&lt;h3&gt;GitHub Actions Support&lt;/h3&gt;
&lt;p&gt;I use yapi's GitHub Action to run &lt;a href="https://github.com/jamierpond/yapi/actions/runs/20426239184/job/58687025263"&gt;integraion tests on the CI for this blog&lt;/a&gt;!&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;name: Integration Tests
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm ci

      - name: Build Application
        run: npm build

      - name: Run Yapi Integration Tests
        uses: jamierpond/yapi/action@0.X.X
        with:
          start: npm run start
          wait-on: http://localhost:3000/health
          command: yapi test ./tests -a
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, here is the output of yapi running integration tests in GitHub Actions:
&lt;img alt="yapi in GitHub Actions" src="https://github.com/jamierpond/madea.blog/blob/main/yapi/ci.png?raw=true"/&gt;&lt;/p&gt;
&lt;h3&gt;Supports for Multiple Environments&lt;/h3&gt;
&lt;p&gt;Yapi make it easy to manage multiple environments (dev, staging, prod, etc).
Define your environments in a &lt;code&gt;yapi.config.yml&lt;/code&gt; file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yapi: v1

default_environment: local

environments:
  local:
    url: http://localhost:3000
    env_file: .env.local
    vars:
      some_param: default_value

  prod:
    url: https://yapi.run
    env_file: .env.prod
    vars:
      some_param: some_value
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then run yapi with the desired environment:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yapi run my-request.yapi.yml --env prod
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This also cleans up your request files a little, now you can use &lt;code&gt;path&lt;/code&gt;s instead of full URLs:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yapi: v1
method: GET
path: /api/v1/status # base URL comes from the selected environment
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Getting Started with Yapi&lt;/h3&gt;
&lt;p&gt;To get started with yapi, simply install it using the instructions on the
&lt;a href="https://github.com/jamierpond/yapi"&gt;yapi GitHub repository&lt;/a&gt; and start creating your
first request files!&lt;/p&gt;
&lt;h3&gt;Contributing to Yapi&lt;/h3&gt;
&lt;p&gt;Yapi is an open-source project maintained by just me, &lt;a href="https://pond.audio"&gt;Jamie&lt;/a&gt;.
If you find bugs or have feature requests, please open an issue on the
&lt;a href="https://github.com/jamierpond/yapi"&gt;yapi GitHub repository&lt;/a&gt;. Pull requests are very welcome too!&lt;/p&gt;&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://yapi.run/blog/what-is-yapi</guid>
      <category>Hacker News</category>
      <pubDate>Mon, 22 Dec 2025 08:49:47 +0000</pubDate>
    </item>
    <item>
      <title>Dancing around the rhythm space with Euclid</title>
      <link>https://pv.wtf/posts/euclidean-rhythms</link>
      <description>2025-12-22</description>
      <content:encoded>&lt;article&gt;
&lt;h1&gt;Dancing around the rhythm space with Euclid&lt;/h1&gt;
&lt;p&gt;2025-12-22&lt;/p&gt;
&lt;p&gt;I've been playing with sequencers and getting out of Euclidean rhythms (kind of) and into Non-Euclidean ones, and at the end of the whole story managed to build a little one of my own. If those words mean nothing to you yet, enjoy the ride.&lt;/p&gt;
&lt;p&gt;There's a little music corner at home, and in it I've got the classic Synth hipster combo of an Elektron Digitone and a Digitakt. Both are really fun to play with, especially the Digitone. I really like the immediacy, and often it's with me on vacation. A powerbank going in and a pair of headphones coming out. The Elektron sequencer workflow makes it easy to get started, and it pairs it with a great FM-synthesis sound engine, which normally has a reputation for being hard to understand.&lt;/p&gt;
&lt;p&gt;And yes, even with the playful Digitone this reputation is still well earned, which has led me to experiment more. Synthesizers are normally fixed in their architecture, oscillator goes into filter and if you are lucky, there's a mysterious mod-matrix to re-jigger parts via cryptic button combinations. I love my hardware, but… Sometimes it's just not the best for learning.&lt;/p&gt;
&lt;p&gt;Modular synthesizers exist as well, but my synth corner needs to stay just a corner. This brings us to VCV Rack, it's a software modular synthesis environment with a lot of overlap with the Eurorack hardware format. You can have a lot of fun following &lt;a href="https://www.youtube.com/channel/UCuWKHSHTHMV_nVSeNH4gYAg"&gt;Omri Cohen&lt;/a&gt;'s videos, cables and modulation going everywhere to end up with something &lt;em&gt;Very Nice&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Of course, when you get a cool sound going, it needs to keep playing while you somehow make it worse and worse. This is where a Sequencer usually helps, and there is a wealth of different ones to play with in VCV Rack. Connect them with semi-random modulation sources and all the little bleeps, thonks and thwangs can keep evolving with what the neighbors will add as wall-based percussion track for hours.&lt;/p&gt;
&lt;p&gt;When I play on the Digitone/takt, it's a bit different. What I do is create a set of patterns that evolve in some small way as you move from one to the next. On top of this I tweak and twiddle parameters, filters and effects to help create transitions over time. I keep to my little corner at home, but I'm romanticizing about the idea of a thrilling live performance, especially with live improvisation.&lt;/p&gt;
&lt;p&gt;The tricky thing is that live improvisation requires tools and a thought out workflow, and the way I've been using my devices requires a lot of preparation up-front. On one end maybe more randomization can help, but pure random usually doesn't sound good either, so I need some way to increase my lucky accidents.&lt;/p&gt;
&lt;p&gt;Now, of course, the real solution is really quite simple, you might already be thinking it: I should practice more with the gear that I already have. Yes, so of course I'm doing something entirely different.&lt;/p&gt;
&lt;p&gt;Among the many sequencers in VCV Rack I found some based on Euclidean rhythms, and it turns out that they might be just what I needed. What are they? Hit play and take it away!&lt;/p&gt;

&lt;p&gt;The Euclidean algorithm tries to position a number of hits as evenly as possible across a pattern. There's a lot of material online that explains it from many angles, &lt;a href="https://www.youtube.com/results?search_query=euclidean+rhythms"&gt;YouTube Videos&lt;/a&gt; and &lt;a href="https://cgm.cs.mcgill.ca/~godfried/publications/banff-extended.pdf"&gt;the original paper&lt;/a&gt; written by Godfried Toussaint is also short and clear.&lt;/p&gt;
&lt;p&gt;It turns out that by varying the hits and length of a pattern, you get a fair number of world music rhythms that you will most likely recognize. We'll use &lt;code&gt;E(N,L)&lt;/code&gt; notation with &lt;code&gt;N=Number of hits&lt;/code&gt; and &lt;code&gt;L=Length of pattern&lt;/code&gt; to describe them.&lt;/p&gt;
&lt;p&gt;Take the &lt;code&gt;E(3,8)&lt;/code&gt; rhythm, it's all over the place. In Elvis's Hound Dog, and 50s rockabilly. In Cuba it's the &lt;em&gt;Tresillo&lt;/em&gt; and you can find it in West African drum rhythms. We've also got &lt;code&gt;E(5,8)&lt;/code&gt; which again is present all over the world. The really interesting part is that certain rhythms have made their home more in certain parts of the world than others. The paper goes into more detail so just read it, and maybe use the widget above for playback.&lt;/p&gt;
&lt;p&gt;Back to where we were. The Euclidean algorithm gives us a fair amount of different rhythms. As you probably paused the widget above at some point, you noticed that it can get quite repetitive. Prime number divisions tend to be interesting but some rhythms like &lt;code&gt;E(4,8)&lt;/code&gt; is just a simple four-on-the floor groove. Many Euclidian sequencers allow you to rotate the pattern to create more variation, but you'd need to keep changing it to prevent it from becoming monotonous over time. On the other hand, this might even be desired for something percussive, a funky bassline or the ticking of hi-hats.&lt;/p&gt;
&lt;p&gt;Then modulate the tension by increasing or decreasing the number of hits in the pattern. Get wild, wire a 4-step sequencer that moves every bar, to the density of hits in a Euclidian pattern so that every bar the feeling changes and creates a much more interesting evolution over time. To me, that bridges the gap between a tool for short percussive rhythms, into something that might even be a melody when given a nice big heap of pitch changes.&lt;/p&gt;
&lt;p&gt;There is a more fundamental drawback to Euclidean Rhythms though, which is also their definition: Evenness.&lt;/p&gt;
&lt;p&gt;Or not really a drawback, but a lot of interesting rhythms have uneven rhythm placement. We can use them to explore the rhythm space, but have to keep on the roads. Another approach is to store a lot of interesting patterns and interpolate between them, Mutable Instruments Grids is an example of such a Eurorack module. But I feel like that gives less control and ability to insert intent into the shape of the rhythms.&lt;/p&gt;
&lt;p&gt;There are people experimenting in another direction. Shakmat Knight's gallop is a Eurorack module with a normal Euclidean mode, but also an Anti-Euclidean one. As far as I can tell from the manual, it makes an attempt to position hits as uneven as possible. Both the Knight's Gallop and another one called the lx-euclid with modes that cluster hits in the beginning or end. This gives us some more room in the rhythm space do dance around in.&lt;/p&gt;

&lt;p&gt;Change the interpolation control above to one of the extremes and what you get was one attempt to create something like an Anti-Euclidian rhythm by creating uneven spacing, but I wasn't really happy with the original outcome. There's some interesting grooves in there at lower densities and lengths, but at higher ones it just became too clumped together.&lt;/p&gt;
&lt;p&gt;In his writing, Toussaint discusses how different metrics can be used to describe the distance between two rhythms. That led me to try interpolating between the Euclidean and the Anti-Euclidean rhythm we just played with. It took a bit of tweaking, the key was to constrain the interpolation so that it only moved one hit at a time. This actually felt like exploring the rhythm space in a much more interesting way. The Anti-Euclidean algorithm tended to cluster hits in the beginning, so it made sense to add a mirror version of it as well.&lt;/p&gt;
&lt;p&gt;Nice. That's just one possible take on "optimize for uneven" and there might be more interesting parts of the rhythm space now that we're off-roading it. Another interesting experiment was clumping the hits, and then attempting to do something similar to Wavefolding where as peaks start to form we push them back in on itself, creating holes in the clusters that turn into new peaks.&lt;/p&gt;

&lt;p&gt;I don't know about you, but to me this version also has a couple of good grooves in its repertoire. A drawback is that as you increase the intensity you also get repeats of the same rhythms you've had before, lessening the feeling that you are exploring the rhythm space.&lt;/p&gt;
&lt;p&gt;Though again, setting one of these as the interpolation target and exploring what you find on the way is quite fun still, so I really liked that.&lt;/p&gt;
&lt;p&gt;I continued to play around with more ideas, thinking of different places in the rhythm space we could visit. Trying things like alternating clusters, but they didn't really result in more interesting rhythms. We could also do something like Euclidean subdivisions, treat the pattern as one of a subdivided length and use that to place the hits in clusters instead.&lt;/p&gt;
&lt;p&gt;The widget below collects a few of those experiments, it's less of a curated tour and more of a notebook of different attempts at this point, select an algorithm in the dropdown and make sure to play with interpolation slider.&lt;/p&gt;

&lt;p&gt;My main takeaway from the experimentation was that Euclidean rhythms are good (no surprise there), and to add some spice you don't need to stray too far away from them to get interesting syncopation. A rhythm that looks beautiful on a circle might still not sound so interesting, as I tried to take higher-order polynomial curves and map them to the hits. In the end, interpolating between a clustered version and the Euclidean was one of the best ways to explore.&lt;/p&gt;
&lt;p&gt;With that in mind, I wanted to try moving these experiments a bit further towards a sequencer that could support longer patterns, and combine it with some of the ideas I mentioned earlier, like changing density over time. The interpolation feature is key, and modulating hit density over time. The closest standalone sequencers I know would be either the Torso T1 or the Oxi one, but I don't think either really support do that outside of manually changing controls.&lt;/p&gt;
&lt;p&gt;I started expanding the widgets above into this, and it grew and grew until I split it out into a separate page. Firstly because then I don't need to scroll to the ends of the earth every time I refreshed the page. It also makes it easier for myself to bookmark for the future.&lt;/p&gt;
&lt;p&gt;Obligatory side-note: I'm writing this in the very tail end of 2025, a time where everything circles around AI in some way. If I had been writing a year or two back, all of the interactive widgets would probably have been images instead. They are not that hard to build, but involve a lot of micro-decisions that take time and energy. I do my faffing about in the mornings before work, so time and energy are not that abundant.&lt;/p&gt;
&lt;p&gt;This was an opportunity to experimenting with the latest batch of AI models while building as well. Claude Opus 4.5 seems like a clear step up from Sonnet 4.5 (but more annoying usage limits), and the new Gemini 3 Flash is also a strong contender. They struggled with some parts though. Neither had an easy time understanding how to make the Shift Register work well in the sequencer context, and I had to rewrite the core rhythm generation a couple of times to make it work the way I needed it to before handing it back.&lt;/p&gt;
&lt;p&gt;With that out of the way, follow the link below.&lt;/p&gt;

&lt;a href="https://pv.wtf/playground/euclidean-sequencer"&gt;Open Really Cool Sequencer&lt;/a&gt;

&lt;p&gt;Back? It might need a little bit of explanation.&lt;/p&gt;
&lt;p&gt;There are four different parts to a pattern, with either a global length or individual part lengths. And for each part we can control the density as an offset from the main one, so it's easy to perform by only tweaking the main one. For each we can also control the distance from the euclidean pattern.&lt;/p&gt;
&lt;p&gt;Now this I feel like has the seed of something that can be used beyond simple-ish percussion loops. Lets you get going quickly but also has enough control to shape what's happening, add intent.&lt;/p&gt;
&lt;p&gt;I added controls for rotation, inverting the rhythm, and manual overrides for forcing a hit or a rest. Having the same pitch for each hit started to get a bit tedious, so I added a Shift Register (also known as Turing Machine) inspired generator for pitch information as well.&lt;/p&gt;
&lt;p&gt;The thing that's really cool (alternatively: really confusing) is the Boolean mode. Normally each part plays in sequence, one after another. But in boolean mode they act as different layers that combine into one output. You can choose which one to listen too, and see what the effect of playing Part 1 OR Part 2 is. And since we already have a Shift Register for pitch, why not reuse it for beats as well?&lt;/p&gt;
&lt;p&gt;There are a couple of more features as well, they are hopefully either easy to understand or fun to click around on anyway so I'm pretty satisfied with this version. What comes next?&lt;/p&gt;
&lt;p&gt;I haven't written a VCV Rack plugin, but it might be fun to try as a next step. Or maybe add WebMIDI to the sequencer, either way would let me see how it plays with the Digitone. End result being that I spend some more time in my synth corner with tiny blinking lights rather than big laptop screens.&lt;/p&gt;
&lt;p&gt;That's it for this dance, see you next time!&lt;/p&gt;


&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://pv.wtf/posts/euclidean-rhythms</guid>
      <category>Hacker News</category>
      <pubDate>Mon, 22 Dec 2025 08:08:46 +0000</pubDate>
    </item>
    <item>
      <title>The post-GeForce era: What if Nvidia abandons PC gaming?</title>
      <link>https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html</link>
      <description>Imagine it’s the year 2030 and Nvidia has just announced its newest RTX 7000-series graphics cards. But the cheapest of the cards is priced over $2,000 and the top model is nearly double that. The series offer minimal uplift on rendering performance, but they’re incredibly good at accelerated upscaling and frame generation. Plus, memory bandwidth is almost double over the last-gen models.</description>
      <content:encoded>&lt;article class="post-3013044 post type-post status-publish format-standard has-post-thumbnail category-gaming category-components-graphics category-technology-business languages-en publication-pcworld publication-us-default story_types-feature origin-wp" id="post-3013044"&gt;
&lt;!-- .entry-header --&gt;
&lt;!-- &lt;hr class="wp-block-separator" /&gt; --&gt;




&lt;img alt="rtx 5070 ti" src="https://www.pcworld.com/wp-content/uploads/2025/12/rtx-5070-ti-1.jpg?quality=50&amp;amp;strip=all&amp;amp;w=1024"/&gt; 
Image: Adam Patrick Murray / Foundry 
&lt;!-- .post-thumbnail --&gt;


&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"&gt;


Summary created by Smart Answers AI&lt;h3&gt;In summary:&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;PCWorld explores Nvidia’s potential shift away from PC gaming as AI data center revenue reached $51.2 billion versus just $4.3 billion from gaming in Q3 2025.&lt;/li&gt;&lt;li&gt;Rising component costs and memory shortages driven by AI demand may force Nvidia to cut gaming GPU supply, with RTX 7000-series cards expected to start over $2,000.&lt;/li&gt;&lt;li&gt;This transition could push gaming toward cloud services like GeForce Now, fundamentally changing how consumers access high-end graphics performance through subscription models instead of hardware ownership.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Imagine it’s the year 2030 and Nvidia has just announced its newest RTX 7000-series graphics cards. But the cheapest of the cards is priced over $2,000 and the top model is nearly double that. The series offer minimal uplift on rendering performance, but they’re incredibly good at accelerated upscaling and frame generation. Plus, memory bandwidth is almost double over the last-gen models.&lt;/p&gt;
&lt;p&gt;Let’s continue the hypothetical: Nvidia’s new xx60-series cards aren’t expected for months while Nvidia stockpiles enough defective GPUs. But don’t worry if you can’t afford these new cards or don’t want to wait. Why? Because GeForce Now offers the full upgrade &lt;em&gt;right now&lt;/em&gt; for an “affordable” monthly fee, especially with an annual sub locked in.&lt;/p&gt;


&lt;p&gt;I wrote the above as a nightmare scenario, but it’s odd how close it sounds to the launch of the RTX 50-series. It’s a history that seems likely to repeat and accelerate as Nvidia’s gaming division becomes an ever-more-minor side hustle to its AI initiatives.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nvidia could effectively give up on gaming in the near future,&lt;/strong&gt; and that might be the most financially sensible thing to do if the AI bubble doesn’t burst. But what would happen if they did?&lt;/p&gt;


&lt;!--js block injected --&gt;




&lt;h2&gt;Just follow the money&lt;/h2&gt;
&lt;p&gt;The numbers behind my pessimistic prognosis paint a stark picture. &lt;a href="https://go.skimresources.com?id=111346X1569483&amp;amp;xs=1&amp;amp;url=https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-third-quarter-fiscal-2026&amp;amp;xcust=2-1-3013044-1-0-0-0-0&amp;amp;sref=https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html"&gt;Nvidia’s Q3 2025 revenue&lt;/a&gt; topped $57 billion. Guess how much of that money came from data centers? A whopping $51.2 billion. That’s just shy of 90% of its total revenue and represents a 25% increase over the previous quarter and a 66% increase year on year.&lt;/p&gt;
&lt;p&gt;How much revenue do you think Nvidia pulled in from gaming? A measly $4.3 billion by comparison. That’s down 1% on the previous quarter, and that’s despite having the most powerful graphics cards available &lt;em&gt;and&lt;/em&gt; with stock and prices being far more favorable than they were earlier in the year. &lt;a href="https://go.skimresources.com?id=111346X1569483&amp;amp;xs=1&amp;amp;url=https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-third-quarter-fiscal-2025&amp;amp;xcust=2-1-3013044-1-0-0-0-0&amp;amp;sref=https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html"&gt;It’s still up 30% on last year&lt;/a&gt;, but the difference in potential between data centers and gaming is &lt;em&gt;staggering&lt;/em&gt;.&lt;/p&gt;
&lt;img alt="Nvidia sign outside luxurious building" src="https://b2c-contenthub.com/wp-content/uploads/2025/12/Nvidia-sign-outside-luxurious-building.jpg?quality=50&amp;amp;strip=all&amp;amp;w=1200"/&gt;

&lt;p&gt;Nvidia&lt;/p&gt;
&lt;p&gt;Indeed, gaming makes up less than 8% of Nvidia’s total revenue as of now, and although the overall income from gaming continues to increase, it’s miniscule in comparison to its data center take. &lt;a href="https://go.skimresources.com?id=111346X1569483&amp;amp;xs=1&amp;amp;url=https://bullfincher.io/companies/nvidia-corporation/revenue-by-segment&amp;amp;xcust=2-1-3013044-1-0-0-0-0&amp;amp;sref=https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html"&gt;Bullfincher highlights how quickly that’s changed&lt;/a&gt;, too: just a few years ago, gaming represented over 33% of Nvidia’s total revenue.&lt;/p&gt;




&lt;p&gt;Where do you think it’s going to be in another five years? Assuming the AI bubble doesn’t pop as catastrophically as it could, gaming is going to become a tiny footnote on Nvidia’s balance sheet. Will Jensen Huang even bother doing gaming hardware keynotes at that point?&lt;/p&gt;
&lt;img alt="Nvidia Jensen Huang" src="https://b2c-contenthub.com/wp-content/uploads/2025/01/20250107_103508.jpg?quality=50&amp;amp;strip=all&amp;amp;w=1200"/&gt;

&lt;p&gt;Mark Hachman / IDG&lt;/p&gt;
&lt;p&gt;Nvidia might be the biggest megacorp in this space, but its contemporaries show similar gaming red flags on their balance sheets. &lt;a href="https://go.skimresources.com?id=111346X1569483&amp;amp;xs=1&amp;amp;url=https://ir.amd.com/news-events/press-releases/detail/1265/amd-reports-third-quarter-2025-financial-results&amp;amp;xcust=2-1-3013044-1-0-0-0-0&amp;amp;sref=https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html"&gt;AMD made just over $9 billion this past quarter&lt;/a&gt;, but $4.3 billion was from data center sales while only $1.3 billion came from gaming. That’s &lt;a href="https://go.skimresources.com?id=111346X1569483&amp;amp;xs=1&amp;amp;url=https://ir.amd.com/news-events/press-releases/detail/1224/amd-reports-third-quarter-2024-financial-results&amp;amp;xcust=2-1-3013044-1-0-0-0-0&amp;amp;sref=https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html"&gt;much better than last year&lt;/a&gt;—when data centers brought in $3.5 billion and gaming just $462 million—but data centers are still a far bigger portion of AMD’s revenue than gaming.&lt;/p&gt;
&lt;p&gt;These numbers make a compelling case for giving up some interest and investment in gaming hardware development. It doesn’t mean they’re going to stop make gaming GPUs entirely. (Or does it?) But if you’re Jensen Huang facing off against shareholders who are demanding the revenue numbers go up as much as possible as fast as possible, what are you going to sell them on: a new gaming GPU that has historically low margins, or a new generation of data center hardware to feed into the accelerating AI bubble with untold potential?&lt;/p&gt;
&lt;p&gt;You could even argue that Nvidia’s increasing focus over the past few years on DLSS and ray tracing over pure rasterization performance is an early sign of it putting its eggs in the data center basket.&lt;/p&gt;




&lt;h2&gt;A canary in the RAM mines&lt;/h2&gt;
&lt;p&gt;The biggest side effect of all these new data center builds hasn’t been GPU scarcity, surprisingly. (At least, not to the degree we saw during the cryptocurrency craze.) Rather, it’s &lt;a href="https://www.pcworld.com/article/3010391/ram-prices-are-painfully-out-of-control-4-ways-to-avoid-the-gouging.html"&gt;skyrocketing memory prices&lt;/a&gt;. RAM kits have increased in price by over 200 percent in some cases, making large capacity kits more costly than top-tier GPUs. Some modest RAM options are even more expensive than gaming consoles.&lt;/p&gt;
&lt;p&gt;Consumer RAM is shooting up in price because all the major memory manufacturers are &lt;a href="https://www.pcworld.com/article/2998935/ram-is-so-expensive-samsung-wont-even-sell-it-to-samsung.html"&gt;inundated with orders for data center memory&lt;/a&gt;, like HBM and LPDDR. Some have begun pivoting their fabrication lines to these higher-margin memory types, leading to shortages of NAND chips—and, consequently, &lt;a href="https://www.pcworld.com/article/3003682/memory-prices-are-climbing-its-making-these-5-other-products-cost-more.html"&gt;shortages of consumer memory and SSDs&lt;/a&gt;.&lt;/p&gt;
&lt;img alt="Micron Crucial RAM modules sticks DDR memory" src="https://b2c-contenthub.com/wp-content/uploads/2025/12/Micron-Crucial-RAM-modules-sticks-DDR-memory.jpg?quality=50&amp;amp;strip=all&amp;amp;w=1200"/&gt;

&lt;a href="https://go.skimresources.com?id=111346X1569483&amp;amp;xs=1&amp;amp;url=https://www.shutterstock.com/image-photo/bangkok-thailand-april-28-2023-ddr3l-2295520815&amp;amp;xcust=2-1-3013044-1-0-0-0-0&amp;amp;sref=https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html"&gt;Nor Gal / Shutterstock.com&lt;/a&gt;
&lt;p&gt;Those shortages are making RAM and SSDs far more expensive. And yet, despite the increased margins and diminishing supply versus demand, &lt;a href="https://www.pcworld.com/article/2999460/micron-will-end-crucial-consumer-brand-in-another-ai-casualty.html"&gt;Micron just closed its Crucial brand&lt;/a&gt; of consumer RAM and SSDs.&lt;/p&gt;
&lt;p&gt;It was profitable, it was popular, it had a distinct market niche that served consumers and gamers for decades. But even Micron didn’t see the point of keeping it going when it could instead make heaps more cash from selling Micron NAND chips and server memory.&lt;/p&gt;
&lt;p&gt;And &lt;a href="https://www.pcworld.com/article/3001184/i-hope-crucials-death-isnt-a-canary-in-a-pc-memory-coal-mine.html"&gt;if Micron is so willing to pull out&lt;/a&gt; of the consumer space due to AI-driven demand, how much more will Nvidia be tempted to do the same? What’s stopping Nvidia from reaching the same conclusion?&lt;/p&gt;
&lt;p&gt;For further proof of this future, Nvidia is &lt;a href="https://go.skimresources.com?id=111346X1569483&amp;amp;xs=1&amp;amp;url=https://overclock3d.net/news/gpu-displays/nvidia-plans-heavy-cuts-to-gpu-supply-in-early-2026/&amp;amp;xcust=2-1-3013044-1-0-0-0-0&amp;amp;sref=https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html"&gt;rumored to be cutting its gaming GPU supply in 2026&lt;/a&gt; due to memory shortages. It’s especially notable how Nvidia appears to be cutting the more affordable mid-range graphics cards first, leaving ultra-budget and ultra-high-end lines intact for now. Is this just the first step in Nvidia leaving gamers behind?&lt;/p&gt;




&lt;h2&gt;Where things could go from here&lt;/h2&gt;
&lt;p&gt;There are some intriguing comparisons to make between Nvidia and other big businesses that found growth and revenue in avenues that weren’t where they started. IBM went from being &lt;em&gt;the&lt;/em&gt; name in computing hardware to one that largely runs in the background. It sold off its core hardware businesses and became a software and services company that’s still worth tens of billions of dollars. It recently spun off again, creating a separate company to handle IT services while the core business refocused on cloud computing and AI.&lt;/p&gt;
&lt;p&gt;Nvidia could do that: spin or sell off its gaming divisions and license its GPU technology to that spun-or-sold-off subsidiary.&lt;/p&gt;
&lt;img alt="Nvidia GeForce Now Ultimate upgrade 1" src="https://b2c-contenthub.com/wp-content/uploads/2025/08/GeForce_NOW_Blackwell_KV_.jpg?quality=50&amp;amp;strip=all&amp;amp;w=1200"/&gt;

Notice the lack of graphics cards in this Nvidia promo image.&lt;p&gt;Nvidia&lt;/p&gt;
&lt;p&gt;Perhaps Nvidia could even end up like Adobe. In the mid-2010s, the developer of Photoshop launched Creative Cloud and slowly pushed all its once-in-perpetuity software licenses into a subscription model that’s still going on today. Could that apply to &lt;a href="https://www.pcworld.com/article/630335/geforce-now-review.html"&gt;Nvidia’s GeForce Now streaming service&lt;/a&gt;? It had 25 million subscribers as of 2023 and ran on GPUs designed for data center server racks. Nvidia could leave dedicated desktop and laptop GPUs behind entirely and pivot its gaming divisions into software/hardware-as-a-service firms.&lt;/p&gt;
&lt;p&gt;If gaming goes a similar way to TV and movie streaming, it’s possible Nvidia could even pull a Netflix and slowly de-emphasize its DVD-like hardware business in favor of powering it all from the cloud.&lt;/p&gt;




&lt;h2&gt;Gaming won’t die, but it will change&lt;/h2&gt;
&lt;p&gt;As much as this article is heavy on the doom, Nvidia is unlikely to exit gaming &lt;em&gt;entirely&lt;/em&gt;. People want to play games and there’s money to be made there, so &lt;em&gt;someone&lt;/em&gt; will keep tapping that market. But how that revenue is extracted may change—dramatically so.&lt;/p&gt;
&lt;p&gt;Microsoft is already talking about making the &lt;a href="https://www.pcworld.com/article/2953703/did-pcs-win-the-console-war-the-next-xbox-will-run-windows-report-claims.html"&gt;next Xbox more of a PC/console hybrid&lt;/a&gt;. And with the latest Xbox consoles being the third wheel of this generation, it wouldn’t be a surprise to see the future of Xbox focus more on &lt;em&gt;streaming&lt;/em&gt; games than buying/owning them. Xbox Game Pass already has over 37 million subscribers—that’s more than the number of Xbox Series X/S consoles sold this generation.&lt;/p&gt;
&lt;p&gt;Nvidia could do something similar. Or it could spin off. Or it could stop making gaming GPUs entirely. The only thing we know for sure is this: when a gaming company starts making astronomical amounts of money due to AI-driven demand, it’s hard to imagine it wouldn’t be tempted to dive head-first into an AI-first strategy at the expense of gaming.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further reading:&lt;/strong&gt; &lt;a href="https://www.pcworld.com/article/2542269/pcs-vs-consoles-the-future-of-gaming-looks-blurrier-than-ever.html"&gt;PC vs. consoles? Gaming’s future is blurrier than ever&lt;/a&gt;&lt;/p&gt;




 
&lt;h3&gt;
&lt;a href="https://www.pcworld.com/author/jon-martindale"&gt;
		Author: Jon Martindale&lt;/a&gt;, Contributor, PCWorld		&lt;/h3&gt;



&lt;img src="https://www.pcworld.com/wp-content/uploads/2025/12/author_photo_Jon-Martindale_1728563874-7.jpg?quality=50&amp;amp;strip=all&amp;amp;w=150&amp;amp;h=150&amp;amp;crop=1"/&gt;

&lt;p&gt;Jon Martindale is a voracious writer and technology fanboy who loves nothing more than digging into the specs of the latest graphics cards, processors, and displays. He's passionate about everything PC, but also enjoys experimenting with AIs, and covering new standing desks that can help avoid his worst posture habits.&lt;/p&gt;


&lt;h3&gt; Recent stories by Jon Martindale:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href="https://www.pcworld.com/article/3003682/memory-prices-are-climbing-its-making-these-5-other-products-cost-more.html"&gt;
									RAM costs are skyrocketing—and these 5 other gadgets are paying the price								&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://www.pcworld.com/article/2975248/i-used-chatgpt-to-turn-my-old-laptop-into-an-alien-rpg-muthur-terminal.html"&gt;
									I used ChatGPT to turn my old laptop into an Alien RPG MUTHUR terminal								&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href="https://www.pcworld.com/article/2945507/the-future-of-laptops-is-already-here-6-trends-making-2025-a-breakout-year.html"&gt;
									The future of laptops is already here: 6 trends making 2025 a breakout year								&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;







 


&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://www.pcworld.com/article/3013044/the-post-geforce-era-what-if-nvidia-abandons-pc-gaming.html</guid>
      <category>Hacker News</category>
      <pubDate>Sat, 20 Dec 2025 10:25:38 +0000</pubDate>
    </item>
    <item>
      <title>The Polyglot NixOS</title>
      <link>https://x86.lol/generic/2025/12/19/polyglot.html</link>
      <description>Recently a colleague mentioned building
NixOS images that run unchanged on multiple architectures. Given the past
adventures on this blog with systemd-repart and cross-compiling NixOS , I decide to give this a
go.</description>
      <content:encoded>&lt;article class="post h-entry" itemscope="" itemtype="http://schema.org/BlogPosting"&gt;


&lt;p&gt;Recently a &lt;a href="https://github.com/samueldr"&gt;colleague&lt;/a&gt; mentioned building
NixOS images that run unchanged on multiple architectures. Given the past
adventures on this blog with &lt;a href="https://x86.lol/generic/2024/08/28/systemd-sysupdate.html"&gt;systemd-repart&lt;/a&gt; and
&lt;a href="https://x86.lol/generic/2024/09/21/cross-compile-riscv.html"&gt;cross-compiling NixOS&lt;/a&gt;, I decide to give this a
go.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; You can find a quick’n’dirty implementation
&lt;a href="https://github.com/blitz/polyglot-image"&gt;here&lt;/a&gt;. Check the repo for
details on how to build and run it.&lt;/p&gt;
&lt;p&gt;So do we want to do: We want to build &lt;em&gt;one&lt;/em&gt; disk image that boots on
x86_64, ARM AArch64, and RISC-V 64-bit. We limit ourselves here to
UEFI platforms, which makes this pretty straight forward.&lt;/p&gt;
&lt;p&gt;From a high-level we need to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Have a NixOS configuration.&lt;/li&gt;
&lt;li&gt;Build the system closure for each target.&lt;/li&gt;
&lt;li&gt;Throw everything into one &lt;code&gt;/nix/store&lt;/code&gt; partition.&lt;/li&gt;
&lt;li&gt;Populate the &lt;a href="https://en.wikipedia.org/wiki/EFI_system_partition"&gt;ESP&lt;/a&gt; to
boot the right closure depending on the architecture.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All of this is surprisingly straight-forward. The ESP has
architecture-dependent default filenames for what the firmware should
boot, given no other configuration. This means we can build an
&lt;a href="https://wiki.archlinux.org/title/Unified_kernel_image"&gt;UKI&lt;/a&gt; per
architecture and drop it at the right place in the ESP
(&lt;code&gt;/EFI/BOOT/BOOTX64.EFI&lt;/code&gt; for 64-bit x86) and we are done!&lt;/p&gt;
&lt;p&gt;By linking the system’s UKI in these locations on the ESP, we skip
over having an actual bootloader and thus can’t have multiple
generations, but it makes for a much leaner example!&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/blitz/polyglot-image"&gt;example repo&lt;/a&gt; puts the
closure for each architecture in a single Nix store partition. I
&lt;em&gt;thought&lt;/em&gt; this would bring some space savings, because files that are
not binary code should be largely the same. This doesn’t really pan
out in this small example and we only save a couple of percent. Maybe
it makes a bigger difference for larger closures.&lt;/p&gt;
&lt;p&gt;If you want to dig into the details, the &lt;a href="https://github.com/blitz/polyglot-image"&gt;example
repo&lt;/a&gt; has the instructions
how to build and boot the image. I’m also eager to see someone
building a more comprehensive version of this that includes a fully
functioning bootloader and multiple generations!&lt;/p&gt;

&lt;a href="https://x86.lol/generic/2025/08/10/change-monitoring.html"&gt;Quick and Dirty Website Change Monitoring 👈&lt;/a&gt;
&lt;a href="https://x86.lol/generic/2025/12/19/polyglot.html"&gt;&lt;/a&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://x86.lol/generic/2025/12/19/polyglot.html</guid>
      <category>Hacker News</category>
      <pubDate>Sat, 20 Dec 2025 07:39:00 +0000</pubDate>
    </item>
    <item>
      <title>Perfect Software – Software for an Audience of One</title>
      <link>https://outofdesk.netlify.app/blog/perfect-software</link>
      <description>Perfect Software - Software for an Audience of One</description>
      <content:encoded>&lt;div class="content col-span-12 text-justify mb-4 md:mb-4"&gt;&lt;h2&gt;&lt;!-- HTML_TAG_START --&gt;Perfect Software - Software for an Audience of One&lt;!-- HTML_TAG_END --&gt;&lt;/h2&gt;&lt;/div&gt;</content:encoded>
      <guid isPermaLink="false">https://outofdesk.netlify.app/blog/perfect-software</guid>
      <category>Hacker News</category>
      <pubDate>Sat, 20 Dec 2025 07:14:53 +0000</pubDate>
    </item>
    <item>
      <title>Here you can find the contents of the Unix v4 tape ready for bootstrapping</title>
      <link>http://squoze.net/UNIX/v4/README</link>
      <description>Here you can find the contents of
the UNIX v4 tape ready for bootstrapping, including a tar file of the filesystem.</description>
      <content:encoded>&lt;body&gt;



&lt;a href="https://github.com/aap/"&gt;github&lt;/a&gt; |
      &lt;a href="https://mastodon.sdf.org/@aap"&gt;mastodon&lt;/a&gt; |
      &lt;a href="http://tx-0.net"&gt;TX-0&lt;/a&gt; |
      &lt;a href="http://pdp-1.net"&gt;PDP-1&lt;/a&gt; |
      &lt;a href="http://pdp-6.net"&gt;PDP-6/10&lt;/a&gt; |
    

Related sites:
      | &lt;a href="http://squoze.net/sitemap"&gt;site map&lt;/a&gt;



&lt;h1&gt;&lt;a href="http://squoze.net/"&gt;squoze.net &lt;/a&gt;&lt;/h1&gt;

&lt;br/&gt;



&lt;ul&gt;
&lt;li&gt;&lt;a href="http://squoze.net/B/"&gt;› B/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/C/"&gt;› C/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/NB/"&gt;› NB/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/"&gt;» UNIX/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/32v/"&gt;› 32v/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/V6_on_rl01"&gt;› V6 on rl01&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/bltj/"&gt;› bltj/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/fs/"&gt;› fs/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/restored/"&gt;› restored/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/sysIII_pdp11/"&gt;› sysIII pdp11/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/sysIII_vax/"&gt;› sysIII vax/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/sysV_pdp11/"&gt;› sysV pdp11/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v1man/"&gt;› v1man/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v2man/"&gt;› v2man/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v3man/"&gt;› v3man/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v4/"&gt;» v4/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v4/README"&gt;» README&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v4man/"&gt;› v4man/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v5/"&gt;› v5/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v5man/"&gt;› v5man/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v6/"&gt;› v6/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v6man/"&gt;› v6man/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v7/"&gt;› v7/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v8/"&gt;› v8/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/ling/"&gt;› ling/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/math/"&gt;› math/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/misc/"&gt;› misc/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/plan_9/"&gt;› plan 9/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;h1&gt;UNIX Fourth Edition&lt;/h1&gt;
&lt;p&gt;Here you can find the contents of
the &lt;a href="https://archive.org/details/utah_unix_v4_raw"&gt;UNIX v4 tape&lt;/a&gt;
ready for bootstrapping, including a tar file of the filesystem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v4/unix_v4.tap"&gt;unix_v4.tap&lt;/a&gt; is the original tape file in simh format&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v4/bootstrap"&gt;bootstrap&lt;/a&gt; are the first 38400 bytes of the tape&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v4/disk.rk"&gt;disk.rk&lt;/a&gt; is the rest of the tape, an RK05 image&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v4/unix_v4.tar"&gt;unix_v4.tar&lt;/a&gt; is the filesystem extracted&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v4/install.ini"&gt;install.ini&lt;/a&gt; is an ini file for simh to install the system&lt;/li&gt;
&lt;li&gt;&lt;a href="http://squoze.net/UNIX/v4/boot.ini"&gt;boot.ini&lt;/a&gt; is an ini file for simh to boot the system&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At first I extracted the disk image manually from the tape,
which resulted in &lt;code&gt;bootstrap&lt;/code&gt; and &lt;code&gt;disk.rk&lt;/code&gt;.
These are really nothing more than
the first 38400 bytes of the raw tape content and the rest.
Because &lt;code&gt;unix_v4.tap&lt;/code&gt; is block based, one first has to strip it of its block sizes
to get the raw content.&lt;/p&gt;
&lt;p&gt;Actually it's easier to just use the tape as it is and install a new system from it.&lt;/p&gt;
&lt;h1&gt;Installing the system&lt;/h1&gt;
&lt;p&gt;To install the system we just dump an RK05 disk image from tape to disk:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% pdp11 install.ini

[...]
        ; boot from TM0, now in mboot
=list
dldr
dtf
list
mboot
mcopy
rkf
tboot
uboot
=mcopy
'p' for rp; 'k' for rk
k
disk offset
0
tape offset
75
count
4000
=
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Afterwards, we can just load &lt;code&gt;uboot&lt;/code&gt; from tape to start UNIX:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;=uboot
k
unix
mem = 64530

login: root
# ls
bin
dev
etc
lib
mnt
tmp
unix
usr
# sync
# sync
# sync
# ^E        ; end emulation
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Running the system&lt;/h1&gt;
&lt;p&gt;To boot the system we don't need the tape.
Instead, we load &lt;code&gt;uboot&lt;/code&gt; directly from the boot sector.
We specify &lt;code&gt;k&lt;/code&gt; for RK05, then the filename &lt;code&gt;unix&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;% pdp11 boot.ini

[...]
        ; boot from RK0, now in uboot
k
unix
mem = 64530

login: root
#
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Rebuilding the kernel&lt;/h1&gt;
&lt;p&gt;Put the following into &lt;code&gt;/usr/sys/run&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rm -f low.o mch.o conf.o lib1 lib2

chdir ken
cc -c *.c
sh mklib
rm *.o

chdir ../dmr
cc -c *.c
sh mklib
rm *.o

chdir ..
cc -c conf/conf.c
mv conf/conf.o conf.o
as conf/low.s
mv a.out low.o
as conf/mch.s
mv a.out mch.o
ld -x low.o mch.o conf.o lib1 lib2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;/usr/sys/ken/mklib&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ar r ../lib1 main.o
ar r ../lib1 alloc.o
ar r ../lib1 iget.o
ar r ../lib1 prf.o
ar r ../lib1 rdwri.o
ar r ../lib1 slp.o
ar r ../lib1 subr.o
ar r ../lib1 text.o
ar r ../lib1 trap.o
ar r ../lib1 sig.o
ar r ../lib1 sysent.o
ar r ../lib1 sys1.o
ar r ../lib1 sys2.o
ar r ../lib1 sys3.o
ar r ../lib1 sys4.o
ar r ../lib1 nami.o
ar r ../lib1 fio.o
ar r ../lib1 clock.o
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;/usr/sys/dmr/mklib&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ar r ../lib2 bio.o
ar r ../lib2 tty.o
ar r ../lib2 malloc.o
ar r ../lib2 pipe.o
ar r ../lib2 cat.o
ar r ../lib2 dc.o
ar r ../lib2 dn.o
ar r ../lib2 dc.o
ar r ../lib2 dn.o
ar r ../lib2 dp.o
ar r ../lib2 kl.o
ar r ../lib2 mem.o
ar r ../lib2 pc.o
ar r ../lib2 rf.o
ar r ../lib2 rk.o
ar r ../lib2 tc.o
ar r ../lib2 tm.o
ar r ../lib2 vs.o
ar r ../lib2 vt.o
ar r ../lib2 partab.o
ar r ../lib2 rp.o
ar r ../lib2 lp.o
ar r ../lib2 dhdm.o
ar r ../lib2 dh.o
ar r ../lib2 dhfdm.o
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then in &lt;code&gt;/usr/sys&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# sh run
alloc.c:
clock.c:
fio.c:
iget.c:
main.c:
nami.c:
prf.c:
rdwri.c:
sig.c:
60: Warning: assignment understood
61: Warning: assignment understood
slp.c:
subr.c:
sys1.c:
sys2.c:
sys3.c:
sys4.c:
sysent.c:
text.c:
trap.c:
bio.c:
cat.c:
dc.c:
dh.c:
dhdm.c:
dhfdm.c:
dn.c:
dp.c:
dv.c:
kl.c:
lp.c:
malloc.c:
mem.c:
partab.c:
pc.c:
pipe.c:
rf.c:
rk.c:
rp.c:
tc.c:
tm.c:
tty.c:
vs.c:
vt.c:
#
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now install and boot the new kernel:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# mv a.out /nunix
# sync
# sync
# sync
# ^E
Simulation stopped, PC: 002040 (MOV (SP)+,177776)
sim&amp;gt; b rk
k
nunix
mem = 64529

login: root
#
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;TODO&lt;/h1&gt;
&lt;p&gt;There are a bunch of other things I would like to document
(or do in the first place):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;automatic installation (expect)&lt;/li&gt;
&lt;li&gt;configure kernel (/dev/mem missing)&lt;/li&gt;
&lt;li&gt;add device files&lt;/li&gt;
&lt;li&gt;add man pages&lt;/li&gt;
&lt;li&gt;try rp disk&lt;/li&gt;
&lt;li&gt;try rf swap (ps assumes rf0)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reconstruction:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;get pre-v4 nsys kernel to work (buffer handling seems broken, i suspect synchronization bugs)&lt;/li&gt;
&lt;li&gt;get B restoration to work&lt;/li&gt;
&lt;/ul&gt;


&lt;a href="http://werc.cat-v.org/"&gt;Powered by werc&lt;/a&gt;
&lt;!-- TODO Maybe should add a programatically generated google search box --&gt;
&lt;a href="http://squoze.net/_users/login"&gt;User Login&lt;/a&gt;
&lt;br/&gt;
&lt;br/&gt;

&lt;/body&gt;</content:encoded>
      <guid isPermaLink="false">http://squoze.net/UNIX/v4/README</guid>
      <category>Hacker News</category>
      <pubDate>Sat, 20 Dec 2025 05:17:32 +0000</pubDate>
    </item>
    <item>
      <title>The Coffee Warehouse</title>
      <link>https://www.scopeofwork.net/the-coffee-warehouse/</link>
      <description>I have a bit of a love hate relationship with Starbucks. It feels expensive. The lines are long. And I resent the fact that I give them an interest-free loan every time I use their mobile app . But my go-to Pike and banana nut loaf are delicious, and the baristas at my preferred location are fun. They give me a hard time if I deviate from my usual, which I appreciate. When I’m feeling uncertain about the day, making a Starbucks run is a surprisingly good way to get my head on straight.</description>
      <content:encoded>&lt;article class="post tag-feature content-wrap post-access-paid"&gt;


&lt;p&gt;I have a bit of a love hate relationship with Starbucks. It feels expensive. The lines are long. And I resent the fact that I give them an &lt;a href="https://www.morningstar.com/news/marketwatch/20250430172/starbucks-customers-are-giving-the-company-over-200-million-of-free-money?ref=scopeofwork.net"&gt;interest-free loan every time I use their mobile app&lt;/a&gt;. But my go-to Pike and banana nut loaf are delicious, and the baristas at my preferred location are fun. They give me a hard time if I deviate from my usual, which I appreciate. When I’m feeling uncertain about the day, making a Starbucks run is a surprisingly good way to get my head on straight.&lt;/p&gt;&lt;p&gt;On a recent visit, I arrived at the pickup counter and found my order incomplete. The slice of banana bread had been warmed, bagged, and labeled, but my cup of black coffee had yet to be dispensed into its paper cup. Looking behind the bar, I saw the usual blur of green-aproned baristas moving from task to task. It was frenetic, but organized too, with defined work areas and clear routines. The scene reminded me of my days working at a distribution center. The area where we packed orders had a similar manic choreography, and watching the baristas go about their jobs I found myself trying to understand their order flows and processes. Specifically, I wanted to understand how Starbucks organizes and prioritizes its work.&lt;/p&gt;&lt;h3&gt;Customer Flexibility vs. Operational Complexity.&lt;/h3&gt;&lt;p&gt;Starbucks is in a bit of a slump. Sales in established locations have &lt;a href="https://www.wsj.com/business/hospitality/starbucks-says-its-making-progress-on-quest-to-fulfill-orders-more-quickly-39492de6?st=SHLAvf&amp;amp;reflink=desktopwebshare_permalink&amp;amp;ref=scopeofwork.net"&gt;fallen for 5 consecutive quarters&lt;/a&gt;, contributing to a recent change in leadership. In an attempt to win back customers, the new CEO, Brian Niccol, has made operations a focus and pledged to reduce wait times and improve the customer experience. They are investing heavily in their order sorting algorithms and store processes, with the topic getting conspicuous attention in recent &lt;a href="https://s203.q4cdn.com/326826266/files/doc_financials/2025/q2/SBUX-2Q25-Corrected-Transcript.pdf?utm_source=chatgpt.com"&gt;earnings calls&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Some of the operational challenges stem from &lt;a href="https://www.qsrmagazine.com/story/the-fix-at-starbucks-had-to-start-with-mobile-ordering/?ref=scopeofwork.net"&gt;the increasing importance of their mobile app&lt;/a&gt;. &lt;a href="https://archive.starbucks.com/record/going-mobile?ref=scopeofwork.net"&gt;Since 2015&lt;/a&gt;, Starbucks has allowed customers to place orders remotely, before they arrive at the store. This grants convenience and flexibility, offloads the labor associated with order entry, and as mentioned previously, encourages customers to give them free loans to earn “mobile rewards.” But this convenience and savings come at the cost of operational complexity. There are three sales channels at a typical Starbucks today: walk-ins, drive thru, and mobile. Drinks are processed in the order received, whether placed in person or through the mobile app. This first-in, first-out system creates challenges, particularly at busy times. Operational capacity is often devoted to mobile customers who have yet to arrive, while those already at the store grow impatient. The staging area gets crowded with completed drinks, leading to that awkward seek and find many of us have experienced. &lt;/p&gt;&lt;p&gt;Though mobile orders create challenges, they also represent a kind of operational opportunity. They are different from orders placed through the traditional sales channels, where customers are present at the restaurant and presumably want their coffee as soon as they can get it. With mobile, customers generally place the order before they arrive and don’t care precisely when it is finished, as long as it is complete, and reasonably fresh, when they get there. This arrival delay makes it sensible to consider processing work &lt;em&gt;out of sequence&lt;/em&gt;.&lt;/p&gt;&lt;h3&gt;Warehouse work&lt;/h3&gt;&lt;p&gt;Back in my warehousing days, we thought a lot about how and when we processed work. Through our website, customers could place orders at any time of the day or night, and we committed to getting them their stuff in two days. The parcel carriers needed most of this time to get the shipment to the customer, but we generally had a few hours to fill the order, pack it, and load it onto the truck. We took advantage of this window to operate more efficiently. A few principles guided our thinking:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Urgent Work First:&lt;/strong&gt; We prioritized packages that had time constraints. If an order’s truck was leaving soon, we’d complete it first — even if there were other orders that had come in before it.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Stay In Sync:&lt;/strong&gt; Many customers ordered more than one thing. Even if the items were stored in areas far from one another, the customer would still expect them to show up in the same box. We would begin processing all components of that order simultaneously and only when we had enough capacity for all the processes involved.  As a result, the components of an order would arrive in the packing area at a similar time, where they would be combined and put on a truck. This reduced the amount of incomplete work floating around the warehouse, which in turn reduced opportunities for error.&lt;br/&gt;&lt;br/&gt;A consequence of the above is that simple orders (with fewer work areas involved) tended to move through the system faster. They didn’t get held up behind large and complex shipments that gobbled up lots of warehouse capacity.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Batch Work Where Appropriate&lt;/strong&gt;: The longer the work queue, the more likely you are to see collections of similar jobs in the queue. So we designed work areas to take advantage of this phenomenon, batching jobs that used the same skills and materials. &lt;br/&gt;&lt;br/&gt;Imagine a work area where people pack boxes. A packer will complete two identical boxes more quickly and accurately if she does them back to back, as the packages require the same materials (container, tape, infill, and packing surface) and skills. If she needs to shift to some other task in between, say to pack a different type of package, the process will naturally go more slowly. This is pretty much the same principle that makes assembly lines work: Keep people focused on the same task, and they will perform that task more efficiently.&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Starbucks as a Coffee Distribution Center&lt;/h3&gt;&lt;p&gt;The work queues at Starbucks are measured in minutes of work rather than hours. Still, there are a number of warehousing principles they could easily adopt.&lt;/p&gt;




&lt;p&gt;&lt;em&gt;Scope of Work is supported by members who love these kinds of operational deep dives. Join as a paid subscriber to support thoughtful, original work like this.&lt;/em&gt;&lt;/p&gt;

&lt;a href="http://scopeofwork.net/membership?ref=scopeofwork.net"&gt;
                            Support SOW &amp;amp; Unlock More →
                        &lt;/a&gt;





&lt;h3&gt;Read the full story&lt;/h3&gt;
&lt;p&gt;
      The rest of this post is for paid members only. Sign up now to read the full post — and all of Scope of Work’s other paid posts.
    &lt;/p&gt;
&lt;a href="https://www.scopeofwork.net/signup/"&gt;Sign up now&lt;/a&gt;

Already have an account?
&lt;a href="https://www.scopeofwork.net/signin/"&gt;Sign in&lt;/a&gt;



&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://www.scopeofwork.net/the-coffee-warehouse/</guid>
      <category>Hacker News</category>
      <pubDate>Fri, 19 Dec 2025 20:16:27 +0000</pubDate>
    </item>
    <item>
      <title>Astrophotography Target Planner: Discover Hidden Nebulas</title>
      <link>https://astroimagery.com/techniques/imaging/astrophotography-target-planner/</link>
      <description>Posted by</description>
      <content:encoded>&lt;main class="wp-block-group alignfull no-margin no-padding is-layout-constrained wp-container-core-group-is-layout-da976f36 wp-block-group-is-layout-constrained" style="padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px"&gt;&lt;a href="https://astroimagery.com/techniques/imaging/"&gt;Astro Imaging&lt;/a&gt;&lt;h1&gt;Astrophotography Target Planner: Discover Hidden Nebulas with My New App&lt;/h1&gt;&lt;p&gt;Posted by&lt;/p&gt;&lt;p&gt;Karl Perera MA&lt;/p&gt;&lt;p&gt;–&lt;/p&gt;December 17, 2025&lt;p&gt;Have you ever gone out on a clear night, fired up &lt;a href="https://stellarium.org/"&gt;Stellarium&lt;/a&gt;, scrolled through endless objects… and still ended up shooting Andromeda for the seventh time?&lt;/p&gt;&lt;p&gt;That was me, over and over. I love M31, but at some point I realised I wasn’t really exploring the sky anymore – I was just defaulting to the same “safe” ten targets. So I ended up building my own astrophotography target planner. The process of finding something new that was visible, well placed, matched my focal length, and wasn’t completely impossible from my Bortle 5 backyard was just too much friction.&lt;/p&gt;&lt;p&gt;So I built a tool to fix that:&lt;/p&gt;&lt;img alt="Astrophotography target planner app" src="https://astroimagery.com/wp-content/uploads/2025/12/Screenshot-2025-11-22-110231.png"/&gt;How my planner app looks on a desktop&lt;p&gt;My Astrophotography Target Planner helped me discover objects like NGC 7822 and the &lt;a href="https://www.constellation-guide.com/question-mark-nebula-ngc-7822/"&gt;Question Mark Nebula&lt;/a&gt; – targets I genuinely didn’t know existed a few months ago. In this article, I’ll break down the main ideas from the video so you can see how it works and whether it might actually help you plan your own sessions.&lt;/p&gt;&lt;p&gt;Watch the full walkthrough of my Astrophotography Target Planner here:&lt;/p&gt;
&lt;h3&gt;What You’ll Learn in This Video&lt;/h3&gt;&lt;p&gt;In the video, I walk through how I’ve gone from “winging it” every clear night to planning genuinely new targets in minutes.&lt;/p&gt;&lt;p&gt;You’ll see:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;How I used to plan targets manually with Stellarium (and why I kept ending up on the same objects).&lt;/li&gt;&lt;li&gt;What the Astro Target app does differently – filtering by location, visibility, &lt;a href="https://astroimagery.com/equipment/how-to-choose-the-best-focal-length-for-deep-sky-objects/"&gt;focal length&lt;/a&gt;, and difficulty.&lt;/li&gt;&lt;li&gt;How “Discovery Mode” surfaces lesser‑known nebulae that are actually well suited to your gear.&lt;/li&gt;&lt;li&gt;Examples of hidden gems like NGC 7822, the Headphone Nebula, and the Question Mark Nebula.&lt;/li&gt;&lt;li&gt;How the app’s timing, RA/Dec, and difficulty notes help you decide if a target is realistic from your sky.&lt;/li&gt;&lt;li&gt;Why this change in workflow helped me go from ~10 repeat targets to over 40 different &lt;a href="https://astroimagery.com/astrophotography/deep-space-astrophotography/brightest-deep-sky-objects/"&gt;deep sky objects&lt;/a&gt; in a year.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Key Takeaways&lt;/h3&gt;&lt;h4&gt;From Manual Hunting to Smart Target Selection&lt;/h4&gt;&lt;p&gt;For years, my target selection routine looked like this:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Open Stellarium.&lt;/li&gt;&lt;li&gt;Scroll through a long list of objects.&lt;/li&gt;&lt;li&gt;Check what’s visible tonight.&lt;/li&gt;&lt;li&gt;Check the moon phase.&lt;/li&gt;&lt;li&gt;Check if it fits my focal length.&lt;/li&gt;&lt;li&gt;Repeat until my patience runs out.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;After 15–20 minutes of that, I’d usually shrug and point the rig back at something familiar. Usually Andromeda. Again.&lt;/p&gt;&lt;p&gt;The problem wasn’t that there’s a lack of interesting objects – it’s the friction. Every step requires you to cross‑check: altitude, timing, framing, difficulty, and sky conditions. None of that is hard, but together it adds up, especially on a work night when you just want to get something in the can.&lt;/p&gt;&lt;p&gt;Astro Target is basically my way of compressing that whole process into a couple of clicks. You put in:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Your sky quality (for me: suburban Bortle 5),&lt;/li&gt;&lt;li&gt;Your focal length (e.g. 650 mm),&lt;/li&gt;&lt;li&gt;Your camera (sensor size is stored automatically),&lt;/li&gt;&lt;li&gt;What types of targets you want (e.g. Nebula only),&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;…and it just returns a short list of objects that are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Visible from your exact location tonight,&lt;/li&gt;&lt;li&gt;Well framed in your field of view,&lt;/li&gt;&lt;li&gt;Tagged by difficulty (beginner / intermediate / advanced),&lt;/li&gt;&lt;li&gt;And annotated with basic imaging notes.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;How This Astrophotography Target Planner Works&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Try the Astrophotography Target Planner (beta) here:&lt;/strong&gt;&lt;/p&gt;&lt;a href="https://astrotarget-planner-30718389249.us-west1.run.app/"&gt;AstroTarget Planner&lt;/a&gt;&lt;h4&gt;What “Discovery Mode” Actually Does&lt;/h4&gt;&lt;p&gt;The real game changer for me has been Discovery Mode.&lt;/p&gt;&lt;p&gt;Instead of showing you the usual greatest hits, Discovery Mode prioritises objects that almost nobody photographs – the hidden gems that are still practical for a typical backyard rig.&lt;/p&gt;&lt;p&gt;A few examples that popped up for me:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;NGC 7822&lt;/strong&gt; – a beautiful emission nebula I’d literally never thought about imaging before.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;The Headphone Nebula (Jones–Emberson 1)&lt;/strong&gt; – an object I’d never heard of until the astrophotography target planner for deep sky objects surfaced it.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;The Witch Head Nebula (IC 2118)&lt;/strong&gt; – a large, faint reflection nebula that matches my FOV surprisingly well.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;The Question Mark Nebula&lt;/strong&gt; – another one of those “how did I not know this existed?” moments.&lt;/li&gt;&lt;/ul&gt;&lt;img alt="Results from the astrophotography target planning app" src="https://astroimagery.com/wp-content/uploads/2025/12/Untitled-7-6-1024x576.png"/&gt;A few targets I found on my planning app&lt;p&gt;For each object, this planner for new astrophotography targets gives you:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The optimal imaging time for &lt;em&gt;tonight&lt;/em&gt; (e.g. “Best around 4:00 a.m. when it’s highest in the sky”).&lt;/li&gt;&lt;li&gt;RA/Dec coordinates so you can slew to it easily.&lt;/li&gt;&lt;li&gt;A size estimate and how it frames in your setup.&lt;/li&gt;&lt;li&gt;A difficulty rating and notes like “faint, requires many hours of total exposure from Bortle 5” or “large, challenging to separate from sky glow.”&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;That last bit is important. It’s honest about whether a target is going to be a slog in mediocre skies. You still have to decide if it’s worth the effort, but at least you’re making that decision up front instead of after three wasted hours.&lt;/p&gt;&lt;h4&gt;How This Changed My Imaging Year&lt;/h4&gt;&lt;p&gt;The difference in practice has been big.&lt;/p&gt;&lt;p&gt;When I did things manually, I timed it: about &lt;strong&gt;13 minutes&lt;/strong&gt; just to end up on a target I’d already shot before.&lt;/p&gt;&lt;p&gt;With Discovery Mode, it took about &lt;strong&gt;85 seconds&lt;/strong&gt; to find something entirely new that:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Was visible that night,&lt;/li&gt;&lt;li&gt;Fit my 650 mm FOV,&lt;/li&gt;&lt;li&gt;And wasn’t completely insane to attempt from Bortle 5.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;That’s how I went from essentially cycling through the same 10 comfortable objects… to aiming for &lt;strong&gt;40+ different targets&lt;/strong&gt; in a year.&lt;/p&gt;&lt;p&gt;To put it another way:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;This is my &lt;strong&gt;seventh&lt;/strong&gt; Andromeda image.&lt;/li&gt;&lt;li&gt;This is my &lt;strong&gt;first&lt;/strong&gt; Question Mark Nebula.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Both are valid choices. But one of them feels like genuine exploration again.&lt;/p&gt;&lt;p&gt;And that, more than anything, is what I wanted back – the sense that I’m still discovering things in the night sky, not just perfecting the same photos over and over.&lt;/p&gt;&lt;p&gt;A couple of notes for transparency:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;The target planner app is still &lt;strong&gt;in beta&lt;/strong&gt;. I built it primarily for myself because I was frustrated with the manual workflow.&lt;/li&gt;&lt;li&gt;It currently works &lt;strong&gt;best for northern hemisphere&lt;/strong&gt; targets. I’m actively adding more southern targets.&lt;/li&gt;&lt;li&gt;It’s &lt;strong&gt;free while I’m testing it&lt;/strong&gt;. The link is in the video description (and I’ll add it here too when this goes live).&lt;/li&gt;&lt;li&gt;If you find bugs or have ideas, I genuinely want to hear them – I’m shaping it around what you actually need out there.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Gear Used in This Video&lt;/h3&gt;&lt;p&gt;Some of the links I use are affiliate links, which means they help support the channel at no extra cost to you. I only recommend gear I actually use in my own imaging.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Imaging Rig&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Lens: &lt;strong&gt;Samyang 135mm F2.0&lt;/strong&gt; – &lt;a href="https://amzn.to/3KYXUkn"&gt;Check on Amazon&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Mount: &lt;strong&gt;Ioptron CEM26&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;Imaging camera: &lt;strong&gt;ZWO ASI533 MCPRO&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;Guide scope: &lt;strong&gt;SVBony 60mm&lt;/strong&gt; – &lt;a href="https://amzn.to/4j3ESWNhttps://amzn.to/4j3ESWN"&gt;Check same model by Astromania&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Guide camera: &lt;strong&gt;SV305&lt;/strong&gt; – &lt;a href="https://amzn.to/4qlUZS5"&gt;Check on Amazon&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Control &amp;amp; Accessories&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Filter(s) : &lt;strong&gt;Optolong LPro&lt;/strong&gt; – &lt;a href="https://amzn.to/44y5ucq"&gt;Check on Amazon&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Software&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Astro Target app (Discovery Mode) – &lt;a href="https://astrotarget-planner-30718389249.us-west1.run.app/"&gt;AstroTarget Planner&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Stellarium (for comparison / star hopping) – &lt;a href="https://stellarium.org/"&gt;Stellarium Astronomy Software&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Feel free to swap these for your own equivalents – the app doesn’t require this exact setup, it just needs your focal length and sensor size to do its thing.&lt;/p&gt;&lt;h3&gt;What to Watch / Read Next&lt;/h3&gt;&lt;p&gt;If this kind of planning and target discovery is interesting to you, here are some related topics I’d recommend exploring next:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A beginner‑friendly guide to choosing your &lt;strong&gt;&lt;a href="https://astroimagery.com/astrophotography/deep-space-astrophotography/brightest-deep-sky-objects/"&gt;first five deep sky targets&lt;/a&gt;&lt;/strong&gt; from light‑polluted skies.&lt;/li&gt;&lt;li&gt;A breakdown of &lt;a href="https://astroimagery.com/equipment/telescopes/light-pollution-filter-comparison-for-astrophotography/"&gt;&lt;strong&gt;light pollution vs. narrowband vs. broadband imaging&lt;/strong&gt; &lt;/a&gt;and when each makes sense.&lt;/li&gt;&lt;li&gt;A “real world” comparison of &lt;strong&gt;&lt;a href="https://astroimagery.com/astrophotography/deep-space-astrophotography/nebula-photography-guide/"&gt;different focal lengths for nebulae&lt;/a&gt;&lt;/strong&gt; – what 135 mm, and 650 mm actually look like on the same target.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;I’ll link to the relevant videos and articles once this post is live so you can dive straight in.&lt;/p&gt;&lt;h3&gt;A Small Way to Carry the Night Sky with You&lt;/h3&gt;&lt;p&gt;Most of the time, our astrophotography lives on hard drives, in processing projects, or as big prints that only a few people ever see. That’s great, but I also like having a quieter, everyday reminder of the night sky.&lt;/p&gt;&lt;a href="https://astroimagery.com/astroimagery-shop/"&gt;&lt;img src="https://astroimagery.com/wp-content/uploads/2025/12/Screenshot-2025-12-16-215849-1-152x300.png"/&gt;&lt;/a&gt;&lt;p&gt;That’s why I started turning some of my favourite Astroimagery captures into &lt;strong&gt;phone cases&lt;/strong&gt;. They’re not shouty or over‑branded – just subtle, high‑resolution slices of nebulae and galaxies you can literally carry around in your hand.&lt;/p&gt;&lt;p&gt;If you’d like a small, practical way to keep a bit of the night sky with you during the day, have a look here:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Browse the Astroimagery phone cases here: &lt;/strong&gt;&lt;a href="https://astroimagery.com/astroimagery-shop/"&gt;Astroimagery Shop&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
  &lt;img src="https://secure.gravatar.com/avatar/b335d8c790c525812a40fe540abf1a93e70e3fdc9cffecabe0b1817804633194?s=48&amp;amp;d=mm&amp;amp;r=g"/&gt;&lt;p&gt;Karl Perera MA&lt;/p&gt;I’m Karl Perera, an experienced astrophotographer, author, and blogger with a master’s degree in teaching. I’m a member of the British Astronomy Association. Welcome!
  &lt;h3&gt;&lt;strong&gt;Follow&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.instagram.com/karl_astroimagery/"&gt;karl_astroimagery&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.youtube.com/@Astroimagery"&gt;@Astroimagery&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.linkedin.com/company/astroimagery/"&gt;Astroimagery&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;strong&gt;Recent Posts&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://astroimagery.com/techniques/imaging/astrophotography-target-planner/"&gt;Astrophotography Target Planner: Discover Hidden Nebulas with My New App&lt;/a&gt;December 17, 2025&lt;/li&gt;&lt;li&gt;&lt;a href="https://astroimagery.com/techniques/post-processing/pleiades-two-filters/"&gt;&lt;img alt="Pleiades two filters workflow" src="https://astroimagery.com/wp-content/uploads/2025/12/Untitled-3-22-150x150.png"/&gt;&lt;/a&gt;&lt;a href="https://astroimagery.com/techniques/post-processing/pleiades-two-filters/"&gt;Better Images with Two Filters for the Pleiades (At 135mm)&lt;/a&gt;December 16, 2025&lt;/li&gt;&lt;li&gt;&lt;a href="https://astroimagery.com/techniques/astrophotography-skills-and-techniques/"&gt;&lt;img alt="astrophotography skills and techniques - shooting the Milky Way" src="https://astroimagery.com/wp-content/uploads/2025/11/astrophotography-skills-and-techniques-150x150.jpg"/&gt;&lt;/a&gt;&lt;a href="https://astroimagery.com/techniques/astrophotography-skills-and-techniques/"&gt;Master Astrophotography Skills and Techniques&lt;/a&gt;November 6, 2025&lt;/li&gt;&lt;li&gt;&lt;a href="https://astroimagery.com/astrophotography/deep-space-astrophotography/nebula-photography-guide/"&gt;Nebula Photography the Best Gear, Filters, and Processing&lt;/a&gt;November 1, 2025&lt;/li&gt;&lt;li&gt;&lt;a href="https://astroimagery.com/uncategorized/astrophotography-update/"&gt;Astrophotography Update: New Deep‑Sky Images, Special Offers, and Print Releases | AstroImagery&lt;/a&gt;October 21, 2025&lt;/li&gt;&lt;/ul&gt;&lt;/main&gt;</content:encoded>
      <guid isPermaLink="false">https://astroimagery.com/techniques/imaging/astrophotography-target-planner/</guid>
      <category>Hacker News</category>
      <pubDate>Fri, 19 Dec 2025 19:44:41 +0000</pubDate>
    </item>
    <item>
      <title>What makes you senior</title>
      <link>https://terriblesoftware.org/2025/11/25/what-actually-makes-you-senior/</link>
      <description>People love to describe senior engineers with a big checklist: architecture, communication, ownership, leadership, etc.</description>
      <content:encoded>&lt;main class="wp-block-group is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;img src="https://terriblesoftware.org/wp-content/uploads/2025/11/cznmcy1wcml2yxrll3jhd3bpegvsx2ltywdlcy93zwjzaxrlx2nvbnrlbnqvbhivzmw0ndc0nji0mtg2mi1pbwfnzs1rewjlahu4ni5qcgc.webp?w=1024"/&gt;
&lt;h2&gt;What Actually Makes You Senior&lt;/h2&gt;

&lt;p&gt;People love to describe senior engineers with a big checklist: architecture, communication, ownership, leadership, etc.&lt;/p&gt;
&lt;p&gt;But if you strip away the title, the salary, and the years of experience, there’s one core skill that separates senior+ engineers from everyone else: &lt;strong&gt;reducing ambiguity&lt;/strong&gt;. Everything else flows from that.&lt;/p&gt;
&lt;p&gt;Here’s what I mean. A mid-level engineer can absolutely crush a well-defined problem. Give them a clear spec, some reasonable constraints, and they’ll deliver solid work. Don’t get me wrong, that &lt;em&gt;is&lt;/em&gt; valuable.&lt;/p&gt;
&lt;p&gt;The moment you hand them something fuzzy, though, like &lt;em&gt;“we need to improve performance”&lt;/em&gt;, &lt;em&gt;“users are complaining about the onboarding flow”&lt;/em&gt; or &lt;em&gt;“we should probably think about scaling”&lt;/em&gt;, that’s when you see the difference. Not because mid-level engineers are bad at their jobs, but because ambiguous problems require something more.&lt;/p&gt;
&lt;p&gt;Senior engineers look at the big, messy, abstract thing and start digging:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;They ask questions nobody else thought to ask.&lt;/li&gt;
&lt;li&gt;They separate what matters from noise.&lt;/li&gt;
&lt;li&gt;They identify what should be done now vs. what to punt.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It’s one of the reasons why senior engineers are worth their salaries. Not just because they write good code (which they often do!), but because &lt;strong&gt;they derisk projects&lt;/strong&gt;. They turn &lt;em&gt;“I don’t even know what this is”&lt;/em&gt; into &lt;em&gt;“there are two small projects and one thing we should cut.”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;And you know what’s funny? When senior engineers do this well, it looks easy. Like nothing was even done. The project just… goes smoothly. Fewer surprises, production fires, or emergency meetings. But what actually happened was that someone did a lot of invisible work upfront.&lt;/p&gt;
&lt;p&gt;Just a few questions, as an example, that senior+ engineers ask:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;What problem are we actually trying to solve?&lt;/strong&gt; (Not what solution do we want, but what’s the underlying problem?)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Who’s the user here and what’s painful for them?&lt;/strong&gt; (They try to be specific. “Users” isn’t an answer.)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;What are we assuming that might be wrong?&lt;/strong&gt; (Every plan has hidden assumptions.)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;What happens if we’re wrong and ship this anyway?&lt;/strong&gt; (How bad is the downside?)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In other words, they first make the problem clear. Then, and only then, they go to solve it.&lt;/p&gt;
&lt;p&gt;One frustrating part is that many companies are still terrible at hiring for this. Job descriptions list technologies and years of experience. Interviews focus on LeetCode. None of that really measures your ability to take a vague product requirement and turn it into a shippable plan.&lt;/p&gt;
&lt;p&gt;So we end up with “senior” engineers who can reverse a binary tree on the whiteboard but freeze when the spec is half-baked.&lt;/p&gt;
&lt;p&gt;I’m not saying the other stuff doesn’t matter. Architecture matters. Communication matters. &lt;strong&gt;But those things are way more valuable once you’ve figured out what you’re actually building&lt;/strong&gt;. If you can’t reduce ambiguity, all your other skills are just elegant ways of solving the wrong problem.&lt;/p&gt;
&lt;p&gt;So if you’re wondering whether you’re operating at a senior+ level, here’s one test: What happens when someone hands you something abstract/fuzzy/complex? Do you wait for someone else to clarify it for you? Do you start coding immediately and hope for the best? Or do you spend time up front making it concrete enough that you and your team can actually execute with confidence?&lt;/p&gt;
&lt;p&gt;If it’s the last one, you’re probably already there. If it’s not, the good news is this isn’t talent, but practice: start with the next vague ticket that’s assigned to you.&lt;/p&gt;
&lt;h3&gt;Share this:&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://terriblesoftware.org/2025/11/25/what-actually-makes-you-senior/?share=facebook"&gt;
Click to share on Facebook (Opens in new window)
Facebook
&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://terriblesoftware.org/2025/11/25/what-actually-makes-you-senior/?share=linkedin"&gt;
Click to share on LinkedIn (Opens in new window)
LinkedIn
&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://terriblesoftware.org/2025/11/25/what-actually-makes-you-senior/?share=threads"&gt;
Click to share on Threads (Opens in new window)
Threads
&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://terriblesoftware.org/2025/11/25/what-actually-makes-you-senior/?share=bluesky"&gt;
Click to share on Bluesky (Opens in new window)
Bluesky
&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://terriblesoftware.org/2025/11/25/what-actually-makes-you-senior/?share=mastodon"&gt;
Click to share on Mastodon (Opens in new window)
Mastodon
&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://terriblesoftware.org/2025/11/25/what-actually-makes-you-senior/?share=x"&gt;
Click to share on X (Opens in new window)
X
&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;/li&gt;&lt;/ul&gt;Like Loading…&lt;a&gt;&lt;/a&gt;


&lt;a href="https://terriblesoftware.org/category/opinion/"&gt;opinion&lt;/a&gt;
&lt;a href="https://terriblesoftware.org/tag/career/"&gt;career&lt;/a&gt;, &lt;a href="https://terriblesoftware.org/tag/engineering/"&gt;engineering&lt;/a&gt;, &lt;a href="https://terriblesoftware.org/tag/growth/"&gt;growth&lt;/a&gt;, &lt;a href="https://terriblesoftware.org/tag/programming/"&gt;programming&lt;/a&gt;



&lt;h3&gt;Discover more from Terrible Software&lt;/h3&gt;
&lt;p&gt;Subscribe to get the latest posts sent to your email.&lt;/p&gt;
 










&lt;/main&gt;</content:encoded>
      <guid isPermaLink="false">https://terriblesoftware.org/2025/11/25/what-actually-makes-you-senior/</guid>
      <category>Hacker News</category>
      <pubDate>Fri, 19 Dec 2025 16:34:41 +0000</pubDate>
    </item>
    <item>
      <title>Adobe Photoshop 1.0 Source Code (1990)</title>
      <link>https://computerhistory.org/blog/adobe-photoshop-source-code/</link>
      <description>Adobe Photoshop Source Code - CHM</description>
      <content:encoded>&lt;div class="right-content"&gt;

&lt;a href="https://donate.computerhistory.org"&gt;Donate&lt;/a&gt;
&lt;a href="https://connect.computerhistory.org"&gt;Tickets&lt;/a&gt;


Menu

&lt;/div&gt;</content:encoded>
      <guid isPermaLink="false">https://computerhistory.org/blog/adobe-photoshop-source-code/</guid>
      <category>Hacker News</category>
      <pubDate>Thu, 18 Dec 2025 15:37:18 +0000</pubDate>
    </item>
    <item>
      <title>Executorch: On-device AI across mobile, embedded and edge for PyTorch</title>
      <link>https://github.com/pytorch/executorch</link>
      <description>On-device AI inference powered by PyTorch</description>
      <content:encoded>&lt;article class="markdown-body entry-content container-lg" itemprop="text"&gt;
&lt;a href="https://github.com/pytorch/executorch/blob/main/docs/source/_static/img/et-logo.png"&gt;&lt;img alt="ExecuTorch logo mark" src="https://github.com/pytorch/executorch/raw/main/docs/source/_static/img/et-logo.png"/&gt;&lt;/a&gt;
&lt;h1&gt;ExecuTorch&lt;/h1&gt;&lt;a href="#executorch"&gt;&lt;/a&gt;
&lt;p&gt;&lt;strong&gt;On-device AI inference powered by PyTorch&lt;/strong&gt;&lt;/p&gt;


&lt;a href="https://pypi.org/project/executorch/"&gt;&lt;img alt="PyPI - Version" src="https://camo.githubusercontent.com/0a655d9cfc2a1d6c3f54fba80bc87ebea4321ade5755afd0e8dada56fe35f159/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6578656375746f7263683f7374796c653d666f722d7468652d626164676526636f6c6f723d626c7565"/&gt;&lt;/a&gt;
&lt;a href="https://github.com/pytorch/executorch/graphs/contributors"&gt;&lt;img alt="GitHub - Contributors" src="https://camo.githubusercontent.com/629c378cadd0fd62df5cf89107f8bc8b08b3f8d4fc5b939bde95d7acb9049cdb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f7079746f7263682f6578656375746f7263683f7374796c653d666f722d7468652d626164676526636f6c6f723d626c7565"/&gt;&lt;/a&gt;
&lt;a href="https://github.com/pytorch/executorch/stargazers"&gt;&lt;img alt="GitHub - Stars" src="https://camo.githubusercontent.com/461ec8564ce148802f8e6dc5bde3bc70997ad25ecc838c4c7b9813efd9533db4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7079746f7263682f6578656375746f7263683f7374796c653d666f722d7468652d626164676526636f6c6f723d626c7565"/&gt;&lt;/a&gt;
&lt;a href="https://discord.gg/Dh43CKSAdc"&gt;&lt;img alt="Discord - Chat with Us" src="https://camo.githubusercontent.com/5665a739b7459f532d6d1bdb198268464b4e52bbfa6f28b2f36bcd159467db62/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d4a6f696e25323055732d626c75653f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465267374796c653d666f722d7468652d6261646765"/&gt;&lt;/a&gt;
&lt;a href="https://docs.pytorch.org/executorch/main/index.html"&gt;&lt;img alt="Documentation" src="https://camo.githubusercontent.com/4ea00c7ce642fa2d5117120b3237ef4e7f310fdb8f96c6a7ed607d215348dcf9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f63756d656e746174696f6e2d626c75653f6c6f676f3d676f6f676c65646f6373266c6f676f436f6c6f723d7768697465267374796c653d666f722d7468652d6261646765"/&gt;&lt;/a&gt;

&lt;p&gt;&lt;strong&gt;ExecuTorch&lt;/strong&gt; is PyTorch's unified solution for deploying AI models on-device—from smartphones to microcontrollers—built for privacy, performance, and portability. It powers Meta's on-device AI across &lt;strong&gt;Instagram, WhatsApp, Quest 3, Ray-Ban Meta Smart Glasses&lt;/strong&gt;, and &lt;a href="https://docs.pytorch.org/executorch/main/success-stories.html"&gt;more&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Deploy &lt;strong&gt;LLMs, vision, speech, and multimodal models&lt;/strong&gt; with the same PyTorch APIs you already know—accelerating research to production with seamless model export, optimization, and deployment. No manual C++ rewrites. No format conversions. No vendor lock-in.&lt;/p&gt;

&lt;strong&gt;📘 Table of Contents&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#why-executorch"&gt;Why ExecuTorch?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-it-works"&gt;How It Works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#quick-start"&gt;Quick Start&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#export-and-deploy-in-3-steps"&gt;Export and Deploy in 3 Steps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#run-on-device"&gt;Run on Device&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#llm-example-llama"&gt;LLM Example: Llama&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#platform--hardware-support"&gt;Platform &amp;amp; Hardware Support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#production-deployments"&gt;Production Deployments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples--models"&gt;Examples &amp;amp; Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#community--contributing"&gt;Community &amp;amp; Contributing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Why ExecuTorch?&lt;/h2&gt;&lt;a href="#why-executorch"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;🔒 Native PyTorch Export&lt;/strong&gt; — Direct export from PyTorch. No .onnx, .tflite, or intermediate format conversions. Preserve model semantics.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;⚡ Production-Proven&lt;/strong&gt; — Powers billions of users at &lt;a href="https://engineering.fb.com/2025/07/28/android/executorch-on-device-ml-meta-family-of-apps/"&gt;Meta with real-time on-device inference&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;💾 Tiny Runtime&lt;/strong&gt; — 50KB base footprint. Runs on microcontrollers to high-end smartphones.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;🚀 &lt;a href="https://docs.pytorch.org/executorch/main/backends-overview.html"&gt;12+ Hardware Backends&lt;/a&gt;&lt;/strong&gt; — Open-source acceleration for Apple, Qualcomm, ARM, MediaTek, Vulkan, and more.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;🎯 One Export, Multiple Backends&lt;/strong&gt; — Switch hardware targets with a single line change. Deploy the same model everywhere.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;How It Works&lt;/h2&gt;&lt;a href="#how-it-works"&gt;&lt;/a&gt;
&lt;p&gt;ExecuTorch uses &lt;strong&gt;ahead-of-time (AOT) compilation&lt;/strong&gt; to prepare PyTorch models for edge deployment:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;🧩 Export&lt;/strong&gt; — Capture your PyTorch model graph with &lt;code&gt;torch.export()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;⚙️ Compile&lt;/strong&gt; — Quantize, optimize, and partition to hardware backends → &lt;code&gt;.pte&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;🚀 Execute&lt;/strong&gt; — Load &lt;code&gt;.pte&lt;/code&gt; on-device via lightweight C++ runtime&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Models use a standardized &lt;a href="https://docs.pytorch.org/executorch/main/compiler-ir-advanced.html#intermediate-representation"&gt;Core ATen operator set&lt;/a&gt;. &lt;a href="https://docs.pytorch.org/executorch/main/compiler-delegate-and-partitioner.html"&gt;Partitioners&lt;/a&gt; delegate subgraphs to specialized hardware (NPU/GPU) with CPU fallback.&lt;/p&gt;
&lt;p&gt;Learn more: &lt;a href="https://docs.pytorch.org/executorch/main/intro-how-it-works.html"&gt;How ExecuTorch Works&lt;/a&gt; • &lt;a href="https://docs.pytorch.org/executorch/main/getting-started-architecture.html"&gt;Architecture Guide&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Quick Start&lt;/h2&gt;&lt;a href="#quick-start"&gt;&lt;/a&gt;
&lt;h3&gt;Installation&lt;/h3&gt;&lt;a href="#installation"&gt;&lt;/a&gt;
&lt;pre&gt;pip install executorch&lt;/pre&gt;
&lt;p&gt;For platform-specific setup (Android, iOS, embedded systems), see the &lt;a href="https://docs.pytorch.org/executorch/main/quick-start-section.html"&gt;Quick Start&lt;/a&gt; documentation for additional info.&lt;/p&gt;
&lt;h3&gt;Export and Deploy in 3 Steps&lt;/h3&gt;&lt;a href="#export-and-deploy-in-3-steps"&gt;&lt;/a&gt;
&lt;pre&gt;import torch
from executorch.exir import to_edge_transform_and_lower
from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner

# 1. Export your PyTorch model
model = MyModel().eval()
example_inputs = (torch.randn(1, 3, 224, 224),)
exported_program = torch.export.export(model, example_inputs)

# 2. Optimize for target hardware (switch backends with one line)
program = to_edge_transform_and_lower(
    exported_program,
    partitioner=[XnnpackPartitioner()]  # CPU | CoreMLPartitioner() for iOS | QnnPartitioner() for Qualcomm
).to_executorch()

# 3. Save for deployment
with open("model.pte", "wb") as f:
    f.write(program.buffer)

# Test locally via ExecuTorch runtime's pybind API (optional)
from executorch.runtime import Runtime
runtime = Runtime.get()
method = runtime.load_program("model.pte").load_method("forward")
outputs = method.execute([torch.randn(1, 3, 224, 224)])&lt;/pre&gt;
&lt;h3&gt;Run on Device&lt;/h3&gt;&lt;a href="#run-on-device"&gt;&lt;/a&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pytorch.org/executorch/main/using-executorch-cpp.html"&gt;C++&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;#include &amp;lt;executorch/extension/module/module.h&amp;gt;
#include &amp;lt;executorch/extension/tensor/tensor.h&amp;gt;

Module module("model.pte");
auto tensor = make_tensor_ptr({2, 2}, {1.0f, 2.0f, 3.0f, 4.0f});
auto outputs = module.forward(tensor);&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pytorch.org/executorch/main/ios-section.html"&gt;Swift (iOS)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;import ExecuTorch

let module = Module(filePath: "model.pte")
let input = Tensor&amp;lt;Float&amp;gt;([1.0, 2.0, 3.0, 4.0], shape: [2, 2])
let outputs = try module.forward(input)&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pytorch.org/executorch/main/android-section.html"&gt;Kotlin (Android)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;val module = Module.load("model.pte")
val inputTensor = Tensor.fromBlob(floatArrayOf(1.0f, 2.0f, 3.0f, 4.0f), longArrayOf(2, 2))
val outputs = module.forward(EValue.from(inputTensor))&lt;/pre&gt;
&lt;h3&gt;LLM Example: Llama&lt;/h3&gt;&lt;a href="#llm-example-llama"&gt;&lt;/a&gt;
&lt;p&gt;Export Llama models using the &lt;a href="https://docs.pytorch.org/executorch/main/llm/export-llm.html"&gt;&lt;code&gt;export_llm&lt;/code&gt;&lt;/a&gt; script or &lt;a href="https://github.com/huggingface/optimum-executorch"&gt;Optimum-ExecuTorch&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;# Using export_llm
python -m executorch.extension.llm.export.export_llm --model llama3_2 --output llama.pte

# Using Optimum-ExecuTorch
optimum-cli export executorch \
  --model meta-llama/Llama-3.2-1B \
  --task text-generation \
  --recipe xnnpack \
  --output_dir llama_model&lt;/pre&gt;
&lt;p&gt;Run on-device with the LLM runner API:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pytorch.org/executorch/main/llm/run-with-c-plus-plus.html"&gt;C++&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;#include &amp;lt;executorch/extension/llm/runner/text_llm_runner.h&amp;gt;

auto runner = create_llama_runner("llama.pte", "tiktoken.bin");
executorch::extension::llm::GenerationConfig config{
    .seq_len = 128, .temperature = 0.8f};
runner-&amp;gt;generate("Hello, how are you?", config);&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.pytorch.org/executorch/main/llm/run-on-ios.html"&gt;Swift (iOS)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;import ExecuTorchLLM

let runner = TextRunner(modelPath: "llama.pte", tokenizerPath: "tiktoken.bin")
try runner.generate("Hello, how are you?", Config {
    $0.sequenceLength = 128
}) { token in
    print(token, terminator: "")
}&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Kotlin (Android)&lt;/strong&gt; — &lt;a href="https://docs.pytorch.org/executorch/main/javadoc/org/pytorch/executorch/extension/llm/package-summary.html"&gt;API Docs&lt;/a&gt; • &lt;a href="https://github.com/meta-pytorch/executorch-examples/tree/main/llm/android/LlamaDemo"&gt;Demo App&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;val llmModule = LlmModule("llama.pte", "tiktoken.bin", 0.8f)
llmModule.load()
llmModule.generate("Hello, how are you?", 128, object : LlmCallback {
    override fun onResult(result: String) { print(result) }
    override fun onStats(stats: String) { }
})&lt;/pre&gt;
&lt;p&gt;For multimodal models (vision, audio), use the &lt;a href="https://github.com/pytorch/executorch/blob/main/extension/llm/runner"&gt;MultiModal runner API&lt;/a&gt; which extends the LLM runner to handle image and audio inputs alongside text. See &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/llava/README.md"&gt;Llava&lt;/a&gt; and &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/voxtral/README.md"&gt;Voxtral&lt;/a&gt; examples.&lt;/p&gt;
&lt;p&gt;See &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/llama/README.md"&gt;examples/models/llama&lt;/a&gt; for complete workflow including quantization, mobile deployment, and advanced options.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next Steps:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;📖 &lt;a href="https://docs.pytorch.org/executorch/main/getting-started.html"&gt;Step-by-step tutorial&lt;/a&gt; — Complete walkthrough for your first model&lt;/li&gt;
&lt;li&gt;⚡ &lt;a href="https://colab.research.google.com/drive/1qpxrXC3YdJQzly3mRg-4ayYiOjC6rue3?usp=sharing"&gt;Colab notebook&lt;/a&gt; — Try ExecuTorch instantly in your browser&lt;/li&gt;
&lt;li&gt;🤖 &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/llama/README.md"&gt;Deploy Llama models&lt;/a&gt; — LLM workflow with quantization and mobile demos&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Platform &amp;amp; Hardware Support&lt;/h2&gt;&lt;a href="#platform--hardware-support"&gt;&lt;/a&gt;



&lt;strong&gt;Platform&lt;/strong&gt;
&lt;strong&gt;Supported Backends&lt;/strong&gt;




Android
XNNPACK, Vulkan, Qualcomm, MediaTek, Samsung Exynos


iOS
XNNPACK, MPS, CoreML (Neural Engine)


Linux / Windows
XNNPACK, OpenVINO, CUDA &lt;em&gt;(experimental)&lt;/em&gt;


macOS
XNNPACK, MPS, Metal &lt;em&gt;(experimental)&lt;/em&gt;


Embedded / MCU
XNNPACK, ARM Ethos-U, NXP, Cadence DSP



&lt;p&gt;See &lt;a href="https://docs.pytorch.org/executorch/main/backends-overview.html"&gt;Backend Documentation&lt;/a&gt; for detailed hardware requirements and optimization guides. For desktop/laptop GPU inference with CUDA and Metal, see the &lt;a href="https://github.com/pytorch/executorch/blob/main/desktop/README.md"&gt;Desktop Guide&lt;/a&gt;. For Zephyr RTOS integration, see the &lt;a href="https://github.com/pytorch/executorch/blob/main/zephyr/README.md"&gt;Zephyr Guide&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Production Deployments&lt;/h2&gt;&lt;a href="#production-deployments"&gt;&lt;/a&gt;
&lt;p&gt;ExecuTorch powers on-device AI at scale across Meta's family of apps, VR/AR devices, and partner deployments. &lt;a href="https://docs.pytorch.org/executorch/main/success-stories.html"&gt;View success stories →&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Examples &amp;amp; Models&lt;/h2&gt;&lt;a href="#examples--models"&gt;&lt;/a&gt;
&lt;p&gt;&lt;strong&gt;LLMs:&lt;/strong&gt; &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/llama/README.md"&gt;Llama 3.2/3.1/3&lt;/a&gt;, &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/qwen3/README.md"&gt;Qwen 3&lt;/a&gt;, &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/phi_4_mini/README.md"&gt;Phi-4-mini&lt;/a&gt;, &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/lfm2/README.md"&gt;LiquidAI LFM2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multimodal:&lt;/strong&gt; &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/llava/README.md"&gt;Llava&lt;/a&gt; (vision-language), &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/voxtral/README.md"&gt;Voxtral&lt;/a&gt; (audio-language), &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/gemma3"&gt;Gemma&lt;/a&gt; (vision-language)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vision/Speech:&lt;/strong&gt; &lt;a href="https://github.com/meta-pytorch/executorch-examples/tree/main/mv2"&gt;MobileNetV2&lt;/a&gt;, &lt;a href="https://github.com/meta-pytorch/executorch-examples/tree/main/dl3"&gt;DeepLabV3&lt;/a&gt;, &lt;a href="https://github.com/pytorch/executorch/blob/main/examples/models/whisper/README.md"&gt;Whisper&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt; &lt;a href="https://github.com/pytorch/executorch/blob/main/examples"&gt;&lt;code&gt;examples/&lt;/code&gt;&lt;/a&gt; directory • &lt;a href="https://github.com/meta-pytorch/executorch-examples"&gt;executorch-examples&lt;/a&gt; out-of-tree demos • &lt;a href="https://github.com/huggingface/optimum-executorch"&gt;Optimum-ExecuTorch&lt;/a&gt; for HuggingFace models • &lt;a href="https://docs.unsloth.ai/new/deploy-llms-phone"&gt;Unsloth&lt;/a&gt; for fine-tuned LLM deployment &lt;/p&gt;
&lt;h2&gt;Key Features&lt;/h2&gt;&lt;a href="#key-features"&gt;&lt;/a&gt;
&lt;p&gt;ExecuTorch provides advanced capabilities for production deployment:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Quantization&lt;/strong&gt; — Built-in support via &lt;a href="https://docs.pytorch.org/ao"&gt;torchao&lt;/a&gt; for 8-bit, 4-bit, and dynamic quantization&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory Planning&lt;/strong&gt; — Optimize memory usage with ahead-of-time allocation strategies&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Developer Tools&lt;/strong&gt; — ETDump profiler, ETRecord inspector, and model debugger&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Selective Build&lt;/strong&gt; — Strip unused operators to minimize binary size&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Custom Operators&lt;/strong&gt; — Extend with domain-specific kernels&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Shapes&lt;/strong&gt; — Support variable input sizes with bounded ranges&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See &lt;a href="https://docs.pytorch.org/executorch/main/advanced-topics-section.html"&gt;Advanced Topics&lt;/a&gt; for quantization techniques, custom backends, and compiler passes.&lt;/p&gt;
&lt;h2&gt;Documentation&lt;/h2&gt;&lt;a href="#documentation"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.pytorch.org/executorch/main/index.html"&gt;&lt;strong&gt;Documentation Home&lt;/strong&gt;&lt;/a&gt; — Complete guides and tutorials&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.pytorch.org/executorch/main/api-section.html"&gt;&lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt; — Python, C++, Java/Kotlin APIs&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.pytorch.org/executorch/main/backend-delegates-integration.html"&gt;&lt;strong&gt;Backend Integration&lt;/strong&gt;&lt;/a&gt; — Build custom hardware backends&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.pytorch.org/executorch/main/support-section.html"&gt;&lt;strong&gt;Troubleshooting&lt;/strong&gt;&lt;/a&gt; — Common issues and solutions&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Community &amp;amp; Contributing&lt;/h2&gt;&lt;a href="#community--contributing"&gt;&lt;/a&gt;
&lt;p&gt;We welcome contributions from the community!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;💬 &lt;a href="https://github.com/pytorch/executorch/discussions"&gt;&lt;strong&gt;GitHub Discussions&lt;/strong&gt;&lt;/a&gt; — Ask questions and share ideas&lt;/li&gt;
&lt;li&gt;🎮 &lt;a href="https://discord.gg/Dh43CKSAdc"&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt; — Chat with the team and community&lt;/li&gt;
&lt;li&gt;🐛 &lt;a href="https://github.com/pytorch/executorch/issues"&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/a&gt; — Report bugs or request features&lt;/li&gt;
&lt;li&gt;🤝 &lt;a href="https://github.com/pytorch/executorch/blob/main/CONTRIBUTING.md"&gt;&lt;strong&gt;Contributing Guide&lt;/strong&gt;&lt;/a&gt; — Guidelines and codebase structure&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;License&lt;/h2&gt;&lt;a href="#license"&gt;&lt;/a&gt;
&lt;p&gt;ExecuTorch is BSD licensed, as found in the &lt;a href="https://github.com/pytorch/executorch/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Part of the PyTorch ecosystem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href="https://github.com/pytorch/executorch"&gt;GitHub&lt;/a&gt; •
    &lt;a href="https://docs.pytorch.org/executorch"&gt;Documentation&lt;/a&gt;
&lt;/p&gt;

&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://github.com/pytorch/executorch</guid>
      <category>Hacker News</category>
      <pubDate>Thu, 18 Dec 2025 13:51:11 +0000</pubDate>
    </item>
    <item>
      <title>Partial inlining</title>
      <link>https://xania.org/202512/18-partial-inlining</link>
      <description>Written by me, proof-read by an LLM. Details at end.</description>
      <content:encoded>&lt;div class="small-12 columns article"&gt;
&lt;h2&gt;Partial inlining&lt;/h2&gt;
&lt;p&gt;Written by me, proof-read by an LLM.
&lt;br/&gt;Details at end.&lt;/p&gt;
&lt;p&gt;We’ve learned how important inlining is to optimisation, but also that it might sometimes cause code bloat. Inlining doesn’t have to be all-or-nothing!&lt;/p&gt;
&lt;p&gt;Let’s look at a simple function that has a fast path and slow path; and then see how the compiler handles it&lt;a href="#fn:note"&gt;1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this example we have some &lt;code&gt;process&lt;/code&gt; function that has a really trivial fast case for numbers in the range 0-100. For other numbers it does something more expensive. Then &lt;code&gt;compute&lt;/code&gt; calls &lt;code&gt;process&lt;/code&gt; twice (making it less appealing to inline all of &lt;code&gt;process&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Looking at the assembly output, we see what’s happened: The compiler has split &lt;code&gt;process&lt;/code&gt; into two functions, a &lt;code&gt;process (part.0)&lt;/code&gt; that does the expensive part only. It then rewrites &lt;code&gt;process&lt;/code&gt; into the quick check for 100, returning double the value if less than 100. If not, it jumps to the &lt;code&gt;(part.0)&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;process(unsigned int):
  cmp edi, 99                           ; less than or equal to 99?
  jbe .L7                               ; skip to fast path if so
  jmp process(unsigned int) (.part.0)   ; else jump to the expensive path
.L7:
  lea eax, [rdi+rdi]                    ; return `value * 2`
  ret
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This first step - extracting the cold path into a separate function - is called &lt;strong&gt;function outlining&lt;/strong&gt;. The original &lt;code&gt;process&lt;/code&gt; becomes a thin wrapper handling the hot path, delegating to the outlined &lt;code&gt;process (.part.0)&lt;/code&gt; when needed. This split sets up the real trick: &lt;strong&gt;partial inlining&lt;/strong&gt;. When the compiler later inlines &lt;code&gt;process&lt;/code&gt; into &lt;code&gt;compute&lt;/code&gt;, it inlines just the wrapper whilst keeping calls to the outlined cold path. External callers can still call &lt;code&gt;process&lt;/code&gt; and have it work correctly for all values.&lt;/p&gt;
&lt;p&gt;Let’s see this optimisation in action in the &lt;code&gt;compute&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;compute(unsigned int, unsigned int):
  cmp edi, 99                   ; is a &amp;lt;= 99?
  jbe .L13                      ; if so, go to the inlined fast path for a
  call process(unsigned int) (.part.0) ; else, call expensive case
  mov r8d, eax                  ; save the result of process(a)
  cmp esi, 99                   ; is b &amp;lt;= 99?
  jbe .L14                      ; if so go to the inlined fast path for b
.L11:
  mov edi, esi                  ; otherwise, call expensive case for b
  call process(unsigned int) (.part.0)
  add eax, r8d                  ; add the two slow cases together
  ret                           ; return

.L13:                           ; case where a is fast case
  lea r8d, [rdi+rdi]            ; process(a) is just a + a
  cmp esi, 99                   ; is b &amp;gt; 99?
  ja .L11                       ; jump to b slow case if so
                                ; (falls through to...)
.L14:                           ; b fast case
  lea eax, [rsi+rsi]            ; double b
  add eax, r8d                  ; return 2*a + 2*b
  ret
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at &lt;code&gt;compute&lt;/code&gt;, we can see the benefits of this approach clearly: The simple range check and arithmetic (&lt;code&gt;cmp&lt;/code&gt;, &lt;code&gt;lea&lt;/code&gt;) are inlined directly, avoiding the function call overhead for the fast path. When a value is 100 or greater, it calls the outlined &lt;code&gt;process (.part.0)&lt;/code&gt; function for the more expensive computation.&lt;/p&gt;
&lt;p&gt;This is the best of both worlds: we get the performance benefit of inlining the lightweight check and simple arithmetic, whilst avoiding code bloat from duplicating the expensive computation&lt;a href="#fn:again"&gt;2&lt;/a&gt;. The original &lt;code&gt;process&lt;/code&gt; function remains intact and callable, so external callers still work correctly.&lt;/p&gt;
&lt;p&gt;Partial inlining lets the compiler make nuanced trade-offs about what to inline and what to keep shared. The compiler can outline portions of a function based on its heuristics about code size and performance&lt;a href="#fn:tradeoff"&gt;3&lt;/a&gt;, giving you benefits of inlining without necessarily paying the full code size cost. In this example, the simple check is duplicated whilst the complex computation stays shared.&lt;/p&gt;
&lt;p&gt;As with many optimisations, the compiler’s heuristics&lt;a href="#fn:clang"&gt;4&lt;/a&gt; usually make reasonable choices about when to apply partial inlining, but it’s worth checking your hot code paths to see if the compiler has made the decisions you expect. Taking a quick peek in &lt;a href="https://godbolt.org"&gt;Compiler Explorer&lt;/a&gt; is a good way to develop your intuition.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;See &lt;a href="https://youtu.be/STZb5K5sPDs"&gt;the video&lt;/a&gt; that accompanies this post.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This post is day 18 of &lt;a href="https://xania.org/AoCO2025"&gt;Advent of Compiler Optimisations 2025&lt;/a&gt;,
a 25-day series exploring how compilers transform our code.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This post was written by a human (&lt;a href="https://xania.org/MattGodbolt"&gt;Matt Godbolt&lt;/a&gt;) and reviewed and proof-read by LLMs and humans.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Support Compiler Explorer on &lt;a href="https://patreon.com/c/mattgodbolt"&gt;Patreon&lt;/a&gt;
or &lt;a href="https://github.com/sponsors/compiler-explorer"&gt;GitHub&lt;/a&gt;,
or by buying CE products in the &lt;a href="https://shop.compiler-explorer.com"&gt;Compiler Explorer Shop&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;


&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;I have had to cheat a little here to get the output I want: I’ve actually disabled GCC’s main inlining pass, otherwise it chooses to inline the whole of &lt;code&gt;process&lt;/code&gt;. With a larger, more complex “slow path” that would be unnecessary, but in order to demonstrate the effect of partial inlining without generating tons of code, I’m using this slight cheat. &lt;a href="#fnref:note"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Again, in this contrived example it probably &lt;em&gt;would&lt;/em&gt; be OK to inline &lt;code&gt;process&lt;/code&gt;, and the compiler really wants to, but for didactic purposes I’ve prevented that here. You can hopefully get the gist of this. &lt;a href="#fnref:again"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Of course, nothing is free - duplicating code still takes up instruction cache space. The compiler’s heuristics have to weigh the benefits against the costs, and different compilers make different choices. &lt;a href="#fnref:tradeoff"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note that this varies substantially from compiler to compiler: I couldn’t trick clang into making similar partial inlining decisions to gcc using flags, so I couldn’t compare like with like. In my experience gcc and clang make quite different choices about inlining. &lt;a href="#fnref:clang"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;</content:encoded>
      <guid isPermaLink="false">https://xania.org/202512/18-partial-inlining</guid>
      <category>Hacker News</category>
      <pubDate>Thu, 18 Dec 2025 13:39:41 +0000</pubDate>
    </item>
    <item>
      <title>Fifty problems with standard web APIs in 2025</title>
      <link>https://zerotrickpony.com/articles/browser-bugs/</link>
      <description>Recently, I made a free game .
It's a little mystery investigation game which I was inspired to write after enjoying games like Case of the Golden Idol , Roottrees are Dead , and Return of the Obra Dinn .
I thought my UX ideas were pretty simple, so I decided to implement it in HTML5 .</description>
      <content:encoded>&lt;article class="page"&gt;
&lt;h1&gt;Fifty problems with standard web APIs in 2025&lt;/h1&gt;
&lt;a href="https://messydesk.social/@zerotrickpony"&gt;Zero Trick Pony&lt;/a&gt; - December 2025
&lt;p&gt;&lt;a&gt;&lt;/a&gt;
Recently, I made a &lt;a href="https://intelligencegame.tech"&gt;free game&lt;/a&gt;.
It's a little mystery investigation game which I was inspired to write after enjoying games like
&lt;a href="https://en.wikipedia.org/wiki/The_Case_of_the_Golden_Idol"&gt;Case of the Golden Idol&lt;/a&gt;,
&lt;a href="https://en.wikipedia.org/wiki/The_Roottrees_are_Dead"&gt;Roottrees are Dead&lt;/a&gt;, and
&lt;a href="https://en.wikipedia.org/wiki/Return_of_the_Obra_Dinn"&gt;Return of the Obra Dinn&lt;/a&gt;.
I thought my UX ideas were pretty simple, so I decided to implement it in &lt;strong&gt;HTML5&lt;/strong&gt;.&lt;/p&gt;

&lt;a href="https://zerotrickpony.com/articles/browser-bugs/game-screenshot.jpg"&gt;
&lt;img src="https://zerotrickpony.com/articles/browser-bugs/game-screenshot.jpg"/&gt;
&lt;/a&gt;

&lt;p&gt;I know, I know: games should be made with game engines. But listen: The game is mostly text. It has
a dirt simple single-page ("SPA") type layout, static images, a few fonts, a few sounds, and that's it.
No need for a backing server, no 3D graphics, no complex animations, and no particularly novel interactions.
Just a single page load, and then clicking and reading.&lt;/p&gt;
&lt;p&gt;I figured it should be easy to make this game (app, really) work on multiple browsers and devices,
so that players could access it without a native install. The web has supported these basic functions for over
a decade. Surely in the year 2025, I thought, &lt;strong&gt;HTML5&lt;/strong&gt; is a good choice for these simple needs.
It's the future now!&lt;/p&gt;
&lt;p&gt;What really happened was, I hit &lt;strong&gt;over 50 surprising problems&lt;/strong&gt; related to gaps in web standards, requiring
me to &lt;strong&gt;spend over half of the total development time on rework&lt;/strong&gt; for cross-browser and cross-device
support. In this article I'd like to list the problems I hit, as an illustration of what a novice web developer
faces on the modern web today.
I'll give some simple advice on reducing rework costs, and conclude with some points on what
web standards in 2025 do and don't do for us.&lt;/p&gt;
&lt;h2&gt;Surely this is my fault&lt;/h2&gt;

&lt;img src="https://zerotrickpony.com/articles/browser-bugs/tombstone.jpg"/&gt;

&lt;p&gt;I know, I know: new features are always being added to browsers. There's always going to be some "wet paint" in
web standards. If I make a web project that depends on the newest features, it's not going to work everywhere.
You might be thinking that in order to hit 50+ cross-browser problems, I must have used the hottest, newest,
most experimental web APIs, and that was all my fault.&lt;/p&gt;
&lt;p&gt;The common wisdom is that if only I restrain myself to the "safe harbor" of the oldest, most time-tested parts of
the standard web, then I can rest assured that my web page will work on any "modern" browser and device.&lt;/p&gt;
&lt;p&gt;But sadly, no.&lt;/p&gt;
&lt;p&gt;Unfortunately some of the most buggy, glitchy, and/or divergent behaviors I found were in 20+ year old standard web features.
It's not (just) the new stuff. I was surprised to find &lt;strong&gt;no clear safe harbor feature set&lt;/strong&gt; for
building polished, interactive web-based projects, even if they are pretty simple.
Even with other projects that were barely interactive, I've hit fairly important platform and browser variances,
and needed at least some rework for multiple browsers.&lt;/p&gt;
&lt;p&gt;Even so, I imagine that an experienced web developer could read the list of problems below and nod to themselves knowingly:
"sure, these are all common pitfalls. That's why we have frameworks that polyfill many of these problems, and
experienced designers and frontend engineers who know all the best practices."&lt;/p&gt;
&lt;p&gt;Perhaps that's true, but my point is: should it be? If a "standard platform" requires a giant device testing lab and a
priesthood of finger-wagging frontend druids with trauma response for engineering intuition, is this "standard"
really helping us a lot? Even if web standards don't let us live the dream of "write once, run everywhere", shouldn't
it at least be predictable where the problems are going to be?&lt;/p&gt;
&lt;h2&gt;The novice web developer experience&lt;/h2&gt;
&lt;p&gt;The way I ended up hitting all these nasty surprises was not as naive as it may sound. I am not a novice
web developer; I am, unhappily, one of the druids. I did not naively blunder into cross-browser pitfalls,
I'm perfectly well aware that huge browser differences exist (spoiler: mobile) and need design consideration,
and testing.&lt;/p&gt;
&lt;p&gt;But I did accidentally simulate the naive web developer experience with my game project, because of
how I was prototyping. In game development, the core game mechanics have to be fun or else the rest of the game is
not worth making. So I whipped up a prototype wireframe in HTML5 to get a feel for core puzzle mechanics, purposely
disregarding cross platform considerations. I willfully ignored phones and tablets and Safari, and iterated just on
my own MacBook with only one browser.&lt;/p&gt;
&lt;p&gt;Only later, after the wireframe game mechanics had been refined with playtesting, did I decide to polish the
prototype into a cross-browser game that also worked well on mobile. If you work at a giant company you wouldn't
work like this; in fact you probably wouldn't get to design the UX &lt;em&gt;at all&lt;/em&gt;, because you'd receive the proposed
interactions from a paid UX designer.&lt;/p&gt;
&lt;p&gt;But a new developer working alone probably would do this? And why shouldn't they? It &lt;em&gt;should&lt;/em&gt; be reasonable to get
an idea working on one's own computer first, and not be surprised later by massive rework on a second device/browser.&lt;/p&gt;
&lt;p&gt;So, where are we on that in 2025? To find out, I compiled all the browser bugs and rework tasks from my game
project into this list.&lt;/p&gt;
&lt;h2&gt;&lt;a&gt;&lt;/a&gt;TLDR: buy an old iPhone and test with it daily&lt;/h2&gt;
&lt;p&gt;If you don't read the rest of this rant, I can sum up my advice as just this: if you're making a web project, even
a simple one, do your rapid, many-times-a-day iteration loop testing on an &lt;strong&gt;older iPhone&lt;/strong&gt; as your test mule.
Yes this is a pain, because none of us are programming on an iPhone soft keyboard. We're sitting at a computer
or laptop, and so that's the platform it's most natural to iterate on. Most frontend tools do not make it easy to
have a quick edit-and-reload cycle with a real mobile device.
So you'll have to either frequently push to a private web server, or use some
&lt;a href="https://apps.apple.com/us/app/ssh-tunnel-with-socks5-proxy/id1260223542"&gt;exotic ssh tunnel&lt;/a&gt; contraption,
to get it so that your test mule iPhone can view your test web project.&lt;/p&gt;
&lt;p&gt;This is not because iPhones are good, &lt;strong&gt;it's because they're bad.&lt;/strong&gt;
iPhones are more peculiar and less compliant than any other device I tried. I promise that if you get your
web project looking good and working smoothly on a crappy iPhone, your
residual costs to test and polish on all other platforms and browsers will be fairly low. (For extra bravery,
I recommend &lt;strong&gt;Firefox iOS&lt;/strong&gt; instead of Safari, because it is the most buggy, least compliant browser
I was able to find. If it works on Firefox iOS it's going to work anywhere. See below.)&lt;/p&gt;
&lt;p&gt;Yes all the desktop browsers have some form of "design for mobile" mode, and that's better than nothing.
But these tools will not reveal 80% of the problems I've described below. Not even the Safari responsive design
mode. Not even the XCode device simulator.&lt;/p&gt;
&lt;p&gt;If you'd like to hear more, then let's start with some gleeful indignation at one of the biggest culprits
in web development:&lt;/p&gt;
&lt;h1&gt;&lt;a&gt;&lt;/a&gt;Culprit #1: Apple has opinions&lt;/h1&gt;
&lt;p&gt;Writing this with fresh bruises from my iOS testing lab, I feel like it's fair to say that Apple's
mobile devices (iPhone, iPad) wilfully and unapologetically diverge from the rest of the web more than any other
platform. Safari has a few bugs and is the slowest to support any given web API. But this section is not about
Safari's &lt;strong&gt;bugs&lt;/strong&gt; and &lt;strong&gt;sandbagging&lt;/strong&gt;; more on that below.
This section is just about Safari's &lt;strong&gt;strong opinions&lt;/strong&gt;. Here are few issues I hit:&lt;/p&gt;
&lt;h2&gt;Can't use the whole screen&lt;/h2&gt;
&lt;p&gt;In a game, and arguably in any well designed UX, the application wants to use all the available screen space.
The design should neither waste space on unintended gutters or dead pixels, nor should it confuse the user
by letting important information fall below the fold where the user will have to scroll to get at it,
or will be confused because they won't notice it. I want to simply fill the whole canvas but not overspill it,
and this is comically difficult on iOS.&lt;/p&gt;

&lt;a href="https://zerotrickpony.com/articles/browser-bugs/browser-bar-resize.mp4"&gt;
&lt;img src="https://zerotrickpony.com/articles/browser-bugs/browser-bar-resize-screenshot.jpg"/&gt;
&lt;/a&gt;

&lt;p&gt;&lt;strong&gt;Mandatory automatic resizing:&lt;/strong&gt; First, the navigation bar at the top of the screen will appear and dissappear depending on the user's most recent
gesture. There is no event your application can receive to know this is happening, nor any measurement you can take
to detect it.
[surprising problem #1]&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://caniuse.com/?search=vh"&gt;standard CSS features &lt;code&gt;vw&lt;/code&gt; and &lt;code&gt;vh&lt;/code&gt; units&lt;/a&gt; are broken by this
behavior, so trying to create a whole-page user experience will break on iOS Safari.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Broken CSS units:&lt;/strong&gt; Experienced web developers have &lt;a href="https://css-tricks.com/the-trick-to-viewport-units-on-mobile/"&gt;tried complicated tricks&lt;/a&gt;
to work around this resizing behavior. Later, &lt;a href="https://caniuse.com/?search=svh"&gt;new units were introduced&lt;/a&gt; to CSS to try to give
the web page more control over this. And although newer Safari iOS versions support them,
&lt;a href="https://github.com/mozilla-mobile/firefox-ios/issues/11574"&gt;Firefox iOS still doesn't.&lt;/a&gt;
[#2]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Anti-competitive non-standards:&lt;/strong&gt; Relatedly, there is a &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Element/requestFullscreen#browser_compatibility"&gt;Fullscreen API&lt;/a&gt; in the web standards, which every browser supports... except Safari iPhone.
[3]
The &lt;a href="https://developer.apple.com/forums/thread/133248"&gt;community hypothesizes that fullscreen is purposely hobbled&lt;/a&gt; on iOS
so that Apple can sell more native iOS games without web-based games being a viable competitor. When I considered
using the fullscreen web standard to give my game better use of the screen, I was thwarted by this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Useless fullscreen behavior:&lt;/strong&gt; The Fullscreen API is
&lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Element/requestFullscreen#browser_compatibility"&gt;&lt;em&gt;supposed&lt;/em&gt; to work on iPad&lt;/a&gt;,
and I wasted time trying to offer it to my users. But it has
three ridiculous problems: First, it replaces the top of the screen with a black area and an irremovable "X"
which doesn't actually reclaim any more space than the address bar would have.
[4]&lt;/p&gt;
&lt;p&gt;Second, if the user &lt;em&gt;scrolls down&lt;/em&gt; in any element anywhere on the page, full-screen mode ends.
[5]&lt;/p&gt;

&lt;img src="https://zerotrickpony.com/articles/browser-bugs/ios-fullscreen-keyboard-error.jpg"/&gt;

&lt;p&gt;Third, Safari will &lt;em&gt;refuse to summon the soft keyboard&lt;/em&gt; if fullscreen is currently on.
Newer versions of Safari simply exit fullscreen, but in iOS 12 I encountered this hilarious error dialog informing the user
that Apple believes the web page is phishing them. (!)
[6]
Since my game had both scrolling elements and occasionally needed the keyboard,
I gave up in disgust so now I simply hide the fullscreen button on iOS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Workarounds:&lt;/strong&gt; My solutions were to (a) give up on the Fullscreen API on iOS, (b) use &lt;code&gt;position:fixed&lt;/code&gt; and some cleverness with
CSS transforms to fill the screen at least horizontally, and (c) to blindly add in some height fudge factors to try to stay below
the unknowable threshold of vertical scrolling on these browsers. My game now uses &lt;em&gt;almost&lt;/em&gt; the whole screen on iOS.&lt;/p&gt;
&lt;h2&gt;Touch screen problems&lt;/h2&gt;
&lt;p&gt;Apple's mobile browser is perfectly okay for scrolling to read articles and tapping hyperlinks. But for my
game I needed to design more interactive elements. Although I tried to keep it simple, I was still hit
by a number of Apple-specific surprises:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dragging does what:&lt;/strong&gt; Dragging with the mouse on a desktop browser will drag any &lt;code&gt;draggable&lt;/code&gt; elements, or
do text selection otherwise. But on Safari iOS, dragging a finger will only scroll.
[7]
Even if the touched area isn't scrollable, and even if the finger lands on a &lt;code&gt;draggable&lt;/code&gt; element, swiping gestures only
scroll. If the user does a compound press-and-hold-and-then-drag gesture, then on Android this will drag
the &lt;code&gt;draggable&lt;/code&gt; element but on iOS it will select text instead.
[8]
I was able to fix this with a liberal sprinkling of &lt;code&gt;user-select:none&lt;/code&gt; styles on many more elements than Android needed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is scrollable:&lt;/strong&gt; In the name of saving space and beautifying the display, Apple defaulted us all to
the "overlay" style of scroll bars. These elements are translucent, narrow, or entirely invisible for most
of their lifetime. The result looks simple and attractive, and next time I want to print out my phone's screen and
hang it on my wall as art, I'll thank them for that. The rest of the time, this makes it impossible for the
user to discern which areas of the screen are scrollable. This is probably a perfectly reasonable tradeoff
decision for when the browser is displaying a wikipedia article about raccoons. But for anything more interactive,
the inability for the user to discover what is scrollable is pretty bad. I had to add on-screen glinting elements
to draw the user's attention to the otherwise invisible scrolling areas.
[9]&lt;/p&gt;

&lt;a href="https://zerotrickpony.com/articles/browser-bugs/hittest_bug_ipad.mp4"&gt;
&lt;img src="https://zerotrickpony.com/articles/browser-bugs/tap-test-screenshot.jpg"/&gt;
&lt;/a&gt;

&lt;p&gt;&lt;strong&gt;Apple's finger location algorithm:&lt;/strong&gt; I assigned &lt;code&gt;click&lt;/code&gt; Event listeners to various on-screen objects, some of which were
&lt;code&gt;&amp;lt;button&amp;gt;&lt;/code&gt; but many of which were &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; elements. My testing revealed that Android Chrome will deliver
a click event if the user's finger touches most/all of the area of the element. If the finger hits multiple
elements, Chrome Android delivers the event to the one closest to its own proprietary idea of the perceptual center
of the finger touch. When I tested my game board UX scaled &lt;em&gt;way&lt;/em&gt; down for a tiny mobile screen, I was pleasantly
surprised to find that the user can &lt;a href="https://zerotrickpony.com/articles/browser-bugs/hittest_bug_android.mp4"&gt;still tap them somewhat successfully&lt;/a&gt; even if they're visually small...
&lt;a href="https://zerotrickpony.com/articles/browser-bugs/hittest_bug_ipad.mp4"&gt;but not on iPad&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On Safari iOS with a scaled down UI, the user has to get the &lt;em&gt;center&lt;/em&gt; of their finger to hit
the element or else the click event won't be delivered. [10]
This made my game effectively unplayable on iOS because the playing field had 10px hit targets which were too
small to tap. I fixed it by creating a large invisible &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; over every dot on the field, to catch touch input on iOS.&lt;/p&gt;
&lt;h2&gt;Can't play audio&lt;/h2&gt;
&lt;p&gt;My game has both sound effects and background music. Sound was exotic on the web in 2004, but it's long since been
standardized... supposedly. Not counting composing the sounds in Garage Band, I got these working
on my laptop's browser in under ten minutes. Then getting them to work on iPad and iPhone took me another
week of effort.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AudioContext always sounds silent:&lt;/strong&gt; My first try at sound effects used an &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioContext#browser_compatibility"&gt;AudioContext&lt;/a&gt; to wire up Gain and Oscillator nodes to play game beeps. These features are
"fully supported" on iOS, but no sound would be produced. [11]
I eventually learned that if you use these APIs
to make sound, &lt;a href="https://stackoverflow.com/questions/40789136/ios-ringer-switch-mutes-web-audio"&gt;iOS considers them to be "ringtones"&lt;/a&gt;
rather than "media", which means that the user's
media volume control will have no effect on the game's sounds. The user must enter their "Settings" app and
change their device's ring volume. The normal hard buttons for volume up and down on an iOS device control
media volume, not ring volume. I despaired of instructing any user to do this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Audio tag play() does not function:&lt;/strong&gt; Next I moved to using &lt;code&gt;&amp;lt;audio/&amp;gt;&lt;/code&gt; tags to play pre-recorded sounds in response
to in-game events. I created six &lt;code&gt;HTMLAudioElement&lt;/code&gt; objects for my five kinds of sound effect, and one
more to play the background music. However, the &lt;code&gt;.play()&lt;/code&gt; method, which is marked on
&lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/play#browser_compatibility"&gt;MDN&lt;/a&gt;
as "fully supported" and "widely available", does not function at all on iOS.
[12]&lt;/p&gt;
&lt;p&gt;You see, Apple is of the opinion that &lt;code&gt;.play()&lt;/code&gt; method should only work if it's
&lt;a href="https://stackoverflow.com/questions/66300840/trying-to-play-an-mp3-via-js-on-ios-browsers"&gt;in response to a user action&lt;/a&gt;,
which they implement by checking if the user has tapped an element on the page. Since a game
has story events for the user to &lt;em&gt;react&lt;/em&gt; to, no such sound effects could work. I worked around it (see below)
but it made the system very complicated.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Audio tags skip the first 300ms of sound after the first play:&lt;/strong&gt; Safari will play MP3s through the &lt;code&gt;HTMLAudioElement&lt;/code&gt; ---
&lt;em&gt;once&lt;/em&gt;. But after each tag plays once, Safari aggressively unloads the media, I presume to save memory.
[13]
I think this is reasonable, &lt;em&gt;except&lt;/em&gt; that then when my sound effect plays a second time, the audio has
been unloaded and Safari plays silence until it can get the MP3 data back into its buffers again. I would
actually have been fine with this behavior if it were to wait to play until the audio was loaded, but it
doesn't do that. Safari is of the opinion that it's more important to keep up with the scheduled play
of the sound &lt;em&gt;than to hear it&lt;/em&gt;. So it non-deterministically jumps into the middle of the sound, cutting off the beginning.
This made all my sound effects seem glitchy and truncated.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Audio volume not controllable:&lt;/strong&gt; I created a volume control for both music and sound effects in the game,
in case the user wanted to mute them or make them quieter. I did this by setting the &lt;code&gt;.volume&lt;/code&gt; property
on the Audio tags, and this turned out to be broken on iOS.
[14]
(To their credit, MDN lists the &lt;code&gt;.volume&lt;/code&gt; property of media tags as
&lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/volume#browser_compatibility"&gt;"limited availability"&lt;/a&gt;.)
Apple simply says "volume is under the user's control",
and does not try to make the thing work. Apple &lt;a href="https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/Using_HTML5_Audio_Video/Device-SpecificConsiderations/Device-SpecificConsiderations.html#//apple_ref/doc/uid/TP40009523-CH5-SW1"&gt;documents its behavior&lt;/a&gt;
like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Reading the volume property always returns 1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This would be a handy behavior to at least write some feature detection logic around, but sadly it's not true.
[15]
The actual behavior is that if you modify the &lt;code&gt;.volume&lt;/code&gt; property
of an &lt;code&gt;HTMLAudioElement&lt;/code&gt; then it &lt;em&gt;will&lt;/em&gt; seem to change, but (a) no actual volume change will occur, and
(b) at the next event loop idle, the volume value will be set back to &lt;code&gt;1&lt;/code&gt;. This wasted an hour of my time
to isolate the true behavior and write complicated feature detection logic to check that mutation of the &lt;code&gt;.volume&lt;/code&gt;
property survives past the next idle loop.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Workarounds:&lt;/strong&gt; The &lt;a href="https://stackoverflow.com/questions/31776548/why-cant-javascript-play-audio-files-on-iphone-safari"&gt;recommended workaround&lt;/a&gt;
for the anti-autoplay behavior is to create a little invisible farm of dummy &lt;code&gt;HTMLAudioElement&lt;/code&gt; objects
which play silent sounds when the user taps anything on the screen. I inserted a "start" screen into my
game when the page loads, which has no purpose except to provide the user with something to tap before
the game begins.&lt;/p&gt;
&lt;p&gt;Once the user taps, I can have the farm of elements play their silent sounds once. Then the programmer is
free to rewrite their &lt;code&gt;src&lt;/code&gt; attribute to play other media at any time.
This works because the constraint to play() from within a user action event handler apparently only pertains to the first
use of play on a per-tag basis.
[16]
So after writing about 200 lines of priming logic, I had a bank of
sound effects working. (And can I just say: if this workaround works for me, it works for bad actors too.
So Apple's mitigation adds complexity but doesn't actually protect users as intended. Awesome!)&lt;/p&gt;
&lt;p&gt;To work around the cut-off sound effect problem, I re-recorded all my sound effect MP3s in Garage Band to
start with 300ms of silence. This created an apparent lag in the responsiveness of the game, but at least the user
could hear the entire sound.&lt;/p&gt;
&lt;p&gt;To make mute work, I had to change my code to actually stop the audio instead of setting volume to zero.
I also had to write 20 lines of behavior probing code to detect the non-functioning &lt;code&gt;.volume&lt;/code&gt; property
and offer iOS users an "ON"/"OFF" switch without the gradiated volume controls.&lt;/p&gt;
&lt;h1&gt;&lt;a&gt;&lt;/a&gt;Culprit #2: Behaviors and bugs&lt;/h1&gt;
&lt;p&gt;Although a lot of the behavior differences I've described above are annoying opinions, there are definitely also some
more forgivable but still annoying variances in behavior within the standards. And also, good old browser bugs.&lt;/p&gt;
&lt;h2&gt;Layout problems&lt;/h2&gt;
&lt;h3&gt;Fonts vary by platform, even when they shouldn't&lt;/h3&gt;

&lt;a href="https://zerotrickpony.com/articles/browser-bugs/fonts-comparison.jpg"&gt;
&lt;img src="https://zerotrickpony.com/articles/browser-bugs/fonts-comparison.jpg"/&gt;
&lt;/a&gt;

&lt;p&gt;&lt;strong&gt;Font names aren't reliable:&lt;/strong&gt;
I had a lot of small layout glitches caused by browser differences in choice of font, even fonts that could easily be
exactly the same on all platforms like &lt;code&gt;Arial&lt;/code&gt; and &lt;code&gt;Courier&lt;/code&gt;.
[17]
I fixed these by using custom &lt;code&gt;woff2&lt;/code&gt; fonts loaded
at game start. In a normal web application I think it's arguable that the application should set a specific font,
but in a game where the fonts are part of the aesthetic, this is table stakes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aside: woff2 is great:&lt;/strong&gt;
On the bright side, I was pleasantly surprised at how consistent layout can be across platforms when using &lt;code&gt;woff2&lt;/code&gt; fonts
to prevent font fallback.
(And &lt;code&gt;display:grid&lt;/code&gt; is also very consistent, more on that later.)
These were well supported even on my worst browser (Firefox iOS) and my second worst browser (Safari on an old iOS 12
device I had lying around). It is &lt;em&gt;almost&lt;/em&gt; attainable to have pixel-perfect consistency across platforms with just
these two features. I had not realized how much fonts (and localization, if your project has that) are such a large
source of layout ambiguity.&lt;/p&gt;
&lt;p&gt;(Yes yes, expecting pixel-accurate layout across devices is unreasonable. Yes yes make your designs responsive
so that they'll gracefully handle any shape and size of screen and font setting. But since I'm making a &lt;em&gt;game&lt;/em&gt;,
the interaction design had a hard requirement of certain things being on the same screen relative to each other.
The normal vaguaries of the web would result in broken interactions. My responsive compromise was to define
five specific layout sizes and snap to each of those based on font and zoom settings. This made it easier to
trust that a playtest of the game would work for a given size of screen.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Incorrect ligature defaults on Safari iOS:&lt;/strong&gt;
I used Google's very clever &lt;a href="https://fonts.google.com/icons"&gt;Material design icons&lt;/a&gt; to save myself a lot of
time drawing and flowing simple icons in the app. It's implemented with a kooky abuse of the ligature feature
of custom font faces, which... weird! But okay! Unfortunately even when pasting the Google-provided code
snippet verbatim, Safari's defaults for the &lt;code&gt;font-ligature&lt;/code&gt; property are not to spec, so Google's icons don't
work on older Safari iOS.
[18]
I was able to fix this easily just by explicitly setting the property, but it
wasted about an hour of my time debugging that.&lt;/p&gt;
&lt;h3&gt;Safari iOS: late to every party&lt;/h3&gt;
&lt;p&gt;Although all the browsers adopt new features in their own time, it is my unscientific sense that Safari is
unusually slow. And it had a few more bugs, as well. To wit:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unsupported flex gaps:&lt;/strong&gt;
There are a lot of situations where I prefer the mental model of &lt;code&gt;display:flex&lt;/code&gt; over &lt;code&gt;display:grid&lt;/code&gt;, but this
caused a number of cross-browser unpleasant surprises. First, Safari was unreasonably late to the party on
&lt;a href="https://developer.mozilla.org/en-US/docs/Web/CSS/Reference/Properties/column-gap#browser_compatibility"&gt;correctly supporting &lt;code&gt;row-gap&lt;/code&gt; and &lt;code&gt;column-gap&lt;/code&gt; with &lt;code&gt;flex&lt;/code&gt;&lt;/a&gt;,
even though they do what you expect with &lt;code&gt;grid&lt;/code&gt; as far back as iOS 12.
[19]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flex shrink problems:&lt;/strong&gt;
Safari chooses to aggressively shrink text in &lt;code&gt;flex&lt;/code&gt; layouts in situations where other browsers do
not. Specifically it seemed to be related to when a &lt;code&gt;flex-grow&lt;/code&gt; area was marked as scrollable. Other browsers
would let the elements before and after the scrollable item size naturally, but Safari truncates them.
[20]
So after a lot of layout testing I had to sprinkle &lt;code&gt;flex-shrink:0&lt;/code&gt; in a bunch of places. Ugh.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Missing support for lh units:&lt;/strong&gt;
I had used the &lt;code&gt;lh&lt;/code&gt; length unit in some of my layouts, not realizing that Safari didn't add support for this
&lt;a href="https://caniuse.com/?search=lh+unit"&gt;until last year&lt;/a&gt;.
[21]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Missing support for scale syntax:&lt;/strong&gt;
In all browsers I tested except Safari iOS 12, you can say &lt;code&gt;transform:scale(50%)&lt;/code&gt; but this does not work on
Safari. I had to change these to &lt;code&gt;transform:scale(0.5)&lt;/code&gt;.
[22]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Missing support for ResizeObserver:&lt;/strong&gt;
Safari iOS now supports &lt;code&gt;ResizeObserver&lt;/code&gt; for reacting to element size changes, but it was last to do so. To
support older browsers I had to write some nasty timer-based measurement code.
[23]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bugs in CSS custom properties:&lt;/strong&gt;
I tried to set the &lt;code&gt;background-image&lt;/code&gt; of my game using a &lt;code&gt;var(--my-background-url)&lt;/code&gt; CSS custom property.
This seemed to mostly work, but in my testing I found that if the custom property changes, Safari will not
propagate it correctly to the &lt;code&gt;BODY:backdrop&lt;/code&gt; element and will cause a missing background bug in full screen.
(I was also able to reproduce this on Safari MacOS, but no other MacOS browsers.)
[24]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Late support for smooth scrolling:&lt;/strong&gt;
I had code in my game to programmatically scroll an element during the tutorial, to show the user that
it can be scrolled. I set the &lt;code&gt;scroll-behavior:smooth&lt;/code&gt; CSS property so that the user's attention would be
drawn to the animating motion of the scrolling area. Older versions of Safari iOS disregarded this directive,
making the element pop instantaneously instead of visibly scrolling. This doesn't seem like a failure,
but without it my tutorial failed to teach the user something important.
[25]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Button spacing:&lt;/strong&gt;
Safari, and only Safari, adds a bunch of padding to their &lt;code&gt;&amp;lt;button&amp;gt;&lt;/code&gt; elements which you have to style
away if you care how big your buttons are.
[26]
(Maybe they do this to compensate for their hit target choices?)
Although the agent stylesheet is allowed to contain anything
it wants, in practice this can disrupt the intended layout. I didn't catch it until I did cross-browser testing.&lt;/p&gt;
&lt;h3&gt;meta viewport incantations&lt;/h3&gt;

&lt;img src="https://zerotrickpony.com/articles/browser-bugs/metaviewport-differences.jpg"/&gt;

&lt;p&gt;Single-page application designs that want to work on mobile will need to sprinkle attributes into the &lt;code&gt;meta&lt;/code&gt;
header tag in order to persuade mobile browsers to scale elements predictably:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;meta name="viewport" content="width=device-width, height=device-height, initial-scale=1"&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Without these directives, browsers apply various opinionated fudge factors to try to guess what the web
page &lt;em&gt;intended&lt;/em&gt; while disregarding how the web page is actually styled.
Viewport control directives are
very useful and worth understanding... but without them, desktop browsers tend to do about the same thing,
and mobile browsers do quite different things.
[27]&lt;/p&gt;
&lt;h3&gt;Firefox iOS has no idea where the edges of the screen are&lt;/h3&gt;
&lt;p&gt;Firefox iOS wins the award for least competent browser in this area:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Broken CSS units:&lt;/strong&gt; Firefox iOS has a &lt;a href="https://github.com/mozilla-mobile/firefox-ios/issues/11574"&gt;bug&lt;/a&gt;
which prevents use of the special &lt;code&gt;svh&lt;/code&gt; and &lt;code&gt;lvh&lt;/code&gt; units which are themselves workarounds to the
self-hiding address bar behavior that all the mobile browsers have.
[28]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Occluded layout on iPhone:&lt;/strong&gt; Newer iOS devices lack a physical home button, instead consuming the
bottom edge of the touch screen with a line-shaped affordance which the user is supposed to know to
swipe or touch depending on their intention. Applications are supposed to either stay out of that area
or draw transparently behind it. Firefox does neither! It simply draws your web page with some of the
bottom edge cut off, but only on these newer iOS devices.
[29]
Safari doesn't have this problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Crash on fullscreen:&lt;/strong&gt; Firefox iOS overrides the &lt;em&gt;requestFullScreen&lt;/em&gt; function with an agent script...
but that agent script has a &lt;a href="https://github.com/mozilla-mobile/firefox-ios/issues/30540"&gt;bug&lt;/a&gt;
so that if you call it, you just get a Javascript error. Cool.
[30]&lt;/p&gt;
&lt;h3&gt;Mobile browsers have crazy behavior when you pinch zoom&lt;/h3&gt;
&lt;p&gt;To try to fill the screen on all platforms I admittedly made a fragile choice: I used &lt;code&gt;transform:scale&lt;/code&gt;
to ensure that the main screen element was centered and sized. This worked like a charm on all the
desktop browsers. And it &lt;em&gt;almost&lt;/em&gt; worked on all the mobile browsers too. Except that if the user
used pinch zoom in and then out again, the browser would draw the screen with a few pixels at the top
missing.
[31]
The more times the user did this, the more pixels would be missing.
[32]
My application was not receiving any resize, drag, or scroll events when this happened, and the measurements of the screen
as reported by &lt;code&gt;offsetTop&lt;/code&gt;, &lt;code&gt;scrollTop&lt;/code&gt;, etc were all unchanged. I spent many hours trying to find
a cause or workaround for this problem.&lt;/p&gt;
&lt;p&gt;Eventually I created an isolated test case, and fixed the problem by adding a &lt;code&gt;position:fixed;top:0;left:0;&lt;/code&gt;
style to the root element. There is &lt;strong&gt;no&lt;/strong&gt; standards-based reason why that should have
fixed the problem, but it did.&lt;/p&gt;
&lt;h2&gt;Animation and graphics woes&lt;/h2&gt;
&lt;p&gt;Games need more precisely controlled animations and visuals than is typical for a Wikipedia page
about raccoons. This caused me more than a few problems:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Text selection styling:&lt;/strong&gt;
It is technically possible to style the selected text using the &lt;code&gt;::selection&lt;/code&gt; pseudoclass, and I tried
to use this to create an animating typewriter effect in my game. Sadly,
&lt;a href="https://developer.mozilla.org/en-US/docs/Web/CSS/Reference/Selectors/::selection#browser_compatibility"&gt;this is not supported on Safari iOS&lt;/a&gt;
so I had to rework the animation to work by modifying the DOM in a Javascript loop. Inelegant but functional.
[33]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Blur bugs:&lt;/strong&gt;
I used &lt;code&gt;filter:blur()&lt;/code&gt; and &lt;code&gt;backdrop-filter:blur()&lt;/code&gt; CSS properties in various places to
create glow and frosted glass effects for the game's visuals. These &lt;em&gt;sometimes&lt;/em&gt; worked in Safari,
but were broken in certain situations, like on &lt;code&gt;&amp;lt;svg&amp;gt;&lt;/code&gt; elements.
[34]
Safari was also late to support this
and needed a &lt;code&gt;-webkit-backdrop-filter:blur()&lt;/code&gt; variation to get the visual effect to appear
on older tablets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sync animations:&lt;/strong&gt;
The &lt;code&gt;animation&lt;/code&gt; CSS property is surprisingly excellent, a much needed alternative to the often
useless &lt;code&gt;transition&lt;/code&gt; system, especially on browsers that don't yet support the just-added &lt;code&gt;content-visibility&lt;/code&gt;
property. (There, I said something nice about web standards.)
There's a &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Animation"&gt;Javascript API&lt;/a&gt;
to manipulate and restart animations so that (for example) blinking warning lights can be sync'd
up with each other. As usual my 2019 iPad was just old enough to not support this.
[35]
I didn't bother with any workaround, so on older tablets this remains a glitch.&lt;/p&gt;
&lt;h2&gt;Audio problems&lt;/h2&gt;
&lt;p&gt;Apple's audio opinions described above were the biggest problem with audio, but Apple wasn't the only culprit:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Error on play interruption:&lt;/strong&gt;
Chrome, and only Chrome, is of the opinion that it's a bug to &lt;code&gt;.pause()&lt;/code&gt; or assign &lt;code&gt;.currentTime&lt;/code&gt; on an audio
element while a previous &lt;code&gt;.play()&lt;/code&gt; request is "in progress", whatever that means. The other browsers will simply
fulfill your request, but not Chrome. Chrome generates an unhandled promise rejection which looks like a game
crash when the audio pauses.
[36]
So I had to write some complicated guarding logic to protect the audio widgets
from concurrent manipulation during their sensitive moments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Popping and skipping sounds:&lt;/strong&gt;
Firefox on both MacOS and Android cannot seem to just run an &lt;code&gt;OscillatorNode&lt;/code&gt; competently. Before I switched away from &lt;code&gt;AudioContext&lt;/code&gt;,
I found that although Chrome and Safari on MacOS could play my graph of sound effects smoothly, Firefox
introduced "pops" (audio artifacts caused by incorrectly reproducing a sound wave) and missing sounds.
[37]
I was able to mostly fix these by adding more attenuation ramps and silence at the start and end of sounds,
but I definitely shouldn't have to. Avoiding mathematical errors that cause popping sounds should be
table stakes for any piece of software that produces audio. You guys.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Safari iOS late to support AudioContext:&lt;/strong&gt;
&lt;code&gt;AudioContext&lt;/code&gt; is documented as &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioContext#browser_compatibility"&gt;fully supported&lt;/a&gt;
since forever, but you have to read the fine print! The function has been in Safari iOS
for many years, but only if you call it via the special &lt;code&gt;webkitAudioContext&lt;/code&gt;.
[38]
It was only renamed to match the other browsers a couple years ago. So that wasted a half hour of my
time to chase down.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;iPad GainNode timings inaccurate:&lt;/strong&gt; I found that Safari on iOS 12 does not honor linear gain ramps the way other browsers do.
[39]
So even once my sound effects were audible,
they sounded like strange whooping slide whistles on iPad even though they sounded correct on Android and MacOS.
I can't tell if this is a bug or an opinion, but either way, it was unacceptable. These two problems caused me
to abandon the &lt;code&gt;AudioContext&lt;/code&gt; API entirely. (My iOS 17 device doesn't have this behavior, so I guess Safari agreed
that it was a bug.)&lt;/p&gt;
&lt;h2&gt;Other interactions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Drag and drop control:&lt;/strong&gt; My drag-and-drop Safari woes were not only caused by Apple's strong opinions. I found that I had quite
a few touch screen bugs caused by varying interpretations of CSS and HTML standards. For example, I was
using &lt;code&gt;inert=true&lt;/code&gt; to prevent interactions, but older Safari browsers ignore this attribute.
[40]
I had to also add &lt;code&gt;user-select:none; -webkit-user-select:none; pointer-events:none;&lt;/code&gt; in various places to get
Safari to stop showing my users unwanted text selection areas when dragging was wanted.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Draggable sprites:&lt;/strong&gt; I did get dragging to work on Safari iOS, but I found that if the &lt;code&gt;draggable&lt;/code&gt; element is a &lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt;
which wraps to the next line, then Safari will garble the visual effect of the dragged sprite as
it moves on the screen.
[41]
I ended up reworking the draggable elements to ensure that they could never have a line break.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Clipboard API:&lt;/strong&gt; I used the clipboard API for a simple import/export game feature, since it is documented as
&lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Clipboard#browser_compatibility"&gt;fully supported&lt;/a&gt;
on all major browsers. As usual Safari iOS was the last to add support, and I found that my old iPad
was &lt;em&gt;just&lt;/em&gt; old enough to fail with this API.
[42]&lt;/p&gt;
&lt;h2&gt;Microsoft Edge: copying Chrome's homework real hard&lt;/h2&gt;
&lt;p&gt;The only browser I tested which needed (almost) no rework was Microsoft Edge on Windows 10.&lt;/p&gt;

&lt;img src="https://zerotrickpony.com/articles/browser-bugs/scrollbar-unstyled.jpg"/&gt;

&lt;h3&gt;Styling for traditional scrollbars&lt;/h3&gt;
&lt;p&gt;The one problem I observed with Edge was scrollbar styling.
On Windows 10 the default scroll bar type is more likely to be "traditional" than
"overlay", so I happened to notice that glaring white scrollbars were shown in my dark-themed space game.
[43]
This situation can arise on any OS, not just Windows, because "always show scrollbars" is a user-facing accessibility setting on
several platforms.&lt;/p&gt;
&lt;p&gt;I fixed this by styling darker scrollbars for all browsers using the newly
available &lt;code&gt;scrollbar-color&lt;/code&gt; CSS property.
(Except for Safari, [44] which requires a &lt;code&gt;::-webkit-scrollbar&lt;/code&gt; pseudoclass, of course.)&lt;/p&gt;
&lt;p&gt;Other than this issue, Microsoft Edge was impressively trouble-free.
It passed my full test suite on the first try, and had no other platform-specific problems.
I attribute this not particularly to the skill of Microsoft, but that my game was already very well tested on
Chrome MacOS. Edge seems not to have diverged much at all from the Chrome ancestry from which was is forked.&lt;/p&gt;
&lt;h1&gt;&lt;a&gt;&lt;/a&gt;Culprit #3: Mobile Redesign&lt;/h1&gt;
&lt;p&gt;Zero web druids will be surprised to hear me say that the biggest source of pain with designing a polished,
interactive HTML5 application is the difference in browser behavior between &lt;strong&gt;phones and tablets&lt;/strong&gt; vs desktop computers.
Of course within the priesthood we consider "responsive design" to be table stakes. But taking a step back
from our craft for a moment: is it so crazy for a novice developer to expect that they could design a
web-based project on their laptop and see it work fine on a phone with no changes? Why can't this be a reality?&lt;/p&gt;
&lt;p&gt;Early on in the popularization of touch screens, manufacturers like Apple made some reasonable but aggressive
decisions to permanently fracture the desktop and mobile browsing experiences.
I don't think these were bad decisions! When a user has a small touch screen, the most efficient interactions for them aren't
the same as when they have a very large display, a mouse or trackpad, and a hard keyboard.
But it did double the size of the design space, and theoretically, this could have been avoided.&lt;/p&gt;
&lt;p&gt;Here are some mobile-specific snags I hit while cosplaying a novice web developer:&lt;/p&gt;
&lt;h2&gt;Tapping and touching&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Dragging does what, part 2:&lt;/strong&gt; You would think that dragging would work easily on touch screens,
and I did sort of expect that my game's drag-and-drop interactions would port nicely to mobile.
Except no! Because mobile browsers have reserved dragging for themselves. Vertical swipe,
edge-swipe, and other finger-drag type inputs
command the browser to scroll, navigate, and select text. Those events either don't reach the web
page's elements at all, or they are delivered as an overload with other interactions.
[45]
Various CSS hints like &lt;code&gt;pointer-events:none;&lt;/code&gt; and &lt;code&gt;user-select:none&lt;/code&gt; must be sprinkled into the page
to control these interactions more carefully.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Surprise zoom:&lt;/strong&gt; One silly example is double-tapping. The &lt;code&gt;&amp;lt;button&amp;gt;&lt;/code&gt; element which was designed to submit forms
to a server never needs to be repeatedly tapped. But in a game, the user may repeatedly tap to fire a weapon
or move a gamepiece. On mobile, these repeated taps will rather surprisingly zoom in on the
button.
[46]
These elements must be given the magical &lt;code&gt;touch-action:manipulation;&lt;/code&gt; CSS style to
hint that double-tapping should not be interpreted by the browser. One more thing to know.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What do you call it?&lt;/strong&gt; This is a very small thing, but induced a fair amount of rework for me so
I will mention it here: My game has on-screen instructions which tell the user how to play.
A web page about raccoons would not need to provide instructions saying, "here is how to scroll
down! Here is how to click hyperlinks!" ...but a game does need this! My original prototype
had a lot of "click the arrow to continue" type text, but the word "click" is not intuitive
language for someone using a touch screen. I could have left it in, figuring that most people
could make the deductive leap that anything that could be "clicked" with a mouse they didn't
have could also be "tapped" using their available fingers. Since I wanted the game to feel
as native as possible, I didn't want to leave this in. But I couldn't think of any single
verb that would feel equally intuitive to both mouse and touch users?
[47]
So I added complexity to the game to &lt;a href="https://peterscene.com/blog/detecting-touch-devices-2018-update/"&gt;detect the presence of a touch screen&lt;/a&gt;,
which is much harder than it sounds, and use the word "click" or "tap" depending on the device.&lt;/p&gt;
&lt;h2&gt;Hovering&lt;/h2&gt;
&lt;p&gt;Not because hovering is the most pivotal interaction, but it's a simple example of a willful divergence:
When the user places their mouse inside a region of the screen, that region's "hover" interactions will trigger.
This can be used for non-essential visual texture such as slightly brightening a button to cue its
interactivity. But it &lt;em&gt;could&lt;/em&gt; be (and is) used for anything else, like revealing a hidden sub-menu which
is critical to the user's task success.&lt;/p&gt;
&lt;p&gt;Arguably, mobile browsers &lt;em&gt;could have been&lt;/em&gt; designed to give touch screen users a way to use interfaces
designed for a mouse. As a strawman example, Safari for iPhone could show a little crosshair that you move around with
your finger. When the crosshair enters a region of the screen, that region's &lt;code&gt;:hover&lt;/code&gt; interaction could be
shown, giving the touch screen user the same experience as a mouse user.
But mobile browser designers &lt;em&gt;purposefully chose&lt;/em&gt; to take away the ability to easily express hovering. I'm not saying
this was a bad choice: interpreting finger motions as immediate actions saves the user time and dexterity
difficulties. Moving a crosshair around might be annoying and difficult.&lt;/p&gt;
&lt;p&gt;Apple probably set this standard by being the first to sell an extremely popular touch screen web browser,
so they made this decision for all of us. Right or wrong, this decision means that users can't experience hover
interactions on their iPad. If a developer creates an important hover-based interaction, then they have
introduced an &lt;strong&gt;unsolvable design flaw&lt;/strong&gt; into their user interface which they won't discover without a
mobile testing lab.&lt;/p&gt;
&lt;p&gt;Is &lt;code&gt;:hover&lt;/code&gt; a web standard? Supposedly yes.
&lt;a href="https://developer.mozilla.org/en-US/docs/Web/CSS/Reference/Selectors/:hover#browser_compatibility"&gt;MDN&lt;/a&gt; and
&lt;a href="https://caniuse.com/?search=hover"&gt;caniuse&lt;/a&gt; both list it as "widely available" and describe it as "fully supported"
on mobile browsers. Caniuse mentions an iOS Safari glitch, but doesn't say "this will basically never work on mobile"
even though it should.
[48]
MDN is better, with a blue callout warning the developer to consider "accessibility"
on "devices with limited hovering capabilities". This is technically true but undersells the case pretty significantly.
Here's what MDN &lt;em&gt;should&lt;/em&gt; say:&lt;/p&gt;
&lt;code&gt;
&lt;blockquote&gt;
&lt;p&gt;Hover is effectively unusable on the world's most popular browsers and devices.
Web projects which seek to work consistently for all users should &lt;strong&gt;refrain entirely&lt;/strong&gt; from using this feature.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/code&gt;
&lt;p&gt;The web standards don't say this because web standards aren't trying to create a consistent experience for all
users. There &lt;em&gt;is&lt;/em&gt; tribal knowledge amongst seasoned professional web developers that hover "shouldn't" be used for
any "critical" interactions like providing the user with instructions, or concealing interactive elements necessary
for primary task completion. But if our novice designer/developer created a hover-based interaction and it worked on their
computer, they will not discover until mobile testing that they have stepped outside the safe harbor, and now
need to majorly rework their design.&lt;/p&gt;
&lt;h2&gt;Soft keyboard&lt;/h2&gt;
&lt;p&gt;There's a bunch of completely reasonable interactions that a novice developer could build without
thinking about the fact that they depend upon the user having a physical keyboard. Most mobile users
have access to only a soft keyboard, which (a) lacks a few capabilities and (b) consumes an exhorbitant
amount of screen space. This screen space is almost always better allocated to something else, which is
why mobile browsers aggressively hide the soft keyboard. Here's some problems that are easy to hit:&lt;/p&gt;
&lt;h3&gt;Shift-click&lt;/h3&gt;
&lt;p&gt;An early prototype of my game accelerated certain interactions with shift-click. But shift-click is not
offered on a mobile device, so these interactions had to be redesigned for mobile.
[49]
&lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/MouseEvent/shiftKey"&gt;MDN&lt;/a&gt; lists the &lt;code&gt;shiftKey&lt;/code&gt;
property of MouseEvent as "widely supported", even though soft keyboards on mobile will &lt;strong&gt;NOT&lt;/strong&gt; deliver
this event with &lt;code&gt;shiftKey&lt;/code&gt; set to true, ever. Safari at least has the honesty to say that
it's &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/TouchEvent/shiftKey#browser_compatibility"&gt;unsupported on TouchEvent&lt;/a&gt;.
The rest of the browsers document it as "supported" even though this will 100% not do what
the developer intends.&lt;/p&gt;
&lt;p&gt;You could argue that MouseEvent is obviously unsafe on mobile because there is no mouse. But despite the
name, mobile browsers &lt;em&gt;do&lt;/em&gt; work hard to transparently support the "click" event when the user taps,
and they deliver a &lt;code&gt;MouseEvent&lt;/code&gt; when the touch happens. There was clearly some attempt to ensure that
&lt;code&gt;MouseEvent&lt;/code&gt; was part of the "safe harbor" we all wish existed, but it's not a complete attempt.&lt;/p&gt;
&lt;p&gt;I hope at this point you're yelling at your screen, "these are accessibility design problems! You can't
expect MDN to protect you from not designing properly!!!" And to that I say --- I hear you, but,
are we sure about that? Consider: One major reason why accessibility is so bad across modern computer interfaces is that
&lt;strong&gt;developers must do something extra to offer accessibility&lt;/strong&gt;. But these secondary quality characteristics
will always be, well, secondary. One way we could have ensured that designs are accessible is to
&lt;strong&gt;make it impossible to build anything else.&lt;/strong&gt; Instead, we've filled the standard web API with conditional features that
don't work for most people, and then we describe them as "widely supported". We are making this
problem worse when we could be making it better.&lt;/p&gt;
&lt;h3&gt;Text input&lt;/h3&gt;

&lt;img src="https://zerotrickpony.com/articles/browser-bugs/soft-keyboard-landscape.jpg"/&gt;

&lt;p&gt;The &lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt; tag is like 30 years old, but that has apparently not been enough time for us to figure
out how to make it usable! My original game design had a good amount of typing in it, to let the user input their
guesses to solve a mystery. When I first tested this experience on mobile, I found that the soft keyboard's
consumption of the screen almost always covered up or reflowed an important clue that the user needed
to read as they typed their guess.
[50]
Worse, the game is wide-screen so the user would likely have their
phone tilted to landscape mode. In this orientation, the soft keyboard consumes much more than 50% of the
vertical screen space. What's left is almost entirely consumed by the text input area.
[51]&lt;/p&gt;
&lt;p&gt;This seemingly reasonable allotment of space leaves nothing for the clue context. This made the game
unplayable and required me to dramatically rework the design to eliminate typing. I spent more engineering
time on this rework than on any other single change for mobile. If I had prototyped on mobile first, I would
have realized this problem much sooner.&lt;/p&gt;
&lt;h3&gt;Input focus&lt;/h3&gt;
&lt;p&gt;With a hard keyboard device, calling &lt;code&gt;.focus()&lt;/code&gt; on a likely input element can be an incidental
convenience. It gives the user a very gentle hint at interactivity, and saves them a click if they would
like to start typing. But they can also easily ignore the focused element, and click something else on the
screen if they like.&lt;/p&gt;
&lt;p&gt;Soft and hard keyboards both need the unfortunate concept of keyboard focus, usually indicated
by some extra bold outlines and a blinking vertical line which users understand to be a "cursor".
Unlike on a laptop, a soft keyboard also needs a cue for when it is wanted. Showing the soft keyboard when
it is unwanted is an annoyance because of how much screen space it wastes. But &lt;em&gt;&lt;strong&gt;failing&lt;/strong&gt;&lt;/em&gt; to show
the soft keyboard when it is wanted fully prevents primary task completion. They need to type, and cannot.
Disaster.&lt;/p&gt;
&lt;p&gt;So I definitely sympathize with mobile browsers that show the soft keyboard on &lt;code&gt;.focus()&lt;/code&gt; of any input
element. But if any web page does focus an element, the soft keyboard will take over most of the screen.
[52]
The user will be &lt;em&gt;shut out of all other tasks&lt;/em&gt; until they dismiss the gigantic soft keyboard. This means
that calling &lt;code&gt;.focus()&lt;/code&gt; should only be done when the input is assured to be the primary task. It cannot
be used incidentally like it can on desktop.&lt;/p&gt;
&lt;h2&gt;Responsive layout: yes you have to&lt;/h2&gt;
&lt;p&gt;A lot of the documentation at large about designing for mobile will mention "responsive design". You could be
forgiven for thinking that the entirety of this concept is just learning how to set up the confusingly named
&lt;a href="https://developer.mozilla.org/en-US/docs/Web/CSS/Guides/Media_queries/Using"&gt;media queries&lt;/a&gt; to conditionalize
a web page's layout. There is a lot of advice about responsive design out there, not all of it wise. So here's
what I think is the heart of it:&lt;/p&gt;
&lt;h3&gt;Design for mobile first&lt;/h3&gt;
&lt;p&gt;Unless your HTML5 project is a static list of raccoon facts shown in a single column of black text on a white
background, it is unlikely that a UX designed for a desktop browser will also be usable on a phone.
[53]
You're more likely to have a phone design look okay on desktop than the other way around. So if you really really don't
want to create two separate designs, you need to iterate your design on a mobile phone first. (Or at the very
least a mobile simulator like is integrated into Chrome and Firefox.)&lt;/p&gt;
&lt;h3&gt;You probably need at least two layouts&lt;/h3&gt;
&lt;p&gt;Web designers these days seem unable to resist cramming the margins of web pages with hamburger
menus, nav bars, chat bots, and ads. None of these will look right on a phone's tall, narrow screen.
[54]
So if you can't refrain
from multi-column layout, you will need to make at least two different layouts: a single-column
layout for phones and then a multi-column layout for desktop browsers.&lt;/p&gt;
&lt;h3&gt;No you can't control the real size of anything&lt;/h3&gt;
&lt;p&gt;CSS has a &lt;a href="https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Styling_basics/Values_and_units"&gt;wealth of units&lt;/a&gt;
which you might imagine can give you all the options you need to make text and controls the right size on
screens big and small. But these are all &lt;a href="https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Styling_basics/Values_and_units#numbers_lengths_and_percentages"&gt;lies&lt;/a&gt;, even the ones like &lt;code&gt;cm&lt;/code&gt; and &lt;code&gt;mm&lt;/code&gt; which sound like they
should, you know, be SI distances like we learned in school. Actually, a CSS &lt;code&gt;cm&lt;/code&gt; is defined as a certain number of
&lt;code&gt;px&lt;/code&gt;, and &lt;code&gt;px&lt;/code&gt; are defined as a certain number of &lt;code&gt;cm&lt;/code&gt;. How big is a &lt;code&gt;px&lt;/code&gt; really, then? &lt;strong&gt;You can't know.&lt;/strong&gt;
[55]&lt;/p&gt;
&lt;p&gt;The actual explanation I like best is buried in the fine print on MDN:&lt;/p&gt;
&lt;code&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that 1px doesn't necessarily equal one physical device pixel.
On HD displays, it may span multiple physical pixels.
Similarly, 1cm in CSS often doesn't correspond to one hundredth of SI meter.
On a large TV screen, it typically is longer than that. The lengths are perceptual:
16px looks roughly the same on a phone, laptop, or TV screen at typical viewing distance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/code&gt;
&lt;p&gt;...the "perceptual" distances are determined by the device manufacturer, and then nudged by the
zoom fudge factor from the user's accessibility settings. So you'll have more luck reasoning about
the information density of "screenfuls" of your design, rather than trying to make a hit target the
"natural size" of a real-world object like a book or the size of a hit target for someone's fingertip.
(The latter is a sensible goal but purposely impossible on the web.)&lt;/p&gt;
&lt;p&gt;So for example, if you set your smallest readable font size to about &lt;code&gt;16px&lt;/code&gt; with a &lt;code&gt;line-height&lt;/code&gt; of &lt;code&gt;22px&lt;/code&gt;,
then you can (roughly) reason that about ten lines of text will fit on a &lt;code&gt;220px&lt;/code&gt; high area of the screen.
This is no help at all in determining the &lt;em&gt;size&lt;/em&gt; of the layout, but it will tell you roughly how much
the user can see at once on a mobile screen. Chrome and Firefox have "responsive design" modes in their
developer tools that you can use to get a sense of this.&lt;/p&gt;
&lt;h1&gt;&lt;a&gt;&lt;/a&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;This exercise to write an HTML5 game has generated ample evidence that web standards are doing little to threaten
the thriving occupation of web druid in the year 2025. I realize building a game in vanilla.js is a strange
choice, but I think anyone would hit many of these surprises trying to build any single-page application
design which handled media or had any input interactions. And, I remind you, these were just the problems
that surprised &lt;em&gt;me&lt;/em&gt;, and I have been doing web development since the 1990s. How much worse would this
be for someone inexperienced?&lt;/p&gt;
&lt;p&gt;Blundering into a multi-browser interactive HTML5 project without any thought put towards best
practices, mobile design, or having a testing lab will not result in a functioning UX. For the dream
of "write once run anywhere", HTML5 seems another broken promise. May it rest with the likes of Electron,
Fluttr, Java, Flash, and all the failed abstractions that have come before and since.&lt;/p&gt;
&lt;p&gt;On the bright side, HTML5 has a lot to like as a development platform. And it's &lt;em&gt;much&lt;/em&gt; better than it was even just
5 years ago. No I didn't avoid the need for a testing lab. But it's tremendously helpful to have a &lt;em&gt;mostly&lt;/em&gt; consistent
set of concepts and documentation that can apply to all targeted platforms. Cross-browser and cross-device
development costs aren't &lt;em&gt;eliminated&lt;/em&gt;, no. But they are greatly reduced. (&lt;strong&gt;Especially&lt;/strong&gt; if you choose your primary
testing device wisely: an old iPhone.)
Compared to, say, the cost of developing a Swift app for iOS and then a Kotlin app for Android, and then
still needing desktop and Windows support after that... HTML5 is great.&lt;/p&gt;
&lt;p&gt;Browsers also remain a superior delivery platform vs. native installs for casual and untrusted content.
Users can (mostly) rest easy knowing that your web-based project is not risking their device or
stealing their stuff. And visiting a URL is still less friction than a native install. So HTML5 is
a great choice for projects where friction should be as low as possible.&lt;/p&gt;
&lt;h3&gt;&lt;a&gt;&lt;/a&gt;Footnote: Should I use HTML5 to make games?&lt;/h3&gt;
&lt;p&gt;Probably not? The above 50ish problems should give you a flavor of the costs and limitations of trying to
make a polished, interactive experience. A larger reason not to choose HTML5 for game-like
interactions is just that it isn't very capable. You can trick your way into doing a lot with animation
frameworks like &lt;a href="https://animejs.com/"&gt;animeJS&lt;/a&gt;, or you could do 3D with webGL. But a purpose-built
game framework like Unity would have polyfill to protect you from more of these layout and audio
problems. And unlike with information pages about raccoons, game players seem happy to
receive game applications through the various app stores (Apple, Android, Steam, Playstation)
they've decided to trust.&lt;/p&gt;
&lt;h3&gt;Footnote: How could this be better?&lt;/h3&gt;
&lt;p&gt;It's tempting to feel like a grand technical fix is needed. For the industry to keep developing ever-more
insulating polyfill frameworks which abstract away ever more of animation, media, layout, CSS, and all the
rest of the core web. But I think this is both overkill and unlikely to succeed, because so many have tried and failed at
this already. (Please stop making more frameworks!)&lt;/p&gt;
&lt;p&gt;The unglamorous answer is that this might be just a &lt;strong&gt;documentation problem&lt;/strong&gt;. &lt;a href="https://developer.mozilla.org/en-US/"&gt;MDN&lt;/a&gt;
is pretty good, even though Mozilla increasingly concerns me as a steward of the open web. What if
it were just a little better?&lt;/p&gt;
&lt;p&gt;One problem with MDN is that if you're trying to build a truly polished, consistent experience across many
browsers, there's almost more features to avoid than to use. What if you could tick a box in the upper
right corner of the documentation page saying "I want to support all these devices" and then the MDN pages
would change to hide features that won't work, show example code that's already tested on
all the devices you selected, and show you what percentage of users will have success with the set of browsers
you picked? Wouldn't that be nice? (The Android API docs and the Node.js docs have features that go toward this
idea, though they don't do it quite so thoroughly.)&lt;/p&gt;
&lt;h3&gt;Footnote: achieving a polished experience&lt;/h3&gt;
&lt;p&gt;Although I think there's good evidence for a lack of a "safe harbor" &lt;em&gt;feature set&lt;/em&gt; in HTML5, there &lt;em&gt;is&lt;/em&gt; a way to
enjoy low development costs and easily use HTML5 to support many devices and browsers:
simply &lt;strong&gt;don't care much about the user experience.&lt;/strong&gt; If you don't mind your users hitting glitches,
setbacks, and frustrations as they try to read your text, see your images, and fill out your input forms,
then HTML5 will be very affordable for your project.&lt;/p&gt;
&lt;p&gt;Browsers are designed to be the end user's self-service toolkit to combat our bad websites.
Users can override fonts, mute sounds, enlarge text, pinch-zoom in, open images in a separate tab, copy-and-paste, autofill rote form inputs,
switch to "simplified reading mode", search for the same information elsewhere, reload the page to reset Javascript bugs,
and even try a different browser to see if they happen to have the one on their computer that we bothered to test.&lt;/p&gt;
&lt;p&gt;The problem with my project was that I wanted to &lt;strong&gt;meet a high quality bar&lt;/strong&gt; for smoothness and polish in the resulting app.
Games are supposed to be fun, and frustration from glitches is not fun. As a result, games are some of the highest-quality
user experiences the software industry makes. Game developers &lt;strong&gt;care much more&lt;/strong&gt; than the average software team about the
consistency of look, feel, and smoothness of their UX. Polish was the real source of my high development costs.&lt;/p&gt;
&lt;h3&gt;Footnote: a damning case against vanilla.js?&lt;/h3&gt;
&lt;p&gt;I admit that all these snags made me a bit more pessimistic about using "bare" HTML5 for projects. But
counterpoint, there is no popular web framework that would have polyfilled &lt;em&gt;all&lt;/em&gt; these particular
issues away either. React, tailwind, bootstrap, Angular etc do not try to &lt;em&gt;entirely&lt;/em&gt; abstract things
like CSS, animations, text input, touch input, and media playback. Some of the widget library frameworks would have helped,
but I would still have had a lot of these problems. Unity and Godot might be better choices, but I have
no experience with them and I assume they only make sense for games.&lt;/p&gt;
&lt;p&gt;For my own web projects, I have an ever-growing proprietary polyfill library into which I encode best
practices. And I know from experience that large web shops do the same. Oh well.&lt;/p&gt;

Index
&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tldr"&gt;tl;dr advice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#apple"&gt;Culprit 1: Apple's opinions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bugs"&gt;Culprit 2: Behaviors &amp;amp; bugs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mobile"&gt;Culprit 3: Mobile redesign&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#footnotes"&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://zerotrickpony.com/articles/browser-bugs/</guid>
      <category>Hacker News</category>
      <pubDate>Thu, 18 Dec 2025 04:55:18 +0000</pubDate>
    </item>
    <item>
      <title>Go-boot: bare metal Go UEFI boot manager</title>
      <link>https://github.com/usbarmory/go-boot</link>
      <description>The go-boot project is a TamaGo unikernel implementing a UEFI
Shell and OS loader for AMD64 platforms, allowing UEFI API interaction and OS
loading.</description>
      <content:encoded>&lt;article class="markdown-body entry-content container-lg" itemprop="text"&gt;&lt;h1&gt;Introduction&lt;/h1&gt;&lt;a href="#introduction"&gt;&lt;/a&gt;
&lt;p&gt;The &lt;a href="https://github.com/usbarmory/go-boot"&gt;go-boot&lt;/a&gt; project is a
&lt;a href="https://github.com/usbarmory/tamago"&gt;TamaGo&lt;/a&gt; unikernel implementing a UEFI
Shell and OS loader for AMD64 platforms, allowing UEFI API interaction and OS
loading.&lt;/p&gt;
&lt;p&gt;The OS loading functionality supports launching of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.&lt;/code&gt; EFI application images&lt;/li&gt;
&lt;li&gt;&lt;code&gt;l&lt;/code&gt; Linux kernels, with configuration parsed from Linux Userspace API (UAPI) &lt;a href="https://uapi-group.org/specifications/specs/boot_loader_specification/"&gt;boot loader entries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;w&lt;/code&gt; Windows UEFI boot manager&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The support of
&lt;a href="https://github.com/usbarmory/boot-transparency"&gt;boot-transparency&lt;/a&gt; is planned
for future releases.&lt;/p&gt;
&lt;h1&gt;Authors&lt;/h1&gt;&lt;a href="#authors"&gt;&lt;/a&gt;
&lt;p&gt;Andrea Barisani
&lt;a href="mailto:andrea@inversepath.com"&gt;andrea@inversepath.com&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Operation&lt;/h1&gt;&lt;a href="#operation"&gt;&lt;/a&gt;
&lt;p&gt;The default operation is to present an UEFI shell and its help, the ⏎ shortcut
(identically to &lt;code&gt;l&lt;/code&gt; or &lt;code&gt;linux&lt;/code&gt;) boots the default UAPI entry set at compile
time (see &lt;em&gt;Compiling&lt;/em&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Shell&amp;gt; go-boot.efi

initializing EFI services
initializing console (text)

go-boot • tamago/amd64 (go1.24.1) • UEFI x64

.               &amp;lt;path&amp;gt;                   # load and start EFI image
build                                    # build information
cat             &amp;lt;path&amp;gt;                   # show file contents
clear                                    # clear screen
cpuid           &amp;lt;leaf&amp;gt; &amp;lt;subleaf&amp;gt;         # show CPU capabilities
date            (time in RFC339 format)? # show/change runtime date and time
efivar          (verbose)?               # list UEFI variables
dns             &amp;lt;host&amp;gt;                   # resolve domain
exit,quit                                # exit application
halt,shutdown                            # shutdown system
info                                     # runtime information
linux,l         (loader entry path)?     # boot Linux kernel image
linux,l,\r                               # `l \loader\entries\arch.conf`
log                                      # show runtime logs
ls              (path)?                  # list directory contents
lspci                                    # list PCI devices
memmap          (e820)?                  # show UEFI memory map
mode            &amp;lt;mode&amp;gt;                   # set screen mode
net             &amp;lt;ip&amp;gt; &amp;lt;mac&amp;gt; &amp;lt;gw&amp;gt; (debug)? # start UEFI networking
peek            &amp;lt;hex offset&amp;gt; &amp;lt;size&amp;gt;      # memory display (use with caution)
poke            &amp;lt;hex offset&amp;gt; &amp;lt;hex value&amp;gt; # memory write   (use with caution)
protocol        &amp;lt;registry format GUID&amp;gt;   # locate UEFI protocol
reset           (cold|warm)?             # reset system
stack                                    # goroutine stack trace (current)
stackall                                 # goroutine stack trace (all)
stat            &amp;lt;path&amp;gt;                   # show file information
uefi                                     # UEFI information
uptime                                   # show system running time
windows,win,w                            # launch Windows UEFI boot manager

&amp;gt; uefi
UEFI Revision ......: 2.70
Firmware Vendor ....: Lenovo
Firmware Revision ..: 0x1560
Runtime Services  ..: 0x90e2eb98
Boot Services ......: 0x6bd17690
Frame Buffer .......: 1920x1200 @ 0x4000000000
Configuration Tables: 0x8f426018
  ee4e5898-3914-4259-9d6e-dc7bd79403cf (0x8db6dc98)
  dcfa911d-26eb-469f-a220-38b7dc461220 (0x8b037018)
...

&amp;gt; memmap
Type Start            End              Pages            Attributes
02   0000000090000000 0000000090000fff 0000000000000001 000000000000000f
...

&amp;gt; linux \loader\entries\arch.conf
loading boot loader entry \loader\entries\arch.conf
go-boot exiting EFI boot services and jumping to kernel
Linux version 6.13.6-arch1-1 (linux@archlinux) (gcc (GCC) 14.2.1 20250207, GNU ld (GNU Binutils) 2.44)
...
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Package documentation&lt;/h1&gt;&lt;a href="#package-documentation"&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href="https://pkg.go.dev/github.com/usbarmory/go-boot"&gt;&lt;img alt="Go Reference" src="https://camo.githubusercontent.com/1280be51cfaca015309656f9d1ca547200be57cb206155ba8b2f847901aa08d8/68747470733a2f2f706b672e676f2e6465762f62616467652f6769746875622e636f6d2f75736261726d6f72792f676f2d626f6f742e737667"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Hardware Compatibility List&lt;/h1&gt;&lt;a href="#hardware-compatibility-list"&gt;&lt;/a&gt;
&lt;p&gt;The list of supported hardware is available in the
project wiki &lt;a href="https://github.com/usbarmory/go-boot/wiki#hardware-compatibility-list"&gt;HCL&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The list provides test &lt;code&gt;IMAGE_BASE&lt;/code&gt; values to pass while &lt;em&gt;Compiling&lt;/em&gt;.&lt;/p&gt;
&lt;h1&gt;Compiling&lt;/h1&gt;&lt;a href="#compiling"&gt;&lt;/a&gt;
&lt;p&gt;Build the &lt;a href="https://github.com/usbarmory/tamago-go"&gt;TamaGo compiler&lt;/a&gt;
(or use the &lt;a href="https://github.com/usbarmory/tamago-go/releases/latest"&gt;latest binary release&lt;/a&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget https://github.com/usbarmory/tamago-go/archive/refs/tags/latest.zip
unzip latest.zip
cd tamago-go-latest/src &amp;amp;&amp;amp; ./all.bash
cd ../bin &amp;amp;&amp;amp; export TAMAGO=`pwd`/go
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following environment variables configure the &lt;code&gt;go-boot.efi&lt;/code&gt; executable
build:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;IMAGE_BASE&lt;/code&gt;: must be set (in hex) within a memory range
available in the target UEFI environment for the unikernel allocation, the
&lt;a href="https://github.com/usbarmory/go-boot/wiki#hardware-compatibility-list"&gt;HCL&lt;/a&gt; or
&lt;code&gt;memmap&lt;/code&gt; command from an &lt;a href="https://github.com/pbatard/UEFI-Shell"&gt;UEFI Shell&lt;/a&gt;
can provide such value, when empty a common default value is set.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DEFAULT_EFI_ENTRY&lt;/code&gt;: defines the &lt;code&gt;.&lt;/code&gt; shortcut entry path
for EFI image loading, it defaults to &lt;code&gt;\efi\boot\bootx64.efi&lt;/code&gt;
when unspecified.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DEFAULT_LINUX_ENTRY&lt;/code&gt;: defines the &lt;code&gt;linux,l,\r&lt;/code&gt; shortcut loader entry path
for Linux kernel image booting, it defaults to &lt;code&gt;\loader\entries\arch.conf&lt;/code&gt;
when unspecified.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;CONSOLE&lt;/code&gt;: set to either &lt;code&gt;com1&lt;/code&gt; or &lt;code&gt;text&lt;/code&gt; (default) controls the output
console to either serial port or UEFI console.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;NET&lt;/code&gt;: set to either &lt;code&gt;0&lt;/code&gt; (default) or &lt;code&gt;1&lt;/code&gt; controls enabling of UEFI
networking support (see &lt;em&gt;UEFI networking&lt;/em&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Build the &lt;code&gt;go-boot.efi&lt;/code&gt; executable:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/usbarmory/go-boot &amp;amp;&amp;amp; cd go-boot
make efi IMAGE_BASE=10000000 CONSOLE=text
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Executing as UEFI application&lt;/h1&gt;&lt;a href="#executing-as-uefi-application"&gt;&lt;/a&gt;
&lt;p&gt;The &lt;code&gt;go-boot.efi&lt;/code&gt; application executable, built after &lt;em&gt;Compiling&lt;/em&gt;, can be
loaded from an &lt;a href="https://github.com/pbatard/UEFI-Shell"&gt;UEFI Shell&lt;/a&gt;
or boot manager, the following example shows an entry for
&lt;a href="https://www.freedesktop.org/wiki/Software/systemd/systemd-boot/"&gt;systemd-boot&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# /boot/loader/entries/go-boot.conf
title Go Boot
efi /EFI/Linux/go-boot.efi
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;UEFI boot manager entry&lt;/h1&gt;&lt;a href="#uefi-boot-manager-entry"&gt;&lt;/a&gt;
&lt;p&gt;The following example shows creation of an EFI boot entry using
&lt;a href="https://github.com/rhboot/efibootmgr"&gt;efibootmgr&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;efibootmgr -C -L "go-boot" -d $DISK -p $PART -l '\EFI\go-boot.efi'
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;UEFI networking&lt;/h1&gt;&lt;a href="#uefi-networking"&gt;&lt;/a&gt;
&lt;p&gt;With &lt;code&gt;NET=1&lt;/code&gt; passed in the environment builds include UEFI networking support
through the &lt;a href="https://uefi.org/specs/UEFI/2.10_A/24_Network_Protocols_SNP_PXE_BIS.html"&gt;Simple Network Protocol&lt;/a&gt;
(SNP) and &lt;a href="https://github.com/usbarmory/go-net"&gt;go-net&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On such builds the &lt;code&gt;net&lt;/code&gt; and &lt;code&gt;dns&lt;/code&gt; commands become available and &lt;code&gt;make qemu&lt;/code&gt;
will require a tap0 interface.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;net&lt;/code&gt; command takes an IP address in CIDR notation, a fixed MAC address or
&lt;code&gt;:&lt;/code&gt; to automatically generate a random MAC, and a gateway IP address as
arguments.&lt;/p&gt;
&lt;p&gt;The optional &lt;code&gt;debug&lt;/code&gt; strings can be passed as final argument to &lt;code&gt;net&lt;/code&gt; to enable
Go &lt;a href="https://pkg.go.dev/net/http/pprof"&gt;profiling server&lt;/a&gt; and an unauthenticated
SSH console exposing the UEFI shell.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; net 10.0.0.1/24 : 10.0.0.2 debug
starting debug servers:
        http://10.0.0.1:80/debug/pprof
        ssh://10.0.0.1:22
network initialized (10.0.0.1/24 da:e7:ac:e2:5e:05)

&amp;gt; dns golang.org
[142.251.209.17 2a00:1450:4002:410::2011]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Emulated hardware with QEMU&lt;/h1&gt;&lt;a href="#emulated-hardware-with-qemu"&gt;&lt;/a&gt;
&lt;p&gt;QEMU supported targets can be executed under emulation, using the
&lt;a href="https://github.com/tianocore/tianocore.github.io/wiki/OVMF"&gt;Open Virtual Machine Firmware&lt;/a&gt;
as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make qemu OVMFCODE=&amp;lt;path to OVMF_CODE.fd&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With &lt;code&gt;NET=1&lt;/code&gt; tap0 should be configured as follows (Linux example):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ip tuntap add dev tap0 mode tap group &amp;lt;your user group&amp;gt;
ip addr add 10.0.0.2/24 dev tap0
ip link set tap0 up
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An emulated target can be &lt;a href="https://retrage.github.io/2019/12/05/debugging-ovmf-en.html/"&gt;debugged with GDB&lt;/a&gt;
using &lt;code&gt;make qemu-gdb&lt;/code&gt;, this will make qemu waiting for a GDB connection that
can be launched as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gdb -ex "target remote 127.0.0.1:1234"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Breakpoints can be set in the usual way:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;b cpuinit
continue
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Cloud deployments&lt;/h1&gt;&lt;a href="#cloud-deployments"&gt;&lt;/a&gt;
&lt;p&gt;The following example demonstrates how to create, and deploy, a UEFI-bootable
image for cloud deployments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/usbarmory/go-boot/wiki/Google-Compute-Engine"&gt;Google Compute Engine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;License&lt;/h1&gt;&lt;a href="#license"&gt;&lt;/a&gt;
&lt;p&gt;go-boot | &lt;a href="https://github.com/usbarmory/go-boot"&gt;https://github.com/usbarmory/go-boot&lt;/a&gt;
Copyright (c) The go-boot authors. All Rights Reserved.&lt;/p&gt;
&lt;p&gt;These source files are distributed under the BSD-style license found in the
&lt;a href="https://github.com/usbarmory/go-boot/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://github.com/usbarmory/go-boot</guid>
      <category>Hacker News</category>
      <pubDate>Thu, 18 Dec 2025 00:08:46 +0000</pubDate>
    </item>
    <item>
      <title>Learn Lisp/Fennel Programming Against Neovim</title>
      <link>https://github.com/humorless/fennel-fp-neovim</link>
      <description>A series of Neovim Plugin Development introduction articles, using Fennel as the programming language to explain Lisp, FP, and advanced programming ideas.</description>
      <content:encoded>&lt;article class="markdown-body entry-content container-lg" itemprop="text"&gt;&lt;h1&gt;fennel-fp-neovim&lt;/h1&gt;&lt;a href="#fennel-fp-neovim"&gt;&lt;/a&gt;
&lt;p&gt;A series of Neovim Plugin Development introduction articles, using Fennel as the programming language to explain Lisp, FP, and advanced programming ideas.&lt;/p&gt;
&lt;h2&gt;Exploring Fennel and Functional Programming in Neovim&lt;/h2&gt;&lt;a href="#exploring-fennel-and-functional-programming-in-neovim"&gt;&lt;/a&gt;
&lt;p&gt;In the AI era, developers need to think about new development paradigms: "AI helps us quickly generate code, but debugging and verification still require active developer intervention." This series of articles will start with Neovim + Fennel, guiding readers into the new world of "Interactive Development" and "Functional Programming".&lt;/p&gt;
&lt;p&gt;Content includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fennel Language: Syntax, commonly used libraries.&lt;/li&gt;
&lt;li&gt;Lisp Thinking: S-expression editing, interactive development.&lt;/li&gt;
&lt;li&gt;Functional Programming: Pure functions, practical techniques like map/filter/reduce.&lt;/li&gt;
&lt;li&gt;Neovim Plugin Development: From simple scripts to complete plugins.&lt;/li&gt;
&lt;li&gt;Patterns and Principles: The rules that guide us to success.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's explore smarter and more elegant ways of program development together in the AI era.&lt;/p&gt;
&lt;h2&gt;Acknowledgments&lt;/h2&gt;&lt;a href="#acknowledgments"&gt;&lt;/a&gt;
&lt;p&gt;The completion of this article series is largely inspired by my work at &lt;a href="https://gaiwan.co/"&gt;Gaiwan&lt;/a&gt; and &lt;a href="https://lambdaisland.com/"&gt;LambdaIsland&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This series was first published in Taiwan, and then I translated the original &lt;a href="https://ithelp.ithome.com.tw/users/20161869/ironman/8497"&gt;Traditional Chinese version&lt;/a&gt; into English using &lt;a href="https://github.com/playcanvas/markdown-translator"&gt;markdown-translator&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you have any thoughts after reading this series, feel free to reach out to &lt;a href="https://replware.dev/"&gt;me&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Table of contents&lt;/h2&gt;&lt;a href="#table-of-contents"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day01.md"&gt;Preface: What Should You Learn After AI Accelerates Coding?&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Software Development in the AI Era&lt;/li&gt;
&lt;li&gt;Fennel Lowers the Entry Barrier for Interactive Development / Functional Programming&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day02.md"&gt;Fennel Brief History and Development Environment&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;From Individual to Community: The Birth of Fennel&lt;/li&gt;
&lt;li&gt;Development Environment - Installation&lt;/li&gt;
&lt;li&gt;Development Environment - Plugin Introduction&lt;/li&gt;
&lt;li&gt;Development Environment - Automatic Formatting&lt;/li&gt;
&lt;li&gt;Interactive Development&lt;/li&gt;
&lt;li&gt;S-expression Editing&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day03.md"&gt;Fennel Language Crash Course—Lisp Syntax&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Let's Talk About Syntax First&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day04.md"&gt;Fennel Language Crash Course—Core Syntax&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Functions&lt;/li&gt;
&lt;li&gt;Local Variables&lt;/li&gt;
&lt;li&gt;Numbers and Strings&lt;/li&gt;
&lt;li&gt;Containers&lt;/li&gt;
&lt;li&gt;Loops&lt;/li&gt;
&lt;li&gt;Iteration&lt;/li&gt;
&lt;li&gt;Conditional Statements&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day05.md"&gt;Fennel Language Crash Course—Lua&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Lua and Minimalism&lt;/li&gt;
&lt;li&gt;Lua Overview&lt;/li&gt;
&lt;li&gt;Other Uses of Lua's Table&lt;/li&gt;
&lt;li&gt;Fennel Syntax Extensions for Tables&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day06.md"&gt;Fennel Language Crash Course—LuaRocks&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;The Problem of Multiple Interpreters&lt;/li&gt;
&lt;li&gt;LuaRocks&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day07.md"&gt;Fennel Language Crash Course—nfnl Library&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;nfnl Examples&lt;/li&gt;
&lt;li&gt;Clojure-style Programming&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day08.md"&gt;Lisp In-depth—Interactive Development&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Other Commands for Interactive Development&lt;/li&gt;
&lt;li&gt;Leveraging Interactive Development&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day09.md"&gt;Lisp In-depth—S-expression Editing&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Parenthesis Pairing Issues&lt;/li&gt;
&lt;li&gt;Parenthesis Editing Issues&lt;/li&gt;
&lt;li&gt;Navigating and Editing within the Syntax Tree&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day10.md"&gt;Lisp In-depth—Macro&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;What Macros Does Lisp Provide?&lt;/li&gt;
&lt;li&gt;Fennel's Reader Macro&lt;/li&gt;
&lt;li&gt;Reconsidering Lisp Macros&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day11.md"&gt;Lisp In-depth—Data-Oriented Programming&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;The Troubles of Tree Traversal&lt;/li&gt;
&lt;li&gt;Clojure's Unique Flavor&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day12.md"&gt;Demystifying Functional Programming (FP)—High-Level Semantics&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Why is FP considered a high-level semantic? Starting from Accounting&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day13.md"&gt;Demystifying Functional Programming (FP)—The Challenge of Definition&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;What is Functional Programming (FP)? What is its definition?&lt;/li&gt;
&lt;li&gt;Separating Concept from Implementation&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day14.md"&gt;Demystifying Functional Programming (FP)—Common Mechanisms&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Higher-Order Functions&lt;/li&gt;
&lt;li&gt;Value Copying&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day15.md"&gt;Demystifying Functional Programming (FP)—Advanced Topics&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Functional Programming Idioms (FP idioms)&lt;/li&gt;
&lt;li&gt;FP and Code Reuse (code reuse)&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day16.md"&gt;Neovim Plugin Development—Getting Started&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Why the Learning Failed&lt;/li&gt;
&lt;li&gt;A Relatively Reasonable Learning Strategy&lt;/li&gt;
&lt;li&gt;Neovim Runtime&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day17.md"&gt;Neovim Plugin Development—Hello World&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Hello World Plugin&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day18.md"&gt;Neovim Plugin Development—Standard Plugin&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Plugin Development&lt;/li&gt;
&lt;li&gt;Module Imports&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day19.md"&gt;Neovim Plugin Development—How to debug?&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Inspecting Internal State&lt;/li&gt;
&lt;li&gt;Internal Execution Order&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day20.md"&gt;Project Discussion—auto-conjure&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Problem Overview&lt;/li&gt;
&lt;li&gt;Solution Architecture&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day21.md"&gt;Project Discussion—Conjure Piglet Client&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Problem Overview&lt;/li&gt;
&lt;li&gt;Solution Architecture&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day22.md"&gt;Project Discussion—WebSocket&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Problem Overview&lt;/li&gt;
&lt;li&gt;Solution Architecture&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day23.md"&gt;Project Discussion—CBOR&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Problem Overview&lt;/li&gt;
&lt;li&gt;Solution Architecture&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day24.md"&gt;Project Discussion—Fennel's Jump to Definition&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Problem Overview&lt;/li&gt;
&lt;li&gt;Solution Architecture&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day25.md"&gt;Project Discussion—Tree-sitter Behind Jump to Definition&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Problem Overview&lt;/li&gt;
&lt;li&gt;Solution Architecture&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day26.md"&gt;Patterns and Principles—Bottlenecks and Improvements&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Theory of Constraints&lt;/li&gt;
&lt;li&gt;The Biggest Constraint in the Age of AI&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day27.md"&gt;Patterns and Principles—Uncertainty&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Ways to Respond&lt;/li&gt;
&lt;li&gt;Related Theory&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day28.md"&gt;Patterns and Principles—Complexity&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;The Way to Respond&lt;/li&gt;
&lt;li&gt;Related Theory&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day29.md"&gt;Patterns and Principles—Modification Propagation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;The Way to Respond&lt;/li&gt;
&lt;li&gt;Related Theories&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/humorless/fennel-fp-neovim/blob/main/day30.md"&gt;Patterns and Principles—Tacit Knowledge&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;The Way to Respond&lt;/li&gt;
&lt;li&gt;Related Theories&lt;/li&gt;
&lt;li&gt;Summary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;License&lt;/h2&gt;&lt;a href="#license"&gt;&lt;/a&gt;
&lt;p&gt;Copyright @ Laurence Chen&lt;/p&gt;
&lt;p&gt;Licensed under the term of &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;the Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;</content:encoded>
      <guid isPermaLink="false">https://github.com/humorless/fennel-fp-neovim</guid>
      <category>Hacker News</category>
      <pubDate>Wed, 17 Dec 2025 20:09:17 +0000</pubDate>
    </item>
  </channel>
</rss>
